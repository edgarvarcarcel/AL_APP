{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mk2cQAl9BrC7"
   },
   "source": [
    "# **0. Introduction**\n",
    "With this notebook you can define:\n",
    "\n",
    "1. Preprocessing method : If you want to perform scaling or not\n",
    "2. Over / under sampling method.\n",
    "3. Feature Selection Method\n",
    "4. Methods for handle nans\n",
    "5. Hyperparameters of the Pytorch Neural Network\n",
    "6. Categorical preprocessing (using hash vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5048,
     "status": "ok",
     "timestamp": 1715801900206,
     "user": {
      "displayName": "Edgar David Varcarcel Trujillo",
      "userId": "04708129325083286457"
     },
     "user_tz": 300
    },
    "id": "GGHrhsUkB3k0",
    "outputId": "a12535ad-4dea-4dc5-8880-c335178023d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported.\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "\n",
    "# Data Manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plots\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "from category_encoders import HashingEncoder\n",
    "\n",
    "# Data splitting\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Feature Selection\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import torch as th\n",
    "from torch import Tensor\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "# Feature Selection\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Oversampling\n",
    "from imblearn.over_sampling import SMOTE , SMOTENC , ADASYN , BorderlineSMOTE , KMeansSMOTE ,SVMSMOTE\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import confusion_matrix , f1_score , precision_score , recall_score , make_scorer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve, roc_auc_score , precision_recall_curve, average_precision_score , accuracy_score\n",
    "\n",
    "# Hyperparameter optimization\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Data imputation\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Feature importance\n",
    "import shap\n",
    "\n",
    "# Utils\n",
    "from itertools import product\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "import pickle\n",
    "from scipy.interpolate import make_interp_spline\n",
    "from scipy.interpolate import interp1d\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Set Numpy Seed\n",
    "np.random.seed(0)\n",
    "#plt.style.use('ggplot')\n",
    "print('Libraries imported.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLOBAL PARAMETERS:\n",
      "FEATURE_SELECTION : FIX\n",
      "SAMPLING : ADASYN\n",
      "PERFORM_SCALING : YES\n",
      "NAN_HANDLE : KNN\n",
      "CATEGORICAL_FEATURES : HASH\n",
      "PYTORCH_PARAMETERS : {'hidden_sizes': [[100, 100, 50, 100, 100]], 'dropout_prob': [0.5], 'class_weights': [[1.0, 1.0]], 'BATCH_SIZE': [16]}\n",
      "\n",
      "Experiment Name: Best_Pytorch_Model\n"
     ]
    }
   ],
   "source": [
    "# Define parameters of how to run the notebook\n",
    "# Posible options\n",
    "\"\"\"\n",
    "GLOBAL_PARAMETERS = {'FEATURE_SELECTION' : ['XGBOOST' , 'VARIANCE_THRESHOLD' , 'F_CLASSIF' , 'RFE' , 'NONE' , 'ALL_FEATURE_SELECTION_METHODS'],\n",
    "                    'SAMPLING' : ['SMOTE' , 'ADASYN' , 'BORDERLINE_SMOTE', 'NONE' , 'ALL_OVERSAMPLING_METHODS'],\n",
    "                    'PERFORM_SCALING' : ['YES' , 'NO'],\n",
    "                    'NAN_HANDLE' : ['DROP_ALL' , 'MASK_ALL' , 'DROP_PERCENT' , KNN],\n",
    "                    'CATEGORICAL_FEATURES' : ['NONE' , 'HASH'],\n",
    "                    'PYTORC_PARAMETERS' : {}}\n",
    "\"\"\"\n",
    "GLOBAL_PARAMETERS = {'FEATURE_SELECTION' :  'FIX',\n",
    "                    'SAMPLING' : 'ADASYN' ,\n",
    "                    'PERFORM_SCALING' : 'YES',\n",
    "                    'NAN_HANDLE' : 'KNN' ,\n",
    "                    'CATEGORICAL_FEATURES' : 'HASH',\n",
    "                    'PYTORCH_PARAMETERS' : {'hidden_sizes' : [ [100 , 100 , 50 , 100 , 100]],\n",
    "                                            'dropout_prob' : [0.5],\n",
    "                                            'class_weights' : [[1.0 , 1.0]],\n",
    "                                           'BATCH_SIZE' : [16]}}\n",
    "\n",
    "# Assert that the selection is ok\n",
    "assert GLOBAL_PARAMETERS['FEATURE_SELECTION'] in ['XGBOOST' , 'VARIANCE_THRESHOLD' , 'F_CLASSIF' , 'RFE' , 'NONE' , 'ALL_FEATURE_SELECTION_METHODS' , 'FIX']\n",
    "assert GLOBAL_PARAMETERS['SAMPLING'] in ['SMOTE' , 'ADASYN' , 'BORDERLINE_SMOTE' , 'NONE' , 'ALL_OVERSAMPLING_METHODS']\n",
    "assert GLOBAL_PARAMETERS['PERFORM_SCALING'] in ['YES' , 'NO']\n",
    "assert GLOBAL_PARAMETERS['NAN_HANDLE'] in ['DROP_ALL' , 'MASK_ALL' , 'DROP_PERCENT' , 'KNN']\n",
    "assert GLOBAL_PARAMETERS['CATEGORICAL_FEATURES'] in ['NONE' , 'HASH']\n",
    "\n",
    "print('GLOBAL PARAMETERS:')\n",
    "for _ in GLOBAL_PARAMETERS.keys():\n",
    "    print(_ , ':' , GLOBAL_PARAMETERS[_])\n",
    "\n",
    "MODEL_NAME = 'Best_Pytorch_Model'\n",
    "print('\\nExperiment Name:' ,MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 739
    },
    "executionInfo": {
     "elapsed": 3976,
     "status": "ok",
     "timestamp": 1715801908962,
     "user": {
      "displayName": "Edgar David Varcarcel Trujillo",
      "userId": "04708129325083286457"
     },
     "user_tz": 300
    },
    "id": "ACCUsyi3B3fs",
    "outputId": "78a68a71-dfc4-4016-c63f-31db6ce3b718"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>record_id</th>\n",
       "      <th>data_group</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>bmi</th>\n",
       "      <th>active_smoking</th>\n",
       "      <th>pack_years</th>\n",
       "      <th>alcohol_abuse</th>\n",
       "      <th>real_function_ckd_stages</th>\n",
       "      <th>preoperative_hemoglobin_level</th>\n",
       "      <th>...</th>\n",
       "      <th>approach</th>\n",
       "      <th>conversion</th>\n",
       "      <th>type_of_anastomosis -&gt; das von UK sind alles  Ileocolonic anastomosis</th>\n",
       "      <th>anastomotic_technique</th>\n",
       "      <th>anastomotic_configuration</th>\n",
       "      <th>protective_stomy</th>\n",
       "      <th>surgeon_experience</th>\n",
       "      <th>anastomotic_leackage</th>\n",
       "      <th>BIHistoryOfIschaemicHeartDisease</th>\n",
       "      <th>BIHistoryOfDiabetes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5921</td>\n",
       "      <td>clarunisclaraspita</td>\n",
       "      <td>1.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.6</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59231</td>\n",
       "      <td>clarunisclaraspita</td>\n",
       "      <td>1.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.3</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>592-82</td>\n",
       "      <td>clarunisclaraspita</td>\n",
       "      <td>2.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>18.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.7</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>592-92</td>\n",
       "      <td>clarunisclaraspita</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>22.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.9</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59251</td>\n",
       "      <td>clarunisclaraspita</td>\n",
       "      <td>1.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>17.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.7</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5906</th>\n",
       "      <td>473-772</td>\n",
       "      <td>university_wrzburg</td>\n",
       "      <td>2.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>20.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.6</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5907</th>\n",
       "      <td>473-806</td>\n",
       "      <td>university_wrzburg</td>\n",
       "      <td>2.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5908</th>\n",
       "      <td>473-808</td>\n",
       "      <td>university_wrzburg</td>\n",
       "      <td>2.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>22.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.7</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5909</th>\n",
       "      <td>473-830</td>\n",
       "      <td>university_wrzburg</td>\n",
       "      <td>1.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>24.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.2</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5910</th>\n",
       "      <td>473-92</td>\n",
       "      <td>university_wrzburg</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5911 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     record_id          data_group  sex   age   bmi  active_smoking  \\\n",
       "0         5921  clarunisclaraspita  1.0  87.0   NaN             1.0   \n",
       "1        59231  clarunisclaraspita  1.0  53.0  19.2             1.0   \n",
       "2       592-82  clarunisclaraspita  2.0  71.0  18.8             0.0   \n",
       "3       592-92  clarunisclaraspita  1.0  39.0  22.4             0.0   \n",
       "4        59251  clarunisclaraspita  1.0  67.0  17.2             1.0   \n",
       "...        ...                 ...  ...   ...   ...             ...   \n",
       "5906   473-772  university_wrzburg  2.0  24.0  20.4             0.0   \n",
       "5907   473-806  university_wrzburg  2.0  87.0  32.0             0.0   \n",
       "5908   473-808  university_wrzburg  2.0  74.0  22.7             1.0   \n",
       "5909   473-830  university_wrzburg  1.0  85.0  24.3             0.0   \n",
       "5910    473-92  university_wrzburg  2.0   NaN  14.8             0.0   \n",
       "\n",
       "      pack_years  alcohol_abuse  real_function_ckd_stages  \\\n",
       "0           25.0            2.0                       5.0   \n",
       "1           35.0            3.0                       1.0   \n",
       "2            NaN            3.0                       1.0   \n",
       "3            0.0            3.0                       2.0   \n",
       "4           75.0            3.0                       1.0   \n",
       "...          ...            ...                       ...   \n",
       "5906         NaN            3.0                       1.0   \n",
       "5907         NaN            3.0                       4.0   \n",
       "5908        20.0            3.0                       1.0   \n",
       "5909         NaN            3.0                       2.0   \n",
       "5910         NaN            3.0                       NaN   \n",
       "\n",
       "      preoperative_hemoglobin_level  ...  approach  conversion  \\\n",
       "0                               9.6  ...       1.0         NaN   \n",
       "1                              11.3  ...       3.0         NaN   \n",
       "2                              13.7  ...       3.0         0.0   \n",
       "3                              11.9  ...       3.0         0.0   \n",
       "4                               7.7  ...       3.0         NaN   \n",
       "...                             ...  ...       ...         ...   \n",
       "5906                           13.6  ...       1.0         NaN   \n",
       "5907                            NaN  ...       3.0         NaN   \n",
       "5908                            9.7  ...       3.0         NaN   \n",
       "5909                           13.2  ...       3.0         NaN   \n",
       "5910                            NaN  ...       NaN         1.0   \n",
       "\n",
       "      type_of_anastomosis -> das von UK sind alles  Ileocolonic anastomosis  \\\n",
       "0                                                   3.0                       \n",
       "1                                                   3.0                       \n",
       "2                                                   3.0                       \n",
       "3                                                   3.0                       \n",
       "4                                                   3.0                       \n",
       "...                                                 ...                       \n",
       "5906                                                3.0                       \n",
       "5907                                                3.0                       \n",
       "5908                                                3.0                       \n",
       "5909                                                3.0                       \n",
       "5910                                                3.0                       \n",
       "\n",
       "      anastomotic_technique  anastomotic_configuration  protective_stomy  \\\n",
       "0                       2.0                        1.0               3.0   \n",
       "1                       1.0                        3.0               3.0   \n",
       "2                       NaN                        1.0               3.0   \n",
       "3                       1.0                        3.0               3.0   \n",
       "4                       1.0                        3.0               3.0   \n",
       "...                     ...                        ...               ...   \n",
       "5906                    2.0                        2.0               3.0   \n",
       "5907                    2.0                        2.0               3.0   \n",
       "5908                    1.0                        3.0               3.0   \n",
       "5909                    2.0                        2.0               3.0   \n",
       "5910                    2.0                        NaN               3.0   \n",
       "\n",
       "      surgeon_experience  anastomotic_leackage  \\\n",
       "0                    1.0                     0   \n",
       "1                    1.0                     0   \n",
       "2                    1.0                     0   \n",
       "3                    1.0                     0   \n",
       "4                    1.0                     0   \n",
       "...                  ...                   ...   \n",
       "5906                 1.0                     0   \n",
       "5907                 1.0                     0   \n",
       "5908                 1.0                     0   \n",
       "5909                 1.0                     0   \n",
       "5910                 NaN                     0   \n",
       "\n",
       "      BIHistoryOfIschaemicHeartDisease  BIHistoryOfDiabetes  \n",
       "0                                    0                    0  \n",
       "1                                    0                    0  \n",
       "2                                    0                    0  \n",
       "3                                    0                    0  \n",
       "4                                    1                    0  \n",
       "...                                ...                  ...  \n",
       "5906                                 0                    0  \n",
       "5907                                 0                    0  \n",
       "5908                                 0                    1  \n",
       "5909                                 0                    0  \n",
       "5910                                 0                    0  \n",
       "\n",
       "[5911 rows x 37 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "input_path = r'..\\data'\n",
    "input_filename = r'\\reduced_op_UK_merged_data_final_21062024.csv'\n",
    "df = pd.read_csv(input_path + input_filename , decimal = '.' , sep = ';')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1715801918340,
     "user": {
      "displayName": "Edgar David Varcarcel Trujillo",
      "userId": "04708129325083286457"
     },
     "user_tz": 300
    },
    "id": "czayG8DjZddE",
    "outputId": "f1e3a261-fb7a-48e2-d701-2235102047d3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "uk                    3041\n",
       "university_wrzburg     647\n",
       "university_dalhous     340\n",
       "university_of_east     314\n",
       "military_universit     294\n",
       "university_vilnius     234\n",
       "university_basel       226\n",
       "university_hamburg     220\n",
       "university_las_veg     173\n",
       "kantonspital_liest     106\n",
       "universitt_innsbru     103\n",
       "gzo_wetzikon            90\n",
       "emmental_hospital       67\n",
       "clarunisclaraspita      56\n",
       "Name: data_group, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show group distribution\n",
    "df['data_group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['clarunisclaraspita',\n",
       " 'emmental_hospital',\n",
       " 'gzo_wetzikon',\n",
       " 'kantonspital_liest',\n",
       " 'military_universit',\n",
       " 'uk',\n",
       " 'universitt_innsbru',\n",
       " 'university_basel',\n",
       " 'university_dalhous',\n",
       " 'university_hamburg',\n",
       " 'university_las_veg',\n",
       " 'university_of_east',\n",
       " 'university_vilnius',\n",
       " 'university_wrzburg']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save total of data groups\n",
    "clinics = df['data_group'].unique().tolist()\n",
    "clinics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1715801921370,
     "user": {
      "displayName": "Edgar David Varcarcel Trujillo",
      "userId": "04708129325083286457"
     },
     "user_tz": 300
    },
    "id": "FIrKEQ9IZksT",
    "outputId": "275e2afd-061c-422c-86ba-932e880352f7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['record_id',\n",
       " 'data_group',\n",
       " 'sex',\n",
       " 'age',\n",
       " 'bmi',\n",
       " 'active_smoking',\n",
       " 'pack_years',\n",
       " 'alcohol_abuse',\n",
       " 'real_function_ckd_stages',\n",
       " 'preoperative_hemoglobin_level',\n",
       " 'preoperative_leukocyte_count',\n",
       " 'preoperative_albumin_level',\n",
       " 'preoperative_crp_level',\n",
       " 'liver_metastasis_at_time_of_anastomosis',\n",
       " 'neoadjuvant_therapy',\n",
       " 'preoperative_use_of_immunosuppressive_drugs',\n",
       " 'preoperative_steroid_use',\n",
       " 'dosage_of_steroids',\n",
       " 'preoperative_nsaids_use',\n",
       " 'preoperative_blood_transfusion',\n",
       " 'tnf_alpha_inhib',\n",
       " 'asa_score',\n",
       " 'prior_abdominal_surgery',\n",
       " 'indication',\n",
       " 'operation',\n",
       " 'emergency_surgery',\n",
       " 'perforation',\n",
       " 'approach',\n",
       " 'conversion',\n",
       " 'type_of_anastomosis -> das von UK sind alles  Ileocolonic anastomosis',\n",
       " 'anastomotic_technique',\n",
       " 'anastomotic_configuration',\n",
       " 'protective_stomy',\n",
       " 'surgeon_experience',\n",
       " 'anastomotic_leackage',\n",
       " 'BIHistoryOfIschaemicHeartDisease',\n",
       " 'BIHistoryOfDiabetes']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print columns of the data\n",
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1715801924796,
     "user": {
      "displayName": "Edgar David Varcarcel Trujillo",
      "userId": "04708129325083286457"
     },
     "user_tz": 300
    },
    "id": "kOSaw6tI7yli"
   },
   "outputs": [],
   "source": [
    "# Drop columns to omit\n",
    "drop_columns = ['record_id']\n",
    "df = df.drop(columns = drop_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1715801925918,
     "user": {
      "displayName": "Edgar David Varcarcel Trujillo",
      "userId": "04708129325083286457"
     },
     "user_tz": 300
    },
    "id": "BP2nbR9WB3Yc"
   },
   "outputs": [],
   "source": [
    "# Define target variable\n",
    "TARGET = ['anastomotic_leackage']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kUsNFqsICrC1"
   },
   "source": [
    "# **1. NAN Handle**\n",
    "\n",
    "Here is defined the process of how to handle nan values.\n",
    "\n",
    "1. If DROP_ALL, then all rows with nans are removed \n",
    "2. If MASK_ALL, then all rows with nans are masked with -1\n",
    "3. If DROP_PERCENT, then only features with a percent of missing are dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>bmi</th>\n",
       "      <th>active_smoking</th>\n",
       "      <th>pack_years</th>\n",
       "      <th>alcohol_abuse</th>\n",
       "      <th>real_function_ckd_stages</th>\n",
       "      <th>preoperative_hemoglobin_level</th>\n",
       "      <th>preoperative_leukocyte_count</th>\n",
       "      <th>preoperative_albumin_level</th>\n",
       "      <th>...</th>\n",
       "      <th>conversion</th>\n",
       "      <th>type_of_anastomosis -&gt; das von UK sind alles  Ileocolonic anastomosis</th>\n",
       "      <th>anastomotic_technique</th>\n",
       "      <th>anastomotic_configuration</th>\n",
       "      <th>protective_stomy</th>\n",
       "      <th>surgeon_experience</th>\n",
       "      <th>anastomotic_leackage</th>\n",
       "      <th>BIHistoryOfIschaemicHeartDisease</th>\n",
       "      <th>BIHistoryOfDiabetes</th>\n",
       "      <th>data_group_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.6</td>\n",
       "      <td>6.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.3</td>\n",
       "      <td>12.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>18.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.7</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>22.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.9</td>\n",
       "      <td>8.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>17.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.7</td>\n",
       "      <td>8.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5906</th>\n",
       "      <td>2.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>20.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.6</td>\n",
       "      <td>4.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5907</th>\n",
       "      <td>2.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5908</th>\n",
       "      <td>2.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>22.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.7</td>\n",
       "      <td>9.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5909</th>\n",
       "      <td>1.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>24.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5910</th>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5911 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sex   age   bmi  active_smoking  pack_years  alcohol_abuse  \\\n",
       "0     1.0  87.0   NaN             1.0        25.0            2.0   \n",
       "1     1.0  53.0  19.2             1.0        35.0            3.0   \n",
       "2     2.0  71.0  18.8             0.0         NaN            3.0   \n",
       "3     1.0  39.0  22.4             0.0         0.0            3.0   \n",
       "4     1.0  67.0  17.2             1.0        75.0            3.0   \n",
       "...   ...   ...   ...             ...         ...            ...   \n",
       "5906  2.0  24.0  20.4             0.0         NaN            3.0   \n",
       "5907  2.0  87.0  32.0             0.0         NaN            3.0   \n",
       "5908  2.0  74.0  22.7             1.0        20.0            3.0   \n",
       "5909  1.0  85.0  24.3             0.0         NaN            3.0   \n",
       "5910  2.0   NaN  14.8             0.0         NaN            3.0   \n",
       "\n",
       "      real_function_ckd_stages  preoperative_hemoglobin_level  \\\n",
       "0                          5.0                            9.6   \n",
       "1                          1.0                           11.3   \n",
       "2                          1.0                           13.7   \n",
       "3                          2.0                           11.9   \n",
       "4                          1.0                            7.7   \n",
       "...                        ...                            ...   \n",
       "5906                       1.0                           13.6   \n",
       "5907                       4.0                            NaN   \n",
       "5908                       1.0                            9.7   \n",
       "5909                       2.0                           13.2   \n",
       "5910                       NaN                            NaN   \n",
       "\n",
       "      preoperative_leukocyte_count  preoperative_albumin_level  ...  \\\n",
       "0                              6.4                         0.0  ...   \n",
       "1                             12.4                         0.0  ...   \n",
       "2                              9.0                         0.0  ...   \n",
       "3                              8.8                         0.0  ...   \n",
       "4                              8.6                         0.0  ...   \n",
       "...                            ...                         ...  ...   \n",
       "5906                           4.9                         NaN  ...   \n",
       "5907                           NaN                         NaN  ...   \n",
       "5908                           9.2                         NaN  ...   \n",
       "5909                           4.7                         NaN  ...   \n",
       "5910                          13.4                         NaN  ...   \n",
       "\n",
       "      conversion  \\\n",
       "0            NaN   \n",
       "1            NaN   \n",
       "2            0.0   \n",
       "3            0.0   \n",
       "4            NaN   \n",
       "...          ...   \n",
       "5906         NaN   \n",
       "5907         NaN   \n",
       "5908         NaN   \n",
       "5909         NaN   \n",
       "5910         1.0   \n",
       "\n",
       "      type_of_anastomosis -> das von UK sind alles  Ileocolonic anastomosis  \\\n",
       "0                                                   3.0                       \n",
       "1                                                   3.0                       \n",
       "2                                                   3.0                       \n",
       "3                                                   3.0                       \n",
       "4                                                   3.0                       \n",
       "...                                                 ...                       \n",
       "5906                                                3.0                       \n",
       "5907                                                3.0                       \n",
       "5908                                                3.0                       \n",
       "5909                                                3.0                       \n",
       "5910                                                3.0                       \n",
       "\n",
       "      anastomotic_technique  anastomotic_configuration  protective_stomy  \\\n",
       "0                       2.0                        1.0               3.0   \n",
       "1                       1.0                        3.0               3.0   \n",
       "2                       NaN                        1.0               3.0   \n",
       "3                       1.0                        3.0               3.0   \n",
       "4                       1.0                        3.0               3.0   \n",
       "...                     ...                        ...               ...   \n",
       "5906                    2.0                        2.0               3.0   \n",
       "5907                    2.0                        2.0               3.0   \n",
       "5908                    1.0                        3.0               3.0   \n",
       "5909                    2.0                        2.0               3.0   \n",
       "5910                    2.0                        NaN               3.0   \n",
       "\n",
       "      surgeon_experience  anastomotic_leackage  \\\n",
       "0                    1.0                     0   \n",
       "1                    1.0                     0   \n",
       "2                    1.0                     0   \n",
       "3                    1.0                     0   \n",
       "4                    1.0                     0   \n",
       "...                  ...                   ...   \n",
       "5906                 1.0                     0   \n",
       "5907                 1.0                     0   \n",
       "5908                 1.0                     0   \n",
       "5909                 1.0                     0   \n",
       "5910                 NaN                     0   \n",
       "\n",
       "      BIHistoryOfIschaemicHeartDisease  BIHistoryOfDiabetes  \\\n",
       "0                                    0                    0   \n",
       "1                                    0                    0   \n",
       "2                                    0                    0   \n",
       "3                                    0                    0   \n",
       "4                                    1                    0   \n",
       "...                                ...                  ...   \n",
       "5906                                 0                    0   \n",
       "5907                                 0                    0   \n",
       "5908                                 0                    1   \n",
       "5909                                 0                    0   \n",
       "5910                                 0                    0   \n",
       "\n",
       "      data_group_encoded  \n",
       "0                      0  \n",
       "1                      0  \n",
       "2                      0  \n",
       "3                      0  \n",
       "4                      0  \n",
       "...                  ...  \n",
       "5906                  13  \n",
       "5907                  13  \n",
       "5908                  13  \n",
       "5909                  13  \n",
       "5910                  13  \n",
       "\n",
       "[5911 rows x 36 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode data_group column\n",
    "if 'data_group' in df.columns.tolist():\n",
    "    clinics_2 = df['data_group'].unique().tolist()\n",
    "    code_clinics = {v : k for k , v in enumerate(clinics_2)}\n",
    "    df['data_group_encoded'] = df['data_group'].map(code_clinics)\n",
    "    df = df.drop(columns = ['data_group'])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing data using KNN\n",
      "                                                           0\n",
      "pack_years                                          0.703265\n",
      "preoperative_albumin_level                          0.669768\n",
      "liver_metastasis_at_time_of_anastomosis             0.588733\n",
      "tnf_alpha_inhib                                     0.582643\n",
      "preoperative_nsaids_use                             0.577736\n",
      "real_function_ckd_stages                            0.570969\n",
      "preoperative_leukocyte_count                        0.566402\n",
      "alcohol_abuse                                       0.553037\n",
      "protective_stomy                                    0.536457\n",
      "preoperative_blood_transfusion                      0.507528\n",
      "dosage_of_steroids                                  0.506513\n",
      "preoperative_use_of_immunosuppressive_drugs         0.460497\n",
      "preoperative_steroid_use                            0.459313\n",
      "perforation                                         0.416173\n",
      "conversion                                          0.402977\n",
      "preoperative_crp_level                              0.348334\n",
      "anastomotic_configuration                           0.255287\n",
      "neoadjuvant_therapy                                 0.160041\n",
      "bmi                                                 0.074945\n",
      "preoperative_hemoglobin_level                       0.062595\n",
      "surgeon_experience                                  0.057858\n",
      "active_smoking                                      0.046016\n",
      "anastomotic_technique                               0.030113\n",
      "prior_abdominal_surgery                             0.026222\n",
      "approach                                            0.016410\n",
      "asa_score                                           0.006598\n",
      "type_of_anastomosis -> das von UK sind alles  I...  0.003722\n",
      "age                                                 0.001184\n",
      "emergency_surgery                                   0.000677\n",
      "sex                                                 0.000169\n",
      "operation                                           0.000000\n",
      "indication                                          0.000000\n",
      "anastomotic_leackage                                0.000000\n",
      "BIHistoryOfIschaemicHeartDisease                    0.000000\n",
      "BIHistoryOfDiabetes                                 0.000000\n",
      "data_group_encoded                                  0.000000\n",
      "Columns deleted by missing values: ['pack_years', 'preoperative_albumin_level', 'liver_metastasis_at_time_of_anastomosis', 'tnf_alpha_inhib', 'preoperative_nsaids_use', 'real_function_ckd_stages', 'preoperative_leukocyte_count', 'alcohol_abuse', 'protective_stomy', 'preoperative_blood_transfusion', 'dosage_of_steroids', 'preoperative_use_of_immunosuppressive_drugs', 'preoperative_steroid_use', 'perforation', 'conversion', 'preoperative_crp_level']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>bmi</th>\n",
       "      <th>active_smoking</th>\n",
       "      <th>pack_years</th>\n",
       "      <th>alcohol_abuse</th>\n",
       "      <th>real_function_ckd_stages</th>\n",
       "      <th>preoperative_hemoglobin_level</th>\n",
       "      <th>preoperative_leukocyte_count</th>\n",
       "      <th>preoperative_albumin_level</th>\n",
       "      <th>...</th>\n",
       "      <th>conversion</th>\n",
       "      <th>type_of_anastomosis -&gt; das von UK sind alles  Ileocolonic anastomosis</th>\n",
       "      <th>anastomotic_technique</th>\n",
       "      <th>anastomotic_configuration</th>\n",
       "      <th>protective_stomy</th>\n",
       "      <th>surgeon_experience</th>\n",
       "      <th>anastomotic_leackage</th>\n",
       "      <th>BIHistoryOfIschaemicHeartDisease</th>\n",
       "      <th>BIHistoryOfDiabetes</th>\n",
       "      <th>data_group_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>23.96</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.60</td>\n",
       "      <td>6.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>19.20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.30</td>\n",
       "      <td>12.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>18.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.70</td>\n",
       "      <td>9.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>22.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.90</td>\n",
       "      <td>8.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>17.20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.70</td>\n",
       "      <td>8.60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5906</th>\n",
       "      <td>2.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>20.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.60</td>\n",
       "      <td>4.90</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5907</th>\n",
       "      <td>2.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>32.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.61</td>\n",
       "      <td>7.27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5908</th>\n",
       "      <td>2.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>22.70</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.70</td>\n",
       "      <td>9.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5909</th>\n",
       "      <td>1.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>24.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.20</td>\n",
       "      <td>4.70</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5910</th>\n",
       "      <td>2.0</td>\n",
       "      <td>76.2</td>\n",
       "      <td>14.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>11.01</td>\n",
       "      <td>13.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5911 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sex   age    bmi  active_smoking  pack_years  alcohol_abuse  \\\n",
       "0     1.0  87.0  23.96             1.0        25.0            2.0   \n",
       "1     1.0  53.0  19.20             1.0        35.0            3.0   \n",
       "2     2.0  71.0  18.80             0.0        20.7            3.0   \n",
       "3     1.0  39.0  22.40             0.0         0.0            3.0   \n",
       "4     1.0  67.0  17.20             1.0        75.0            3.0   \n",
       "...   ...   ...    ...             ...         ...            ...   \n",
       "5906  2.0  24.0  20.40             0.0         1.0            3.0   \n",
       "5907  2.0  87.0  32.00             0.0        20.3            3.0   \n",
       "5908  2.0  74.0  22.70             1.0        20.0            3.0   \n",
       "5909  1.0  85.0  24.30             0.0        15.9            3.0   \n",
       "5910  2.0  76.2  14.80             0.0        17.5            3.0   \n",
       "\n",
       "      real_function_ckd_stages  preoperative_hemoglobin_level  \\\n",
       "0                          5.0                           9.60   \n",
       "1                          1.0                          11.30   \n",
       "2                          1.0                          13.70   \n",
       "3                          2.0                          11.90   \n",
       "4                          1.0                           7.70   \n",
       "...                        ...                            ...   \n",
       "5906                       1.0                          13.60   \n",
       "5907                       4.0                           9.61   \n",
       "5908                       1.0                           9.70   \n",
       "5909                       2.0                          13.20   \n",
       "5910                       1.8                          11.01   \n",
       "\n",
       "      preoperative_leukocyte_count  preoperative_albumin_level  ...  \\\n",
       "0                             6.40                         0.0  ...   \n",
       "1                            12.40                         0.0  ...   \n",
       "2                             9.00                         0.0  ...   \n",
       "3                             8.80                         0.0  ...   \n",
       "4                             8.60                         0.0  ...   \n",
       "...                            ...                         ...  ...   \n",
       "5906                          4.90                         0.0  ...   \n",
       "5907                          7.27                         0.0  ...   \n",
       "5908                          9.20                         0.0  ...   \n",
       "5909                          4.70                         0.0  ...   \n",
       "5910                         13.40                         0.0  ...   \n",
       "\n",
       "      conversion  \\\n",
       "0            0.5   \n",
       "1            0.5   \n",
       "2            0.0   \n",
       "3            0.0   \n",
       "4            0.6   \n",
       "...          ...   \n",
       "5906         0.2   \n",
       "5907         0.4   \n",
       "5908         0.5   \n",
       "5909         0.5   \n",
       "5910         1.0   \n",
       "\n",
       "      type_of_anastomosis -> das von UK sind alles  Ileocolonic anastomosis  \\\n",
       "0                                                   3.0                       \n",
       "1                                                   3.0                       \n",
       "2                                                   3.0                       \n",
       "3                                                   3.0                       \n",
       "4                                                   3.0                       \n",
       "...                                                 ...                       \n",
       "5906                                                3.0                       \n",
       "5907                                                3.0                       \n",
       "5908                                                3.0                       \n",
       "5909                                                3.0                       \n",
       "5910                                                3.0                       \n",
       "\n",
       "      anastomotic_technique  anastomotic_configuration  protective_stomy  \\\n",
       "0                       2.0                        1.0               3.0   \n",
       "1                       1.0                        3.0               3.0   \n",
       "2                       1.7                        1.0               3.0   \n",
       "3                       1.0                        3.0               3.0   \n",
       "4                       1.0                        3.0               3.0   \n",
       "...                     ...                        ...               ...   \n",
       "5906                    2.0                        2.0               3.0   \n",
       "5907                    2.0                        2.0               3.0   \n",
       "5908                    1.0                        3.0               3.0   \n",
       "5909                    2.0                        2.0               3.0   \n",
       "5910                    2.0                        2.8               3.0   \n",
       "\n",
       "      surgeon_experience  anastomotic_leackage  \\\n",
       "0                    1.0                   0.0   \n",
       "1                    1.0                   0.0   \n",
       "2                    1.0                   0.0   \n",
       "3                    1.0                   0.0   \n",
       "4                    1.0                   0.0   \n",
       "...                  ...                   ...   \n",
       "5906                 1.0                   0.0   \n",
       "5907                 1.0                   0.0   \n",
       "5908                 1.0                   0.0   \n",
       "5909                 1.0                   0.0   \n",
       "5910                 1.0                   0.0   \n",
       "\n",
       "      BIHistoryOfIschaemicHeartDisease  BIHistoryOfDiabetes  \\\n",
       "0                                  0.0                  0.0   \n",
       "1                                  0.0                  0.0   \n",
       "2                                  0.0                  0.0   \n",
       "3                                  0.0                  0.0   \n",
       "4                                  1.0                  0.0   \n",
       "...                                ...                  ...   \n",
       "5906                               0.0                  0.0   \n",
       "5907                               0.0                  0.0   \n",
       "5908                               0.0                  1.0   \n",
       "5909                               0.0                  0.0   \n",
       "5910                               0.0                  0.0   \n",
       "\n",
       "      data_group_encoded  \n",
       "0                    0.0  \n",
       "1                    0.0  \n",
       "2                    0.0  \n",
       "3                    0.0  \n",
       "4                    0.0  \n",
       "...                  ...  \n",
       "5906                13.0  \n",
       "5907                13.0  \n",
       "5908                13.0  \n",
       "5909                13.0  \n",
       "5910                13.0  \n",
       "\n",
       "[5911 rows x 36 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if GLOBAL_PARAMETERS['NAN_HANDLE'] == 'MASK_ALL':\n",
    "    print('All features masked as -1')\n",
    "    df = df.fillna(-1)\n",
    "if GLOBAL_PARAMETERS['NAN_HANDLE'] == 'DROP_ALL':\n",
    "    percent_of_missing = pd.DataFrame((df.replace(-1 , np.nan).isnull().sum() / df.shape[0]).sort_values(ascending = False))\n",
    "    print(percent_of_missing)\n",
    "    to_drop_missing_columns = percent_of_missing[percent_of_missing[0] > 0.2].index.tolist()\n",
    "    print('Columns deleted by missing values:' , to_drop_missing_columns)\n",
    "    df = df.drop(columns = to_drop_missing_columns)\n",
    "    df = df.dropna()\n",
    "    print('Rows with nan dropped')\n",
    "if GLOBAL_PARAMETERS['NAN_HANDLE'] == 'DROP_PERCENT':\n",
    "    print('Maintaining features with certain percent of NaNs')\n",
    "    percent_of_missing = pd.DataFrame((df.replace(-1 , np.nan).isnull().sum() / df.shape[0]).sort_values(ascending = False))\n",
    "    print(percent_of_missing)\n",
    "    to_drop_missing_columns = percent_of_missing[percent_of_missing[0] > 0.2].index.tolist()\n",
    "    print('Columns deleted by missing values:' , to_drop_missing_columns)\n",
    "    df = df.drop(columns = to_drop_missing_columns)\n",
    "if GLOBAL_PARAMETERS['NAN_HANDLE'] == 'KNN':\n",
    "    print('Imputing data using KNN')\n",
    "    percent_of_missing = pd.DataFrame((df.replace(-1 , np.nan).isnull().sum() / df.shape[0]).sort_values(ascending = False))\n",
    "    print(percent_of_missing)\n",
    "    to_drop_missing_columns = percent_of_missing[percent_of_missing[0] > 0.3].index.tolist()\n",
    "    print('Columns deleted by missing values:' , to_drop_missing_columns)\n",
    "    imputer = KNNImputer(n_neighbors = 10)\n",
    "    df = pd.DataFrame(imputer.fit_transform(df) , columns = df.columns.tolist())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object saved --> ..\\models\\KNN_Imputer.sav\n"
     ]
    }
   ],
   "source": [
    "# Save imputer\n",
    "imputer_path = r'..\\models\\KNN_Imputer.sav'\n",
    "pickle.dump(imputer , open(imputer_path , 'wb'))\n",
    "print('Object saved -->' , imputer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1715801934259,
     "user": {
      "displayName": "Edgar David Varcarcel Trujillo",
      "userId": "04708129325083286457"
     },
     "user_tz": 300
    },
    "id": "sRxQj_luDbsE",
    "outputId": "5bd6d28a-dc22-4d15-e835-86249824ec4b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sex',\n",
       " 'active_smoking',\n",
       " 'alcohol_abuse',\n",
       " 'liver_metastasis_at_time_of_anastomosis',\n",
       " 'neoadjuvant_therapy',\n",
       " 'preoperative_use_of_immunosuppressive_drugs',\n",
       " 'preoperative_steroid_use',\n",
       " 'preoperative_nsaids_use',\n",
       " 'preoperative_blood_transfusion',\n",
       " 'tnf_alpha_inhib',\n",
       " 'prior_abdominal_surgery',\n",
       " 'indication',\n",
       " 'operation',\n",
       " 'emergency_surgery',\n",
       " 'perforation',\n",
       " 'approach',\n",
       " 'conversion',\n",
       " 'type_of_anastomosis -> das von UK sind alles  Ileocolonic anastomosis',\n",
       " 'anastomotic_technique',\n",
       " 'anastomotic_configuration',\n",
       " 'protective_stomy',\n",
       " 'BIHistoryOfIschaemicHeartDisease',\n",
       " 'BIHistoryOfDiabetes',\n",
       " 'data_group_encoded']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define numeric and categorical columns\n",
    "num_columns = ['age' , 'bmi' , 'pack_years' , 'preoperative_hemoglobin_level' , 'preoperative_leukocyte_count',\n",
    "              'preoperative_albumin_level' , 'preoperative_crp_level' , 'dosage_of_steroids']\n",
    "num_columns = [i for i in num_columns if i in df.columns.tolist()]\n",
    "ordinal_columns = ['real_function_ckd_stages' , 'charlson_comorbidity_index' ,\n",
    "                  'asa_score' , 'surgeon_experience']\n",
    "ordinal_columns = [i for i in ordinal_columns if i in df.columns.tolist()]\n",
    "cat_columns = df.drop(columns = num_columns + TARGET + ordinal_columns).columns.tolist()\n",
    "cat_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a73RK0mqKxXV",
    "tags": []
   },
   "source": [
    "# **2. Scaling**\n",
    "1. If YES, then MinMaxScaler is used to numeric features.\n",
    "2. If NO, then no scaling is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling performed\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>bmi</th>\n",
       "      <th>pack_years</th>\n",
       "      <th>preoperative_hemoglobin_level</th>\n",
       "      <th>preoperative_leukocyte_count</th>\n",
       "      <th>preoperative_albumin_level</th>\n",
       "      <th>preoperative_crp_level</th>\n",
       "      <th>dosage_of_steroids</th>\n",
       "      <th>real_function_ckd_stages</th>\n",
       "      <th>asa_score</th>\n",
       "      <th>...</th>\n",
       "      <th>approach</th>\n",
       "      <th>conversion</th>\n",
       "      <th>type_of_anastomosis -&gt; das von UK sind alles  Ileocolonic anastomosis</th>\n",
       "      <th>anastomotic_technique</th>\n",
       "      <th>anastomotic_configuration</th>\n",
       "      <th>protective_stomy</th>\n",
       "      <th>BIHistoryOfIschaemicHeartDisease</th>\n",
       "      <th>BIHistoryOfDiabetes</th>\n",
       "      <th>data_group_encoded</th>\n",
       "      <th>anastomotic_leackage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.835294</td>\n",
       "      <td>0.212811</td>\n",
       "      <td>0.004630</td>\n",
       "      <td>0.377119</td>\n",
       "      <td>0.047077</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013769</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.9</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.435294</td>\n",
       "      <td>0.128114</td>\n",
       "      <td>0.006481</td>\n",
       "      <td>0.449153</td>\n",
       "      <td>0.092635</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010327</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.120996</td>\n",
       "      <td>0.003833</td>\n",
       "      <td>0.550847</td>\n",
       "      <td>0.066819</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008606</td>\n",
       "      <td>0.04</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.270588</td>\n",
       "      <td>0.185053</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.474576</td>\n",
       "      <td>0.065300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.049914</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.092527</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.296610</td>\n",
       "      <td>0.063781</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.080895</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5906</th>\n",
       "      <td>0.094118</td>\n",
       "      <td>0.149466</td>\n",
       "      <td>0.000185</td>\n",
       "      <td>0.546610</td>\n",
       "      <td>0.035687</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009122</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5907</th>\n",
       "      <td>0.835294</td>\n",
       "      <td>0.355872</td>\n",
       "      <td>0.003759</td>\n",
       "      <td>0.377542</td>\n",
       "      <td>0.053683</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.044647</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5908</th>\n",
       "      <td>0.682353</td>\n",
       "      <td>0.190391</td>\n",
       "      <td>0.003704</td>\n",
       "      <td>0.381356</td>\n",
       "      <td>0.068337</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.053924</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5909</th>\n",
       "      <td>0.811765</td>\n",
       "      <td>0.218861</td>\n",
       "      <td>0.002944</td>\n",
       "      <td>0.529661</td>\n",
       "      <td>0.034169</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.033046</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5910</th>\n",
       "      <td>0.708235</td>\n",
       "      <td>0.049822</td>\n",
       "      <td>0.003241</td>\n",
       "      <td>0.436864</td>\n",
       "      <td>0.100228</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001549</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5911 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           age       bmi  pack_years  preoperative_hemoglobin_level  \\\n",
       "0     0.835294  0.212811    0.004630                       0.377119   \n",
       "1     0.435294  0.128114    0.006481                       0.449153   \n",
       "2     0.647059  0.120996    0.003833                       0.550847   \n",
       "3     0.270588  0.185053    0.000000                       0.474576   \n",
       "4     0.600000  0.092527    0.013889                       0.296610   \n",
       "...        ...       ...         ...                            ...   \n",
       "5906  0.094118  0.149466    0.000185                       0.546610   \n",
       "5907  0.835294  0.355872    0.003759                       0.377542   \n",
       "5908  0.682353  0.190391    0.003704                       0.381356   \n",
       "5909  0.811765  0.218861    0.002944                       0.529661   \n",
       "5910  0.708235  0.049822    0.003241                       0.436864   \n",
       "\n",
       "      preoperative_leukocyte_count  preoperative_albumin_level  \\\n",
       "0                         0.047077                         0.0   \n",
       "1                         0.092635                         0.0   \n",
       "2                         0.066819                         0.0   \n",
       "3                         0.065300                         0.0   \n",
       "4                         0.063781                         0.0   \n",
       "...                            ...                         ...   \n",
       "5906                      0.035687                         0.0   \n",
       "5907                      0.053683                         0.0   \n",
       "5908                      0.068337                         0.0   \n",
       "5909                      0.034169                         0.0   \n",
       "5910                      0.100228                         0.0   \n",
       "\n",
       "      preoperative_crp_level  dosage_of_steroids  real_function_ckd_stages  \\\n",
       "0                   0.013769                0.00                       5.0   \n",
       "1                   0.010327                0.00                       1.0   \n",
       "2                   0.008606                0.04                       1.0   \n",
       "3                   0.049914                0.00                       2.0   \n",
       "4                   0.080895                0.00                       1.0   \n",
       "...                      ...                 ...                       ...   \n",
       "5906                0.009122                0.00                       1.0   \n",
       "5907                0.044647                0.00                       4.0   \n",
       "5908                0.053924                0.00                       1.0   \n",
       "5909                0.033046                0.00                       2.0   \n",
       "5910                0.001549                0.00                       1.8   \n",
       "\n",
       "      asa_score  ...  approach  conversion  \\\n",
       "0           2.9  ...       1.0         0.5   \n",
       "1           2.0  ...       3.0         0.5   \n",
       "2           4.0  ...       3.0         0.0   \n",
       "3           2.0  ...       3.0         0.0   \n",
       "4           4.0  ...       3.0         0.6   \n",
       "...         ...  ...       ...         ...   \n",
       "5906        2.0  ...       1.0         0.2   \n",
       "5907        3.0  ...       3.0         0.4   \n",
       "5908        3.0  ...       3.0         0.5   \n",
       "5909        3.0  ...       3.0         0.5   \n",
       "5910        1.0  ...       2.8         1.0   \n",
       "\n",
       "      type_of_anastomosis -> das von UK sind alles  Ileocolonic anastomosis  \\\n",
       "0                                                   3.0                       \n",
       "1                                                   3.0                       \n",
       "2                                                   3.0                       \n",
       "3                                                   3.0                       \n",
       "4                                                   3.0                       \n",
       "...                                                 ...                       \n",
       "5906                                                3.0                       \n",
       "5907                                                3.0                       \n",
       "5908                                                3.0                       \n",
       "5909                                                3.0                       \n",
       "5910                                                3.0                       \n",
       "\n",
       "      anastomotic_technique  anastomotic_configuration  protective_stomy  \\\n",
       "0                       2.0                        1.0               3.0   \n",
       "1                       1.0                        3.0               3.0   \n",
       "2                       1.7                        1.0               3.0   \n",
       "3                       1.0                        3.0               3.0   \n",
       "4                       1.0                        3.0               3.0   \n",
       "...                     ...                        ...               ...   \n",
       "5906                    2.0                        2.0               3.0   \n",
       "5907                    2.0                        2.0               3.0   \n",
       "5908                    1.0                        3.0               3.0   \n",
       "5909                    2.0                        2.0               3.0   \n",
       "5910                    2.0                        2.8               3.0   \n",
       "\n",
       "      BIHistoryOfIschaemicHeartDisease  BIHistoryOfDiabetes  \\\n",
       "0                                  0.0                  0.0   \n",
       "1                                  0.0                  0.0   \n",
       "2                                  0.0                  0.0   \n",
       "3                                  0.0                  0.0   \n",
       "4                                  1.0                  0.0   \n",
       "...                                ...                  ...   \n",
       "5906                               0.0                  0.0   \n",
       "5907                               0.0                  0.0   \n",
       "5908                               0.0                  1.0   \n",
       "5909                               0.0                  0.0   \n",
       "5910                               0.0                  0.0   \n",
       "\n",
       "      data_group_encoded  anastomotic_leackage  \n",
       "0                    0.0                   0.0  \n",
       "1                    0.0                   0.0  \n",
       "2                    0.0                   0.0  \n",
       "3                    0.0                   0.0  \n",
       "4                    0.0                   0.0  \n",
       "...                  ...                   ...  \n",
       "5906                13.0                   0.0  \n",
       "5907                13.0                   0.0  \n",
       "5908                13.0                   0.0  \n",
       "5909                13.0                   0.0  \n",
       "5910                13.0                   0.0  \n",
       "\n",
       "[5911 rows x 36 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if GLOBAL_PARAMETERS['PERFORM_SCALING'] == 'YES':\n",
    "    print('Scaling performed\\n')\n",
    "    # Create scaler and one hot encoding obects\n",
    "    numeric_scaler = MinMaxScaler()\n",
    "    # Fit objects\n",
    "    numeric_scaler.fit(df[num_columns])\n",
    "    # Transform data\n",
    "    aux_numeric = pd.DataFrame(numeric_scaler.transform(df[num_columns]) , columns = df[num_columns].columns.tolist())\n",
    "    # Concat data\n",
    "    df = pd.concat([aux_numeric,\n",
    "                    df[ordinal_columns].fillna(-1).reset_index(drop = True),\n",
    "                    df[cat_columns].reset_index(drop = True),\n",
    "                    df[TARGET].fillna(0).reset_index(drop = True)] , axis = 1)\n",
    "if GLOBAL_PARAMETERS['PERFORM_SCALING'] == 'NO':\n",
    "    print('No scaling performed\\n')\n",
    "    #for i in range(len(train_set)):\n",
    "        #train_set[i] = train_set[i].drop(columns = ['data_group']).fillna(-1)\n",
    "        #test_set[i] = test_set[i].drop(columns = ['data_group']).fillna(-1)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object saved --> ..\\models\\Scaler.sav\n"
     ]
    }
   ],
   "source": [
    "# Save Scaler\n",
    "scaler_path = r'..\\models\\Scaler.sav'\n",
    "pickle.dump(numeric_scaler , open(scaler_path , 'wb'))\n",
    "print('Object saved -->' , scaler_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3 Feature Selection**\n",
    "\n",
    "1. If XGBOOST, then a XGBClassifier is trained and select the top n% (default 80%) of the features\n",
    "2. If VARIANCE_THRESHOLD, then Scikit Learn Variance Threshold with default parameters is applied\n",
    "3. If F_CLASSIF, then ANOVA is applied.\n",
    "4. If RFE, then Recursive Feature Elimination is applied.\n",
    "5. If LASSO, then Lasso model is applied.\n",
    "6. If NONE, then no feature selection is applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed selection\n",
      "Selected Features : ['age', 'preoperative_albumin_level', 'bmi', 'preoperative_hemoglobin_level', 'pack_years', 'sex', 'neoadjuvant_therapy', 'preoperative_use_of_immunosuppressive_drugs', 'tnf_alpha_inhib', 'emergency_surgery', 'approach', 'anastomotic_leackage', 'preoperative_steroid_use', 'preoperative_nsaids_use', 'active_smoking', 'liver_metastasis_at_time_of_anastomosis', 'BIHistoryOfDiabetes', 'preoperative_blood_transfusion', 'perforation', 'anastomotic_technique', 'surgeon_experience', 'conversion', 'anastomotic_configuration', 'protective_stomy', 'BIHistoryOfIschaemicHeartDisease', 'alcohol_abuse', 'asa_score']\n"
     ]
    }
   ],
   "source": [
    "if GLOBAL_PARAMETERS['FEATURE_SELECTION'] == 'XGBOOST':\n",
    "    print('Feature Selection : XGBoost Feature Importance')\n",
    "    # Create XGB Model\n",
    "    feature_selection_model = XGBClassifier().fit(df.drop(columns = TARGET),\n",
    "                                                  df[TARGET])\n",
    "    # Extract feature importance from the model\n",
    "    feature_importances = pd.DataFrame({'Feature' : feature_selection_model.feature_names_in_.tolist(),\n",
    "                                       'Importance' : feature_selection_model.feature_importances_.tolist()}).sort_values(by = 'Importance' , ascending = False).reset_index(drop = True)\n",
    "    feature_importances['Cumulative_Importance'] = feature_importances['Importance'].cumsum()\n",
    "    print('Feature Importance for XGBoost:')\n",
    "    print(feature_importances)\n",
    "    # Select top % of features\n",
    "    threshold = 0.8\n",
    "    features_selected = feature_importances[feature_importances['Cumulative_Importance'] <= threshold]['Feature'].tolist()\n",
    "    print('Features Selected:' , features_selected)\n",
    "    df = df[features_selected + TARGET]\n",
    "if GLOBAL_PARAMETERS['FEATURE_SELECTION'] == 'VARIANCE_THRESHOLD':\n",
    "    print('Feature Selection : Variance Threshold')\n",
    "    # Create the object for selection\n",
    "    feature_selection_model = VarianceThreshold()\n",
    "    feature_selection_model.fit(df.drop(columns = TARGET),\n",
    "                                df[TARGET])\n",
    "    features_selected = feature_selection_model.feature_names_in_.tolist()\n",
    "    print('Features Selected:' , features_selected)\n",
    "    df = df[features_selected + TARGET]\n",
    "if GLOBAL_PARAMETERS['FEATURE_SELECTION'] == 'F_CLASSIF':\n",
    "    print('Feature Selection : ANOVA')\n",
    "    threshold = int(0.6 * df.drop(columns = TARGET).shape[1]) # % of features to maintain\n",
    "    print('Maintaining' , threshold , 'features')\n",
    "    feature_selection_model = SelectKBest(f_classif,\n",
    "                                          k=threshold)\n",
    "    feature_selection_model.fit(df.drop(columns = TARGET),\n",
    "                                df[TARGET])\n",
    "    features_selected = feature_selection_model.get_feature_names_out().tolist()\n",
    "    print('Features Selected:' , features_selected)\n",
    "    df = df[features_selected + TARGET]\n",
    "if GLOBAL_PARAMETERS['FEATURE_SELECTION'] == 'RFE':\n",
    "    print('Feature Selection : RFE')\n",
    "    threshold = int(0.6 * df.drop(columns = TARGET).shape[1]) # % of features to maintain\n",
    "    feature_selection_model = RFECV(\n",
    "                                    estimator = XGBClassifier(),\n",
    "                                    step = 1,\n",
    "                                    cv = StratifiedKFold(5),\n",
    "                                    scoring = \"f1_macro\",\n",
    "                                    min_features_to_select = threshold,\n",
    "                                    n_jobs = -1,\n",
    "                                )\n",
    "    feature_selection_model.fit(df.drop(columns = TARGET),\n",
    "                                df[TARGET])\n",
    "    features_selected = feature_selection_model.get_feature_names_out().tolist()\n",
    "    print('Features Selected:' , features_selected)\n",
    "    df = df[features_selected + TARGET]\n",
    "if GLOBAL_PARAMETERS['FEATURE_SELECTION'] == 'NONE':\n",
    "    print('No Feature Selection applied')\n",
    "if GLOBAL_PARAMETERS['FEATURE_SELECTION'] == 'FIX':\n",
    "    print('Fixed selection')\n",
    "    selected_features = ['age' ,'preoperative_albumin_level', 'bmi', 'preoperative_hemoglobin_level' ,'pack_years',\n",
    "                         'sex' , 'neoadjuvant_therapy' , 'preoperative_use_of_immunosuppressive_drugs' , 'tnf_alpha_inhib' , \n",
    "                         'emergency_surgery' , 'approach' , 'anastomotic_leackage' , 'preoperative_steroid_use' , 'preoperative_nsaids_use',\n",
    "                         'active_smoking' , 'liver_metastasis_at_time_of_anastomosis' , 'BIHistoryOfDiabetes' , 'preoperative_blood_transfusion' ,\n",
    "                         'perforation' , 'anastomotic_technique' , 'surgeon_experience' , 'conversion' , 'anastomotic_configuration' , 'protective_stomy' , \n",
    "                         'BIHistoryOfIschaemicHeartDisease' , 'alcohol_abuse' , 'asa_score']\n",
    "    valid_features = [i for i in selected_features if i in df.columns.tolist()]\n",
    "    df = df[valid_features]\n",
    "    \n",
    "print('Selected Features' , ':' , df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Numeric Features: [['age', 'bmi', 'pack_years', 'preoperative_hemoglobin_level', 'preoperative_albumin_level']]\n",
      "New Categrocial Features: [['sex', 'neoadjuvant_therapy', 'preoperative_use_of_immunosuppressive_drugs', 'tnf_alpha_inhib', 'emergency_surgery', 'approach', 'preoperative_steroid_use', 'preoperative_nsaids_use', 'active_smoking', 'liver_metastasis_at_time_of_anastomosis', 'BIHistoryOfDiabetes', 'preoperative_blood_transfusion', 'perforation', 'anastomotic_technique', 'surgeon_experience', 'conversion', 'anastomotic_configuration', 'protective_stomy', 'BIHistoryOfIschaemicHeartDisease', 'alcohol_abuse', 'asa_score']]\n"
     ]
    }
   ],
   "source": [
    "# Re define selected categorical and numerical columns\n",
    "new_num_columns = []\n",
    "new_cat_columns = []\n",
    "new_num_columns.append([i for i in num_columns if i in df.columns.tolist()])\n",
    "new_cat_columns.append([i for i in df.columns.tolist() if i not in new_num_columns[0] and i not in TARGET])\n",
    "print('New Numeric Features:' , new_num_columns)\n",
    "print('New Categrocial Features:' , new_cat_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Categorical Features Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Feature Hash\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col_0</th>\n",
       "      <th>col_1</th>\n",
       "      <th>col_2</th>\n",
       "      <th>col_3</th>\n",
       "      <th>col_4</th>\n",
       "      <th>col_5</th>\n",
       "      <th>col_6</th>\n",
       "      <th>col_7</th>\n",
       "      <th>col_8</th>\n",
       "      <th>col_9</th>\n",
       "      <th>age</th>\n",
       "      <th>preoperative_albumin_level</th>\n",
       "      <th>bmi</th>\n",
       "      <th>preoperative_hemoglobin_level</th>\n",
       "      <th>pack_years</th>\n",
       "      <th>anastomotic_leackage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.835294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.212811</td>\n",
       "      <td>0.377119</td>\n",
       "      <td>0.004630</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.435294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.128114</td>\n",
       "      <td>0.449153</td>\n",
       "      <td>0.006481</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.120996</td>\n",
       "      <td>0.550847</td>\n",
       "      <td>0.003833</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.270588</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.185053</td>\n",
       "      <td>0.474576</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.092527</td>\n",
       "      <td>0.296610</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5906</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.094118</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.149466</td>\n",
       "      <td>0.546610</td>\n",
       "      <td>0.000185</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5907</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.835294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.355872</td>\n",
       "      <td>0.377542</td>\n",
       "      <td>0.003759</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5908</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.682353</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.190391</td>\n",
       "      <td>0.381356</td>\n",
       "      <td>0.003704</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5909</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.811765</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.218861</td>\n",
       "      <td>0.529661</td>\n",
       "      <td>0.002944</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5910</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.708235</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.049822</td>\n",
       "      <td>0.436864</td>\n",
       "      <td>0.003241</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5911 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      col_0  col_1  col_2  col_3  col_4  col_5  col_6  col_7  col_8  col_9  \\\n",
       "0        12      0      3      0      0      1      0      0      5      0   \n",
       "1        11      0      2      0      0      4      0      0      4      0   \n",
       "2         9      0      2      0      1      3      0      1      5      0   \n",
       "3        10      0      2      1      0      4      0      0      4      0   \n",
       "4         6      0      2      0      0      4      0      0      8      1   \n",
       "...     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "5906     10      0      5      1      0      2      0      0      3      0   \n",
       "5907     11      0      4      0      0      4      0      1      1      0   \n",
       "5908     10      0      2      0      0      5      0      0      4      0   \n",
       "5909     12      0      3      0      0      4      0      0      2      0   \n",
       "5910     11      0      3      0      2      2      0      0      3      0   \n",
       "\n",
       "           age  preoperative_albumin_level       bmi  \\\n",
       "0     0.835294                         0.0  0.212811   \n",
       "1     0.435294                         0.0  0.128114   \n",
       "2     0.647059                         0.0  0.120996   \n",
       "3     0.270588                         0.0  0.185053   \n",
       "4     0.600000                         0.0  0.092527   \n",
       "...        ...                         ...       ...   \n",
       "5906  0.094118                         0.0  0.149466   \n",
       "5907  0.835294                         0.0  0.355872   \n",
       "5908  0.682353                         0.0  0.190391   \n",
       "5909  0.811765                         0.0  0.218861   \n",
       "5910  0.708235                         0.0  0.049822   \n",
       "\n",
       "      preoperative_hemoglobin_level  pack_years  anastomotic_leackage  \n",
       "0                          0.377119    0.004630                   0.0  \n",
       "1                          0.449153    0.006481                   0.0  \n",
       "2                          0.550847    0.003833                   0.0  \n",
       "3                          0.474576    0.000000                   0.0  \n",
       "4                          0.296610    0.013889                   0.0  \n",
       "...                             ...         ...                   ...  \n",
       "5906                       0.546610    0.000185                   0.0  \n",
       "5907                       0.377542    0.003759                   0.0  \n",
       "5908                       0.381356    0.003704                   0.0  \n",
       "5909                       0.529661    0.002944                   0.0  \n",
       "5910                       0.436864    0.003241                   0.0  \n",
       "\n",
       "[5911 rows x 16 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if GLOBAL_PARAMETERS['CATEGORICAL_FEATURES'] == 'NONE':\n",
    "    print('No categorical features processing')\n",
    "if GLOBAL_PARAMETERS['CATEGORICAL_FEATURES'] == 'HASH':\n",
    "    print('Using Feature Hash')\n",
    "    hasher = HashingEncoder(cols = new_cat_columns[0], n_components = 10).fit(df.drop(columns = TARGET) , df[TARGET])\n",
    "    df = pd.concat([hasher.transform(df.drop(columns = TARGET)),\n",
    "                    df[TARGET]] , axis = 1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object saved --> ..\\models\\Hasher.sav\n"
     ]
    }
   ],
   "source": [
    "# Save Hasher\n",
    "hasher_path = r'..\\models\\Hasher.sav'\n",
    "pickle.dump(hasher , open(hasher_path , 'wb'))\n",
    "print('Object saved -->' , hasher_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['sex',\n",
       "  'neoadjuvant_therapy',\n",
       "  'preoperative_use_of_immunosuppressive_drugs',\n",
       "  'tnf_alpha_inhib',\n",
       "  'emergency_surgery',\n",
       "  'approach',\n",
       "  'preoperative_steroid_use',\n",
       "  'preoperative_nsaids_use',\n",
       "  'active_smoking',\n",
       "  'liver_metastasis_at_time_of_anastomosis',\n",
       "  'BIHistoryOfDiabetes',\n",
       "  'preoperative_blood_transfusion',\n",
       "  'perforation',\n",
       "  'anastomotic_technique',\n",
       "  'surgeon_experience',\n",
       "  'conversion',\n",
       "  'anastomotic_configuration',\n",
       "  'protective_stomy',\n",
       "  'BIHistoryOfIschaemicHeartDisease',\n",
       "  'alcohol_abuse',\n",
       "  'asa_score']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_cat_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Sampling Method**\n",
    "\n",
    "Depending of the over/under sampling model is selected, then the object is applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling with ADASYN\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col_0</th>\n",
       "      <th>col_1</th>\n",
       "      <th>col_2</th>\n",
       "      <th>col_3</th>\n",
       "      <th>col_4</th>\n",
       "      <th>col_5</th>\n",
       "      <th>col_6</th>\n",
       "      <th>col_7</th>\n",
       "      <th>col_8</th>\n",
       "      <th>col_9</th>\n",
       "      <th>age</th>\n",
       "      <th>preoperative_albumin_level</th>\n",
       "      <th>bmi</th>\n",
       "      <th>preoperative_hemoglobin_level</th>\n",
       "      <th>pack_years</th>\n",
       "      <th>anastomotic_leackage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.835294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.212811</td>\n",
       "      <td>0.377119</td>\n",
       "      <td>0.004630</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.435294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.128114</td>\n",
       "      <td>0.449153</td>\n",
       "      <td>0.006481</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.120996</td>\n",
       "      <td>0.550847</td>\n",
       "      <td>0.003833</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.270588</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.185053</td>\n",
       "      <td>0.474576</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.092527</td>\n",
       "      <td>0.296610</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11016</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.806494</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.318263</td>\n",
       "      <td>0.380742</td>\n",
       "      <td>0.002526</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11017</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.766012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.305562</td>\n",
       "      <td>0.389642</td>\n",
       "      <td>0.003221</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11018</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.820435</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.323661</td>\n",
       "      <td>0.381727</td>\n",
       "      <td>0.002028</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11019</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.788817</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.194544</td>\n",
       "      <td>0.385523</td>\n",
       "      <td>0.001364</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11020</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.782585</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.314418</td>\n",
       "      <td>0.387255</td>\n",
       "      <td>0.002896</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11021 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       col_0  col_1  col_2  col_3  col_4  col_5  col_6  col_7  col_8  col_9  \\\n",
       "0         12      0      3      0      0      1      0      0      5      0   \n",
       "1         11      0      2      0      0      4      0      0      4      0   \n",
       "2          9      0      2      0      1      3      0      1      5      0   \n",
       "3         10      0      2      1      0      4      0      0      4      0   \n",
       "4          6      0      2      0      0      4      0      0      8      1   \n",
       "...      ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "11016      9      0      3      0      0      4      0      0      3      1   \n",
       "11017      9      0      3      0      0      4      0      0      3      1   \n",
       "11018      9      0      3      0      0      4      0      0      3      1   \n",
       "11019      9      0      3      0      0      4      0      0      3      1   \n",
       "11020      9      0      3      0      0      4      0      0      3      1   \n",
       "\n",
       "            age  preoperative_albumin_level       bmi  \\\n",
       "0      0.835294                         0.0  0.212811   \n",
       "1      0.435294                         0.0  0.128114   \n",
       "2      0.647059                         0.0  0.120996   \n",
       "3      0.270588                         0.0  0.185053   \n",
       "4      0.600000                         0.0  0.092527   \n",
       "...         ...                         ...       ...   \n",
       "11016  0.806494                         0.0  0.318263   \n",
       "11017  0.766012                         0.0  0.305562   \n",
       "11018  0.820435                         0.0  0.323661   \n",
       "11019  0.788817                         0.0  0.194544   \n",
       "11020  0.782585                         0.0  0.314418   \n",
       "\n",
       "       preoperative_hemoglobin_level  pack_years  anastomotic_leackage  \n",
       "0                           0.377119    0.004630                   0.0  \n",
       "1                           0.449153    0.006481                   0.0  \n",
       "2                           0.550847    0.003833                   0.0  \n",
       "3                           0.474576    0.000000                   0.0  \n",
       "4                           0.296610    0.013889                   0.0  \n",
       "...                              ...         ...                   ...  \n",
       "11016                       0.380742    0.002526                   1.0  \n",
       "11017                       0.389642    0.003221                   1.0  \n",
       "11018                       0.381727    0.002028                   1.0  \n",
       "11019                       0.385523    0.001364                   1.0  \n",
       "11020                       0.387255    0.002896                   1.0  \n",
       "\n",
       "[11021 rows x 16 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if GLOBAL_PARAMETERS['SAMPLING'] == 'SMOTE':\n",
    "    print('Oversampling with SMOTE')\n",
    "    X = df.drop(columns = TARGET)\n",
    "    Y =df[TARGET]\n",
    "    oversampler = SMOTE()\n",
    "    X_res , Y_res = oversampler.fit_resample(X , Y)\n",
    "    df = pd.concat([X_res ,\n",
    "                    Y_res] , axis = 1)\n",
    "if GLOBAL_PARAMETERS['SAMPLING'] == 'ADASYN':\n",
    "    print('Oversampling with ADASYN')\n",
    "    X = df.drop(columns = TARGET)\n",
    "    Y = df[TARGET]\n",
    "    oversampler = ADASYN()\n",
    "    X_res , Y_res = oversampler.fit_resample(X , Y)\n",
    "    df = pd.concat([X_res ,\n",
    "                    Y_res] , axis = 1)\n",
    "if GLOBAL_PARAMETERS['SAMPLING'] == 'BORDERLINE_SMOTE':\n",
    "    print('Oversampling with Borderline SMOTE')\n",
    "    X = df.drop(columns = TARGET)\n",
    "    Y = df[TARGET]\n",
    "    oversampler = BorderlineSMOTE(random_state=42)\n",
    "    X_res , Y_res = oversampler.fit_resample(X , Y)\n",
    "    df = pd.concat([X_res ,\n",
    "                    Y_res] , axis = 1)\n",
    "if GLOBAL_PARAMETERS['SAMPLING'] == 'NONE':\n",
    "    print('No oversampling applied')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aNv19lPIGT0V"
   },
   "source": [
    "# **5. Model Training and Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 503,
     "status": "ok",
     "timestamp": 1715802205303,
     "user": {
      "displayName": "Edgar David Varcarcel Trujillo",
      "userId": "04708129325083286457"
     },
     "user_tz": 300
    },
    "id": "Y2PTb7VLJ2H3"
   },
   "outputs": [],
   "source": [
    "# Define functions for evaluation metrics\n",
    "def calculate_confusion_matrix(true_labels, predicted_labels):\n",
    "    # Calculate confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(true_labels, predicted_labels).ravel()\n",
    "\n",
    "    return tn, tp, fp, fn\n",
    "\n",
    "# Define function that takes confusion matrix an compute required metrics\n",
    "def get_metrics(TN,TP,FP,FN):\n",
    "\n",
    "    acc = (TN+TP)/(TN+TP+FN+FP)\n",
    "    precision = TP / (TP + FP)\n",
    "\n",
    "    # Calculate recall\n",
    "    recall = TP / (TP + FN)\n",
    "    # Calulcate specificity\n",
    "    specificity = TN / (TN + FP)\n",
    "\n",
    "    # Calculate False Negative Rate\n",
    "    FNR = FN / (FN + TP)\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    return acc, precision, recall, f1 , specificity , FNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 460,
     "status": "ok",
     "timestamp": 1715802207533,
     "user": {
      "displayName": "Edgar David Varcarcel Trujillo",
      "userId": "04708129325083286457"
     },
     "user_tz": 300
    },
    "id": "CEYT3xz8SSgx"
   },
   "outputs": [],
   "source": [
    "# Function to smooth the ROC curve using a moving average\n",
    "def smooth_roc_curve(fpr, tpr, window_size=5):\n",
    "    df = pd.DataFrame({'fpr': fpr, 'tpr': tpr})\n",
    "    df_smoothed = df.rolling(window=window_size).mean().dropna()\n",
    "    # Add (1,1) point at the end\n",
    "    df_smoothed = pd.concat([df_smoothed,\n",
    "                             pd.DataFrame({'fpr' : [1] , 'tpr' : [1]})] , axis = 0)\n",
    "    return df_smoothed['fpr'].values, df_smoothed['tpr'].values\n",
    "\n",
    "\n",
    "# Function to smooth the Precision Recall curve using a moving average\n",
    "def smooth_precision_curve(precision, recall, window_size=5):\n",
    "    df = pd.DataFrame({'precision': precision, 'recall': recall})\n",
    "    df_smoothed = df.rolling(window=window_size).mean().dropna()\n",
    "    # Add (1,0) point at the end\n",
    "    df_smoothed = pd.concat([df_smoothed,\n",
    "                             pd.DataFrame({'precision' : [1] , 'recall' : [0]})] , axis = 0)\n",
    "    return df_smoothed['precision'].values, df_smoothed['recall'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define fully conected model\n",
    "class FullyConnectedModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, dropout_prob=0.5):\n",
    "        super(FullyConnectedModel, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.output_size = output_size\n",
    "        self.dropout_prob = dropout_prob\n",
    "\n",
    "        layers = []\n",
    "        # Create the hidden layers with linear, batch normalization, and dropout\n",
    "        prev_size = input_size\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            layers.append(nn.BatchNorm1d(hidden_size))\n",
    "            layers.append(nn.Dropout(p=self.dropout_prob))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_size = hidden_size\n",
    "\n",
    "        # Output layer with softmax activation\n",
    "        layers.append(nn.Linear(prev_size, output_size))\n",
    "        layers.append(nn.Softmax(dim = 1))\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Define function to save pytorch model for early stopping\n",
    "def checkpoint(model, filename):\n",
    "    torch.save(model.state_dict(), filename)\n",
    "\n",
    "# Define function to load best early stopping pytorch model to continue with the evaluation\n",
    "def resume(model, filename):\n",
    "    model.load_state_dict(torch.load(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "Combination 1 of 1\n",
      "Parameters --> {'hidden_sizes': [100, 100, 50, 100, 100], 'dropout_prob': 0.5, 'class_weights': [1.0, 1.0], 'BATCH_SIZE': 16}\n",
      "--------------------------------------------------\n",
      "Fold 1 of 5\n",
      "Training Model\n",
      "Epoch [1/1000], Train Loss: 0.7704 , Test Loss: 0.6970\n",
      "Epoch [26/1000], Train Loss: 0.3142 , Test Loss: 0.4214\n",
      "Epoch [51/1000], Train Loss: 0.3146 , Test Loss: 0.4179\n",
      "Epoch [76/1000], Train Loss: 0.6461 , Test Loss: 0.4162\n",
      "Epoch [101/1000], Train Loss: 0.3138 , Test Loss: 0.4155\n",
      "Epoch [126/1000], Train Loss: 0.4804 , Test Loss: 0.4148\n",
      "Epoch [151/1000], Train Loss: 0.3141 , Test Loss: 0.4139\n",
      "Epoch [176/1000], Train Loss: 0.3134 , Test Loss: 0.4136\n",
      "Epoch [201/1000], Train Loss: 0.3133 , Test Loss: 0.4134\n",
      "Epoch [226/1000], Train Loss: 0.3136 , Test Loss: 0.4133\n",
      "Epoch [251/1000], Train Loss: 0.3133 , Test Loss: 0.4132\n",
      "Epoch [276/1000], Train Loss: 0.3134 , Test Loss: 0.4132\n",
      "Epoch [301/1000], Train Loss: 0.3133 , Test Loss: 0.4132\n",
      "Epoch [326/1000], Train Loss: 0.3133 , Test Loss: 0.4131\n",
      "Epoch [351/1000], Train Loss: 0.3380 , Test Loss: 0.4131\n",
      "Epoch [376/1000], Train Loss: 0.3330 , Test Loss: 0.4130\n",
      "Epoch [401/1000], Train Loss: 0.3133 , Test Loss: 0.4130\n",
      "Epoch [426/1000], Train Loss: 0.3133 , Test Loss: 0.4129\n",
      "Epoch [451/1000], Train Loss: 0.4813 , Test Loss: 0.4130\n",
      "Epoch [476/1000], Train Loss: 0.3133 , Test Loss: 0.4128\n",
      "Epoch [501/1000], Train Loss: 0.4799 , Test Loss: 0.4126\n",
      "Epoch [526/1000], Train Loss: 0.3133 , Test Loss: 0.4127\n",
      "Epoch [551/1000], Train Loss: 0.3133 , Test Loss: 0.4127\n",
      "Epoch [576/1000], Train Loss: 0.3133 , Test Loss: 0.4128\n",
      "Epoch [601/1000], Train Loss: 0.3133 , Test Loss: 0.4129\n",
      "Epoch [626/1000], Train Loss: 0.3133 , Test Loss: 0.4128\n",
      "Epoch [651/1000], Train Loss: 0.4799 , Test Loss: 0.4129\n",
      "Epoch [676/1000], Train Loss: 0.3133 , Test Loss: 0.4128\n",
      "Epoch [701/1000], Train Loss: 0.3134 , Test Loss: 0.4129\n",
      "Early stopped training at epoch 714\n",
      "Making predictions\n",
      "#########################\n",
      "Evaluation on train set\n",
      "Computing Metrics for train set\n",
      "[[2725   13]\n",
      " [ 160 2612]]\n",
      "Accuracy: 0.9686025408348458\n",
      "Precision: 0.9950476190476191\n",
      "Recall: 0.9422799422799423\n",
      "False Negative Ratio: 0.05772005772005772\n",
      "F1 Micro: 0.9686025408348458\n",
      "F1 Macro: 0.9685893299809463\n",
      "Making predictions\n",
      "#########################\n",
      "Evaluation on test set\n",
      "Computing Metrics for test set\n",
      "[[2718   20]\n",
      " [ 526 2247]]\n",
      "Accuracy: 0.9009254218835057\n",
      "Precision: 0.9911777679752978\n",
      "Recall: 0.8103137396321674\n",
      "False Negative Ratio: 0.18968626036783268\n",
      "F1 Micro: 0.9009254218835057\n",
      "F1 Macro: 0.9001964226011367\n",
      "--------------------------------------------------\n",
      "Fold 2 of 5\n",
      "Training Model\n",
      "Epoch [1/1000], Train Loss: 0.7124 , Test Loss: 0.6938\n",
      "Epoch [26/1000], Train Loss: 0.3256 , Test Loss: 0.4046\n",
      "Epoch [51/1000], Train Loss: 0.4579 , Test Loss: 0.3931\n",
      "Epoch [76/1000], Train Loss: 0.3198 , Test Loss: 0.3885\n",
      "Epoch [101/1000], Train Loss: 0.4543 , Test Loss: 0.3864\n",
      "Epoch [126/1000], Train Loss: 0.3583 , Test Loss: 0.3850\n",
      "Epoch [151/1000], Train Loss: 0.4557 , Test Loss: 0.3850\n",
      "Epoch [176/1000], Train Loss: 0.3398 , Test Loss: 0.3846\n",
      "Epoch [201/1000], Train Loss: 0.3468 , Test Loss: 0.3835\n",
      "Epoch [226/1000], Train Loss: 0.3525 , Test Loss: 0.3836\n",
      "Epoch [251/1000], Train Loss: 0.4650 , Test Loss: 0.3841\n",
      "Epoch [276/1000], Train Loss: 0.3135 , Test Loss: 0.3846\n",
      "Epoch [301/1000], Train Loss: 0.4572 , Test Loss: 0.3843\n",
      "Epoch [326/1000], Train Loss: 0.4609 , Test Loss: 0.3870\n",
      "Epoch [351/1000], Train Loss: 0.3519 , Test Loss: 0.3871\n",
      "Epoch [376/1000], Train Loss: 0.4114 , Test Loss: 0.3871\n",
      "Epoch [401/1000], Train Loss: 0.5996 , Test Loss: 0.3893\n",
      "Early stopped training at epoch 407\n",
      "Making predictions\n",
      "#########################\n",
      "Evaluation on train set\n",
      "Computing Metrics for train set\n",
      "[[2718   20]\n",
      " [ 439 2334]]\n",
      "Accuracy: 0.9167120304844856\n",
      "Precision: 0.9915038232795242\n",
      "Recall: 0.8416877028489002\n",
      "False Negative Ratio: 0.1583122971510999\n",
      "F1 Micro: 0.9167120304844856\n",
      "F1 Macro: 0.9163056829805386\n",
      "Making predictions\n",
      "#########################\n",
      "Evaluation on test set\n",
      "Computing Metrics for test set\n",
      "[[2584  154]\n",
      " [ 216 2556]]\n",
      "Accuracy: 0.9328493647912885\n",
      "Precision: 0.9431734317343173\n",
      "Recall: 0.922077922077922\n",
      "False Negative Ratio: 0.07792207792207792\n",
      "F1 Micro: 0.9328493647912885\n",
      "F1 Macro: 0.9328476306910208\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "results_test = pd.DataFrame()\n",
    "results_train = pd.DataFrame()\n",
    "skf = StratifiedKFold(2)\n",
    "# Extract total combinations\n",
    "num_combinations = len(list(product(*GLOBAL_PARAMETERS['PYTORCH_PARAMETERS'].values())))\n",
    "all_combinations = [\n",
    "    {key: value for key, value in zip(GLOBAL_PARAMETERS['PYTORCH_PARAMETERS'].keys(), combo)}\n",
    "    for combo in product(*GLOBAL_PARAMETERS['PYTORCH_PARAMETERS'].values())]\n",
    "# Loop throught all pytorch parameters\n",
    "for i in range(len(all_combinations)):\n",
    "    print('#' * 50)\n",
    "    print('Combination' , i + 1, 'of' , num_combinations)\n",
    "    print('Parameters -->' , all_combinations[i])\n",
    "    # Loop throgut test clinics\n",
    "    for ii , (train_index , test_index) in enumerate(skf.split(df.drop(columns = TARGET) , df[TARGET])):\n",
    "        print('-' * 50)\n",
    "        print('Fold' , ii + 1 , 'of 5')\n",
    "        \n",
    "        # Define X and Y in both train and test sets\n",
    "        X_train = df.loc[train_index].drop(columns = TARGET)\n",
    "        Y_train = df.loc[train_index][TARGET]\n",
    "\n",
    "        X_test = df.loc[test_index].drop(columns = TARGET)\n",
    "        Y_test = df.loc[test_index][TARGET]\n",
    "        \n",
    "        # Convert data to Pytorch tensors\n",
    "        x_train_tensor = torch.FloatTensor(X_train.values)\n",
    "        x_test_tensor = torch.FloatTensor(X_test.values)\n",
    "\n",
    "        y_train_tensor = torch.LongTensor([z[0] for z in Y_train.values.tolist()])\n",
    "        y_test_tensor = torch.LongTensor([z[0] for z in Y_test.values.tolist()])\n",
    "\n",
    "        dataset = torch.utils.data.TensorDataset(x_train_tensor, y_train_tensor)\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size = all_combinations[i]['BATCH_SIZE'], shuffle=True)\n",
    "        dataset_size = len(dataloader.dataset)\n",
    "        \n",
    "        # Define model\n",
    "        input_size = X_train.shape[1]\n",
    "        hidden_sizes = all_combinations[i]['hidden_sizes']\n",
    "        output_size = 2\n",
    "        dropout_prob = all_combinations[i]['dropout_prob']\n",
    "        model = FullyConnectedModel(input_size, hidden_sizes, output_size, dropout_prob)\n",
    "        # Define the loss function and optimizer\n",
    "        criterion = nn.CrossEntropyLoss(weight = torch.tensor(all_combinations[i]['class_weights']))  # Weights for each class\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "        \n",
    "        # Define the early stopping criteria\n",
    "        early_stop_thresh = 200\n",
    "        best_val_loss = 1e10\n",
    "        best_epoch = -1\n",
    "\n",
    "        # Define Scheduler for Learning Rate\n",
    "        scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.1, total_iters=30)\n",
    "        \n",
    "        # Training loop\n",
    "        print('Training Model')\n",
    "        num_epochs = 1_000\n",
    "        aux_val_loss = []\n",
    "        train_loss_history = []\n",
    "        test_loss_history = []\n",
    "        for epoch in range(num_epochs):\n",
    "            for id_batch, (x_batch, y_batch) in enumerate(dataloader):\n",
    "                if x_batch.shape[0] > 1:\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(x_batch)\n",
    "                    loss = criterion(outputs, y_batch)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "            # Scheduler learning rate\n",
    "            before_lr = optimizer.param_groups[0][\"lr\"]\n",
    "            scheduler.step()\n",
    "            after_lr = optimizer.param_groups[0][\"lr\"]\n",
    "            # Eval model for early stop\n",
    "            model.eval()\n",
    "            test_outputs = model(x_test_tensor)\n",
    "            test_loss = criterion(test_outputs , y_test_tensor)\n",
    "            test_loss_history.append(test_loss.item())\n",
    "            train_loss_history.append(loss.item())\n",
    "            if test_loss < best_val_loss:\n",
    "                best_val_loss = test_loss\n",
    "                best_epoch = epoch\n",
    "                checkpoint(model, \"pytorch_\" + str(MODEL_NAME) + \".pth\")\n",
    "            elif epoch - best_epoch > early_stop_thresh:\n",
    "                print(\"Early stopped training at epoch %d\" % epoch)\n",
    "                break  # terminate the training loop\n",
    "            if epoch % 25 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item():.4f} , Test Loss: {test_loss.item():.4f}')\n",
    "        \n",
    "        # Evaluation on train set\n",
    "        # Load best model\n",
    "        print('Making predictions')\n",
    "        print('#' * 25)\n",
    "        print('Evaluation on train set')\n",
    "        resume(model, \"pytorch_\" + str(MODEL_NAME) + \".pth\")\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_outputs = model(x_train_tensor)\n",
    "            test_outputs = test_outputs.squeeze()\n",
    "            predicted = torch.argmax(test_outputs , axis = 1).float()\n",
    "        # Compute metrics for test set\n",
    "        print('Computing Metrics for train set')\n",
    "        predictions = predicted.numpy()\n",
    "        TN, TP, FP, FN = calculate_confusion_matrix(Y_train.values,\n",
    "                                                    predictions)\n",
    "        acc, precision, recall, f1 , specificity, fnr = get_metrics(TN,\n",
    "                                                                    TP,\n",
    "                                                                    FP,\n",
    "                                                                    FN)\n",
    "        f1 = f1_score(Y_train.values , predictions , average = 'micro')\n",
    "        f2 = f1_score(Y_train.values , predictions , average = 'macro')\n",
    "        print(confusion_matrix(Y_train.values , predictions))\n",
    "        print('Accuracy:' , acc)\n",
    "        print('Precision:' , precision)\n",
    "        print('Recall:' , recall)\n",
    "        print('False Negative Ratio:' , fnr)\n",
    "        print('F1 Micro:' , f1)\n",
    "        print('F1 Macro:' , f2 )\n",
    "\n",
    "        # Save results of train set\n",
    "        aux_train =  pd.DataFrame({'Accuracy' : [acc],\n",
    "                          'Precision' : [precision],\n",
    "                          'Recall' : [recall],\n",
    "                          'F1_Micro' : [f1],\n",
    "                          'F1_Macro' : [f2],\n",
    "                          'Specificity' : [specificity],\n",
    "                          'False Negative Ratio' : [fnr],\n",
    "                          'Fold' : [ii],\n",
    "                          'model_parameters' :  [all_combinations[i]]})\n",
    "        results_train = pd.concat([results_train,\n",
    "                                  aux_train])\n",
    "        # Evaluation on test set\n",
    "        # Load best model\n",
    "        print('Making predictions')\n",
    "        print('#' * 25)\n",
    "        print('Evaluation on test set')\n",
    "        resume(model, \"pytorch_\" + str(MODEL_NAME) + \".pth\")\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_outputs = model(x_test_tensor)\n",
    "            test_outputs = test_outputs.squeeze()\n",
    "            predicted = torch.argmax(test_outputs , axis = 1).float()\n",
    "        # Compute metrics for test set\n",
    "        print('Computing Metrics for test set')\n",
    "        predictions = predicted.numpy()\n",
    "        TN, TP, FP, FN = calculate_confusion_matrix(Y_test.values,\n",
    "                                                    predictions)\n",
    "        acc, precision, recall, f1 , specificity, fnr = get_metrics(TN,\n",
    "                                                                    TP,\n",
    "                                                                    FP,\n",
    "                                                                    FN)\n",
    "        f1 = f1_score(Y_test.values , predictions , average = 'micro')\n",
    "        f2 = f1_score(Y_test.values , predictions , average = 'macro')\n",
    "        print(confusion_matrix(Y_test.values , predictions))\n",
    "        print('Accuracy:' , acc)\n",
    "        print('Precision:' , precision)\n",
    "        print('Recall:' , recall)\n",
    "        print('False Negative Ratio:' , fnr)\n",
    "        print('F1 Micro:' , f1)\n",
    "        print('F1 Macro:' , f2 )\n",
    "        # Save results of train set\n",
    "        aux_test =  pd.DataFrame({'Accuracy' : [acc],\n",
    "                          'Precision' : [precision],\n",
    "                          'Recall' : [recall],\n",
    "                          'F1_Micro' : [f1],\n",
    "                          'F1_Macro' : [f2],\n",
    "                          'Specificity' : [specificity],\n",
    "                          'False Negative Ratio' : [fnr],\n",
    "                          'model_parameters' :  [all_combinations[i]],\n",
    "                          'Fold' : [ii]})\n",
    "        results_test = pd.concat([results_test,\n",
    "                                  aux_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1_Micro</th>\n",
       "      <th>F1_Macro</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>False Negative Ratio</th>\n",
       "      <th>Fold</th>\n",
       "      <th>model_parameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.968603</td>\n",
       "      <td>0.995048</td>\n",
       "      <td>0.942280</td>\n",
       "      <td>0.968603</td>\n",
       "      <td>0.968589</td>\n",
       "      <td>0.995252</td>\n",
       "      <td>0.057720</td>\n",
       "      <td>0</td>\n",
       "      <td>{'hidden_sizes': [100, 100, 50, 100, 100], 'dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.916712</td>\n",
       "      <td>0.991504</td>\n",
       "      <td>0.841688</td>\n",
       "      <td>0.916712</td>\n",
       "      <td>0.916306</td>\n",
       "      <td>0.992695</td>\n",
       "      <td>0.158312</td>\n",
       "      <td>1</td>\n",
       "      <td>{'hidden_sizes': [100, 100, 50, 100, 100], 'dr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy  Precision    Recall  F1_Micro  F1_Macro  Specificity  \\\n",
       "0  0.968603   0.995048  0.942280  0.968603  0.968589     0.995252   \n",
       "0  0.916712   0.991504  0.841688  0.916712  0.916306     0.992695   \n",
       "\n",
       "   False Negative Ratio  Fold  \\\n",
       "0              0.057720     0   \n",
       "0              0.158312     1   \n",
       "\n",
       "                                    model_parameters  \n",
       "0  {'hidden_sizes': [100, 100, 50, 100, 100], 'dr...  \n",
       "0  {'hidden_sizes': [100, 100, 50, 100, 100], 'dr...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EwdtpqrqZNiJ"
   },
   "source": [
    "# **3. Export Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_path = r'..\\results'\n",
    "output_filename = r'\\Results_25_19_' + MODEL_NAME + '.xlsx' \n",
    "with pd.ExcelWriter(output_path + output_filename) as export:\n",
    "    results_train.to_excel(export , sheet_name = 'train')\n",
    "    results_test.to_excel(export , sheet_name = 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **4. Feature Importance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model\n",
      "Epoch [1/1000], Train Loss: 0.6707 , Test Loss: 0.6842\n",
      "Epoch [26/1000], Train Loss: 0.5188 , Test Loss: 0.4495\n",
      "Epoch [51/1000], Train Loss: 0.3941 , Test Loss: 0.4369\n",
      "Epoch [76/1000], Train Loss: 0.5334 , Test Loss: 0.4305\n",
      "Epoch [101/1000], Train Loss: 0.3703 , Test Loss: 0.4257\n",
      "Epoch [126/1000], Train Loss: 0.5426 , Test Loss: 0.4216\n",
      "Epoch [151/1000], Train Loss: 0.4104 , Test Loss: 0.4192\n",
      "Epoch [176/1000], Train Loss: 0.3396 , Test Loss: 0.4164\n",
      "Epoch [201/1000], Train Loss: 0.3795 , Test Loss: 0.4140\n",
      "Epoch [226/1000], Train Loss: 0.4947 , Test Loss: 0.4126\n",
      "Epoch [251/1000], Train Loss: 0.3838 , Test Loss: 0.4111\n",
      "Epoch [276/1000], Train Loss: 0.3804 , Test Loss: 0.4105\n",
      "Epoch [301/1000], Train Loss: 0.5936 , Test Loss: 0.4085\n",
      "Epoch [326/1000], Train Loss: 0.5151 , Test Loss: 0.4074\n",
      "Epoch [351/1000], Train Loss: 0.3275 , Test Loss: 0.4062\n",
      "Epoch [376/1000], Train Loss: 0.3277 , Test Loss: 0.4060\n",
      "Epoch [401/1000], Train Loss: 0.3808 , Test Loss: 0.4047\n",
      "Epoch [426/1000], Train Loss: 0.3383 , Test Loss: 0.4040\n",
      "Epoch [451/1000], Train Loss: 0.3861 , Test Loss: 0.4029\n",
      "Epoch [476/1000], Train Loss: 0.6927 , Test Loss: 0.4025\n",
      "Epoch [501/1000], Train Loss: 0.3156 , Test Loss: 0.4018\n",
      "Epoch [526/1000], Train Loss: 0.3696 , Test Loss: 0.4012\n",
      "Epoch [551/1000], Train Loss: 0.3830 , Test Loss: 0.4003\n",
      "Epoch [576/1000], Train Loss: 0.3696 , Test Loss: 0.4001\n",
      "Epoch [601/1000], Train Loss: 0.3374 , Test Loss: 0.3995\n",
      "Epoch [626/1000], Train Loss: 0.3817 , Test Loss: 0.3988\n",
      "Epoch [651/1000], Train Loss: 0.4256 , Test Loss: 0.3982\n",
      "Epoch [676/1000], Train Loss: 0.4569 , Test Loss: 0.3979\n",
      "Epoch [701/1000], Train Loss: 0.3411 , Test Loss: 0.3980\n",
      "Epoch [726/1000], Train Loss: 0.3184 , Test Loss: 0.3975\n",
      "Epoch [751/1000], Train Loss: 0.3345 , Test Loss: 0.3972\n",
      "Epoch [776/1000], Train Loss: 0.6625 , Test Loss: 0.3970\n",
      "Epoch [801/1000], Train Loss: 0.3198 , Test Loss: 0.3970\n",
      "Epoch [826/1000], Train Loss: 0.3194 , Test Loss: 0.3964\n",
      "Epoch [851/1000], Train Loss: 0.3149 , Test Loss: 0.3962\n",
      "Epoch [876/1000], Train Loss: 0.3497 , Test Loss: 0.3955\n",
      "Epoch [901/1000], Train Loss: 0.3879 , Test Loss: 0.3955\n",
      "Epoch [926/1000], Train Loss: 0.3385 , Test Loss: 0.3953\n",
      "Epoch [951/1000], Train Loss: 0.3169 , Test Loss: 0.3955\n",
      "Epoch [976/1000], Train Loss: 0.3150 , Test Loss: 0.3944\n"
     ]
    }
   ],
   "source": [
    "# Train model with the best hyperparameters\n",
    "# Define X and Y in both train and test sets\n",
    "X_train = df.loc[train_index].drop(columns = TARGET)\n",
    "Y_train = df.loc[train_index][TARGET]\n",
    "\n",
    "X_test = df.loc[test_index].drop(columns = TARGET)\n",
    "Y_test = df.loc[test_index][TARGET]\n",
    "\n",
    "# Convert data to Pytorch tensors\n",
    "x_train_tensor = torch.FloatTensor(X_train.values)\n",
    "x_test_tensor = torch.FloatTensor(X_test.values)\n",
    "\n",
    "y_train_tensor = torch.LongTensor([z[0] for z in Y_train.values.tolist()])\n",
    "y_test_tensor = torch.LongTensor([z[0] for z in Y_test.values.tolist()])\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(x_train_tensor, y_train_tensor)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size = 64, shuffle=True)\n",
    "dataset_size = len(dataloader.dataset)\n",
    "\n",
    "# Define model\n",
    "input_size = X_train.shape[1]\n",
    "hidden_sizes = [100, 100, 50, 100, 100]\n",
    "output_size = 2\n",
    "dropout_prob = 0.1\n",
    "model = FullyConnectedModel(input_size, hidden_sizes, output_size, dropout_prob)\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(weight = torch.tensor([1.0 , 2.0]))  # Weights for each class\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "\n",
    "# Define the early stopping criteria\n",
    "early_stop_thresh = 200\n",
    "best_val_loss = 1e10\n",
    "best_epoch = -1\n",
    "\n",
    "# Define Scheduler for Learning Rate\n",
    "scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.1, total_iters=30)\n",
    "\n",
    "# Training loop\n",
    "print('Training Model')\n",
    "num_epochs = 1_000\n",
    "aux_val_loss = []\n",
    "train_loss_history = []\n",
    "test_loss_history = []\n",
    "for epoch in range(num_epochs):\n",
    "    for id_batch, (x_batch, y_batch) in enumerate(dataloader):\n",
    "        if x_batch.shape[0] > 1:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    # Scheduler learning rate\n",
    "    before_lr = optimizer.param_groups[0][\"lr\"]\n",
    "    scheduler.step()\n",
    "    after_lr = optimizer.param_groups[0][\"lr\"]\n",
    "    # Eval model for early stop\n",
    "    model.eval()\n",
    "    test_outputs = model(x_test_tensor)\n",
    "    test_loss = criterion(test_outputs , y_test_tensor)\n",
    "    test_loss_history.append(test_loss.item())\n",
    "    train_loss_history.append(loss.item())\n",
    "    if test_loss < best_val_loss:\n",
    "        best_val_loss = test_loss\n",
    "        best_epoch = epoch\n",
    "        checkpoint(model, \"pytorch_\" + str(MODEL_NAME) + \".pth\")\n",
    "    elif epoch - best_epoch > early_stop_thresh:\n",
    "        print(\"Early stopped training at epoch %d\" % epoch)\n",
    "        break  # terminate the training loop\n",
    "    if epoch % 25 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item():.4f} , Test Loss: {test_loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create Pytorch Model Wrapper\n",
    "class PyTorchModelWrapper(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(PyTorchModelWrapper, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.softmax(self.model(x), dim=1)\n",
    "\n",
    "# Create wrapper object\n",
    "wrapped_model = PyTorchModelWrapper(model)\n",
    "\n",
    "# Create explainer\n",
    "explainer = shap.DeepExplainer(wrapped_model, x_test_tensor)\n",
    "instance_idx = 0\n",
    "X_instance = x_test_tensor[instance_idx].unsqueeze(0)\n",
    "shap_values = explainer.shap_values(shap.sample(x_test_tensor , 100) , check_additivity=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x750 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Summary plot for class 0\n",
    "shap.summary_plot(shap_values[: , : , 0],\n",
    "                  plot_type = 'bar',\n",
    "                  feature_names = X.columns.tolist(),\n",
    "                 title = 'Feature Importance for Class 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x750 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Summary plot for class 1\n",
    "shap.summary_plot(shap_values[: , : , 1],\n",
    "                  plot_type = 'bar',\n",
    "                  feature_names = X.columns.tolist(),\n",
    "                 title = 'Feature Importance for Class 0')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
