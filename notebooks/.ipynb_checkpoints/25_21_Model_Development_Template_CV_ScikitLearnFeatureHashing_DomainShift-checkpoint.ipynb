{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mk2cQAl9BrC7"
   },
   "source": [
    "# **0. Introduction**\n",
    "With this notebook you can define:\n",
    "\n",
    "1. Preprocessing method : If you want to perform scaling or not\n",
    "2. Over / under sampling method.\n",
    "3. Feature Selection Method\n",
    "4. Methods for handle nans\n",
    "5. Hyperparameters of the Pytorch Neural Network\n",
    "6. Categorical preprocessing (using hash vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5048,
     "status": "ok",
     "timestamp": 1715801900206,
     "user": {
      "displayName": "Edgar David Varcarcel Trujillo",
      "userId": "04708129325083286457"
     },
     "user_tz": 300
    },
    "id": "GGHrhsUkB3k0",
    "outputId": "a12535ad-4dea-4dc5-8880-c335178023d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported.\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "\n",
    "# Data Manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plots\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "\n",
    "# Data splitting\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Feature Selection\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import torch as th\n",
    "from torch import Tensor\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "# Feature Selection\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Oversampling\n",
    "from imblearn.over_sampling import SMOTE , SMOTENC , ADASYN , BorderlineSMOTE , KMeansSMOTE ,SVMSMOTE\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import confusion_matrix , f1_score , precision_score , recall_score , make_scorer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve, roc_auc_score , precision_recall_curve, average_precision_score , accuracy_score\n",
    "\n",
    "# Hyperparameter optimization\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Data imputation\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Feature importance\n",
    "import shap\n",
    "\n",
    "# Utils\n",
    "from itertools import product\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "import pickle\n",
    "from scipy.interpolate import make_interp_spline\n",
    "from scipy.interpolate import interp1d\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Set Numpy Seed\n",
    "np.random.seed(0)\n",
    "#plt.style.use('ggplot')\n",
    "print('Libraries imported.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLOBAL PARAMETERS:\n",
      "FEATURE_SELECTION : FIX\n",
      "SAMPLING : ADASYN\n",
      "PERFORM_SCALING : YES\n",
      "NAN_HANDLE : KNN\n",
      "CATEGORICAL_FEATURES : HASH\n",
      "PYTORCH_PARAMETERS : {'hidden_sizes': [[100, 100, 50, 100, 100]], 'dropout_prob': [0.1, 0.5], 'class_weights': [[1.0, 1.0], [1.0, 2.0]], 'BATCH_SIZE': [16, 32, 64]}\n",
      "\n",
      "Experiment Name: Best_Pytorch_Model\n"
     ]
    }
   ],
   "source": [
    "# Define parameters of how to run the notebook\n",
    "# Posible options\n",
    "\"\"\"\n",
    "GLOBAL_PARAMETERS = {'FEATURE_SELECTION' : ['XGBOOST' , 'VARIANCE_THRESHOLD' , 'F_CLASSIF' , 'RFE' , 'NONE' , 'ALL_FEATURE_SELECTION_METHODS'],\n",
    "                    'SAMPLING' : ['SMOTE' , 'ADASYN' , 'BORDERLINE_SMOTE', 'NONE' , 'ALL_OVERSAMPLING_METHODS'],\n",
    "                    'PERFORM_SCALING' : ['YES' , 'NO'],\n",
    "                    'NAN_HANDLE' : ['DROP_ALL' , 'MASK_ALL' , 'DROP_PERCENT' , KNN],\n",
    "                    'CATEGORICAL_FEATURES' : ['NONE' , 'HASH'],\n",
    "                    'PYTORC_PARAMETERS' : {}}\n",
    "\"\"\"\n",
    "GLOBAL_PARAMETERS = {'FEATURE_SELECTION' :  'FIX',\n",
    "                    'SAMPLING' : 'ADASYN' ,\n",
    "                    'PERFORM_SCALING' : 'YES',\n",
    "                    'NAN_HANDLE' : 'KNN' ,\n",
    "                    'CATEGORICAL_FEATURES' : 'HASH',\n",
    "                    'PYTORCH_PARAMETERS' : {'hidden_sizes' : [ [100 , 100 , 50 , 100 , 100]],\n",
    "                                               'dropout_prob' : [0.1 , 0.5],\n",
    "                                                'class_weights' : [[1.0 , 1.0],\n",
    "                                                                 [1.0 , 2.0]],\n",
    "                                               'BATCH_SIZE' : [16 , 32 , 64]}}\n",
    "\n",
    "# Assert that the selection is ok\n",
    "assert GLOBAL_PARAMETERS['FEATURE_SELECTION'] in ['XGBOOST' , 'VARIANCE_THRESHOLD' , 'F_CLASSIF' , 'RFE' , 'NONE' , 'ALL_FEATURE_SELECTION_METHODS' , 'FIX']\n",
    "assert GLOBAL_PARAMETERS['SAMPLING'] in ['SMOTE' , 'ADASYN' , 'BORDERLINE_SMOTE' , 'NONE' , 'ALL_OVERSAMPLING_METHODS']\n",
    "assert GLOBAL_PARAMETERS['PERFORM_SCALING'] in ['YES' , 'NO']\n",
    "assert GLOBAL_PARAMETERS['NAN_HANDLE'] in ['DROP_ALL' , 'MASK_ALL' , 'DROP_PERCENT' , 'KNN']\n",
    "assert GLOBAL_PARAMETERS['CATEGORICAL_FEATURES'] in ['NONE' , 'HASH']\n",
    "\n",
    "print('GLOBAL PARAMETERS:')\n",
    "for _ in GLOBAL_PARAMETERS.keys():\n",
    "    print(_ , ':' , GLOBAL_PARAMETERS[_])\n",
    "\n",
    "MODEL_NAME = 'Best_Pytorch_Model'\n",
    "print('\\nExperiment Name:' ,MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 739
    },
    "executionInfo": {
     "elapsed": 3976,
     "status": "ok",
     "timestamp": 1715801908962,
     "user": {
      "displayName": "Edgar David Varcarcel Trujillo",
      "userId": "04708129325083286457"
     },
     "user_tz": 300
    },
    "id": "ACCUsyi3B3fs",
    "outputId": "78a68a71-dfc4-4016-c63f-31db6ce3b718"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>record_id</th>\n",
       "      <th>data_group</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>bmi</th>\n",
       "      <th>active_smoking</th>\n",
       "      <th>pack_years</th>\n",
       "      <th>alcohol_abuse</th>\n",
       "      <th>real_function_ckd_stages</th>\n",
       "      <th>preoperative_hemoglobin_level</th>\n",
       "      <th>...</th>\n",
       "      <th>approach</th>\n",
       "      <th>conversion</th>\n",
       "      <th>type_of_anastomosis -&gt; das von UK sind alles  Ileocolonic anastomosis</th>\n",
       "      <th>anastomotic_technique</th>\n",
       "      <th>anastomotic_configuration</th>\n",
       "      <th>protective_stomy</th>\n",
       "      <th>surgeon_experience</th>\n",
       "      <th>anastomotic_leackage</th>\n",
       "      <th>BIHistoryOfIschaemicHeartDisease</th>\n",
       "      <th>BIHistoryOfDiabetes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5921</td>\n",
       "      <td>clarunisclaraspita</td>\n",
       "      <td>1.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.6</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59231</td>\n",
       "      <td>clarunisclaraspita</td>\n",
       "      <td>1.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.3</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>592-82</td>\n",
       "      <td>clarunisclaraspita</td>\n",
       "      <td>2.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>18.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.7</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>592-92</td>\n",
       "      <td>clarunisclaraspita</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>22.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.9</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59251</td>\n",
       "      <td>clarunisclaraspita</td>\n",
       "      <td>1.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>17.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.7</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5906</th>\n",
       "      <td>473-772</td>\n",
       "      <td>university_wrzburg</td>\n",
       "      <td>2.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>20.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.6</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5907</th>\n",
       "      <td>473-806</td>\n",
       "      <td>university_wrzburg</td>\n",
       "      <td>2.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5908</th>\n",
       "      <td>473-808</td>\n",
       "      <td>university_wrzburg</td>\n",
       "      <td>2.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>22.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.7</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5909</th>\n",
       "      <td>473-830</td>\n",
       "      <td>university_wrzburg</td>\n",
       "      <td>1.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>24.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.2</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5910</th>\n",
       "      <td>473-92</td>\n",
       "      <td>university_wrzburg</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5911 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     record_id          data_group  sex   age   bmi  active_smoking  \\\n",
       "0         5921  clarunisclaraspita  1.0  87.0   NaN             1.0   \n",
       "1        59231  clarunisclaraspita  1.0  53.0  19.2             1.0   \n",
       "2       592-82  clarunisclaraspita  2.0  71.0  18.8             0.0   \n",
       "3       592-92  clarunisclaraspita  1.0  39.0  22.4             0.0   \n",
       "4        59251  clarunisclaraspita  1.0  67.0  17.2             1.0   \n",
       "...        ...                 ...  ...   ...   ...             ...   \n",
       "5906   473-772  university_wrzburg  2.0  24.0  20.4             0.0   \n",
       "5907   473-806  university_wrzburg  2.0  87.0  32.0             0.0   \n",
       "5908   473-808  university_wrzburg  2.0  74.0  22.7             1.0   \n",
       "5909   473-830  university_wrzburg  1.0  85.0  24.3             0.0   \n",
       "5910    473-92  university_wrzburg  2.0   NaN  14.8             0.0   \n",
       "\n",
       "      pack_years  alcohol_abuse  real_function_ckd_stages  \\\n",
       "0           25.0            2.0                       5.0   \n",
       "1           35.0            3.0                       1.0   \n",
       "2            NaN            3.0                       1.0   \n",
       "3            0.0            3.0                       2.0   \n",
       "4           75.0            3.0                       1.0   \n",
       "...          ...            ...                       ...   \n",
       "5906         NaN            3.0                       1.0   \n",
       "5907         NaN            3.0                       4.0   \n",
       "5908        20.0            3.0                       1.0   \n",
       "5909         NaN            3.0                       2.0   \n",
       "5910         NaN            3.0                       NaN   \n",
       "\n",
       "      preoperative_hemoglobin_level  ...  approach  conversion  \\\n",
       "0                               9.6  ...       1.0         NaN   \n",
       "1                              11.3  ...       3.0         NaN   \n",
       "2                              13.7  ...       3.0         0.0   \n",
       "3                              11.9  ...       3.0         0.0   \n",
       "4                               7.7  ...       3.0         NaN   \n",
       "...                             ...  ...       ...         ...   \n",
       "5906                           13.6  ...       1.0         NaN   \n",
       "5907                            NaN  ...       3.0         NaN   \n",
       "5908                            9.7  ...       3.0         NaN   \n",
       "5909                           13.2  ...       3.0         NaN   \n",
       "5910                            NaN  ...       NaN         1.0   \n",
       "\n",
       "      type_of_anastomosis -> das von UK sind alles  Ileocolonic anastomosis  \\\n",
       "0                                                   3.0                       \n",
       "1                                                   3.0                       \n",
       "2                                                   3.0                       \n",
       "3                                                   3.0                       \n",
       "4                                                   3.0                       \n",
       "...                                                 ...                       \n",
       "5906                                                3.0                       \n",
       "5907                                                3.0                       \n",
       "5908                                                3.0                       \n",
       "5909                                                3.0                       \n",
       "5910                                                3.0                       \n",
       "\n",
       "      anastomotic_technique  anastomotic_configuration  protective_stomy  \\\n",
       "0                       2.0                        1.0               3.0   \n",
       "1                       1.0                        3.0               3.0   \n",
       "2                       NaN                        1.0               3.0   \n",
       "3                       1.0                        3.0               3.0   \n",
       "4                       1.0                        3.0               3.0   \n",
       "...                     ...                        ...               ...   \n",
       "5906                    2.0                        2.0               3.0   \n",
       "5907                    2.0                        2.0               3.0   \n",
       "5908                    1.0                        3.0               3.0   \n",
       "5909                    2.0                        2.0               3.0   \n",
       "5910                    2.0                        NaN               3.0   \n",
       "\n",
       "      surgeon_experience  anastomotic_leackage  \\\n",
       "0                    1.0                     0   \n",
       "1                    1.0                     0   \n",
       "2                    1.0                     0   \n",
       "3                    1.0                     0   \n",
       "4                    1.0                     0   \n",
       "...                  ...                   ...   \n",
       "5906                 1.0                     0   \n",
       "5907                 1.0                     0   \n",
       "5908                 1.0                     0   \n",
       "5909                 1.0                     0   \n",
       "5910                 NaN                     0   \n",
       "\n",
       "      BIHistoryOfIschaemicHeartDisease  BIHistoryOfDiabetes  \n",
       "0                                    0                    0  \n",
       "1                                    0                    0  \n",
       "2                                    0                    0  \n",
       "3                                    0                    0  \n",
       "4                                    1                    0  \n",
       "...                                ...                  ...  \n",
       "5906                                 0                    0  \n",
       "5907                                 0                    0  \n",
       "5908                                 0                    1  \n",
       "5909                                 0                    0  \n",
       "5910                                 0                    0  \n",
       "\n",
       "[5911 rows x 37 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "input_path = r'..\\data'\n",
    "input_filename = r'\\reduced_op_UK_merged_data_final_21062024.csv'\n",
    "df = pd.read_csv(input_path + input_filename , decimal = '.' , sep = ';')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1715801918340,
     "user": {
      "displayName": "Edgar David Varcarcel Trujillo",
      "userId": "04708129325083286457"
     },
     "user_tz": 300
    },
    "id": "czayG8DjZddE",
    "outputId": "f1e3a261-fb7a-48e2-d701-2235102047d3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "uk                    3041\n",
       "university_wrzburg     647\n",
       "university_dalhous     340\n",
       "university_of_east     314\n",
       "military_universit     294\n",
       "university_vilnius     234\n",
       "university_basel       226\n",
       "university_hamburg     220\n",
       "university_las_veg     173\n",
       "kantonspital_liest     106\n",
       "universitt_innsbru     103\n",
       "gzo_wetzikon            90\n",
       "emmental_hospital       67\n",
       "clarunisclaraspita      56\n",
       "Name: data_group, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show group distribution\n",
    "df['data_group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['clarunisclaraspita',\n",
       " 'emmental_hospital',\n",
       " 'gzo_wetzikon',\n",
       " 'kantonspital_liest',\n",
       " 'military_universit',\n",
       " 'uk',\n",
       " 'universitt_innsbru',\n",
       " 'university_basel',\n",
       " 'university_dalhous',\n",
       " 'university_hamburg',\n",
       " 'university_las_veg',\n",
       " 'university_of_east',\n",
       " 'university_vilnius',\n",
       " 'university_wrzburg']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save total of data groups\n",
    "clinics = df['data_group'].unique().tolist()\n",
    "clinics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1715801921370,
     "user": {
      "displayName": "Edgar David Varcarcel Trujillo",
      "userId": "04708129325083286457"
     },
     "user_tz": 300
    },
    "id": "FIrKEQ9IZksT",
    "outputId": "275e2afd-061c-422c-86ba-932e880352f7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['record_id',\n",
       " 'data_group',\n",
       " 'sex',\n",
       " 'age',\n",
       " 'bmi',\n",
       " 'active_smoking',\n",
       " 'pack_years',\n",
       " 'alcohol_abuse',\n",
       " 'real_function_ckd_stages',\n",
       " 'preoperative_hemoglobin_level',\n",
       " 'preoperative_leukocyte_count',\n",
       " 'preoperative_albumin_level',\n",
       " 'preoperative_crp_level',\n",
       " 'liver_metastasis_at_time_of_anastomosis',\n",
       " 'neoadjuvant_therapy',\n",
       " 'preoperative_use_of_immunosuppressive_drugs',\n",
       " 'preoperative_steroid_use',\n",
       " 'dosage_of_steroids',\n",
       " 'preoperative_nsaids_use',\n",
       " 'preoperative_blood_transfusion',\n",
       " 'tnf_alpha_inhib',\n",
       " 'asa_score',\n",
       " 'prior_abdominal_surgery',\n",
       " 'indication',\n",
       " 'operation',\n",
       " 'emergency_surgery',\n",
       " 'perforation',\n",
       " 'approach',\n",
       " 'conversion',\n",
       " 'type_of_anastomosis -> das von UK sind alles  Ileocolonic anastomosis',\n",
       " 'anastomotic_technique',\n",
       " 'anastomotic_configuration',\n",
       " 'protective_stomy',\n",
       " 'surgeon_experience',\n",
       " 'anastomotic_leackage',\n",
       " 'BIHistoryOfIschaemicHeartDisease',\n",
       " 'BIHistoryOfDiabetes']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print columns of the data\n",
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1715801924796,
     "user": {
      "displayName": "Edgar David Varcarcel Trujillo",
      "userId": "04708129325083286457"
     },
     "user_tz": 300
    },
    "id": "kOSaw6tI7yli"
   },
   "outputs": [],
   "source": [
    "# Drop columns to omit\n",
    "drop_columns = ['record_id']\n",
    "df = df.drop(columns = drop_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1715801925918,
     "user": {
      "displayName": "Edgar David Varcarcel Trujillo",
      "userId": "04708129325083286457"
     },
     "user_tz": 300
    },
    "id": "BP2nbR9WB3Yc"
   },
   "outputs": [],
   "source": [
    "# Define target variable\n",
    "TARGET = ['anastomotic_leackage']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kUsNFqsICrC1"
   },
   "source": [
    "# **1. NAN Handle**\n",
    "\n",
    "Here is defined the process of how to handle nan values.\n",
    "\n",
    "1. If DROP_ALL, then all rows with nans are removed \n",
    "2. If MASK_ALL, then all rows with nans are masked with -1\n",
    "3. If DROP_PERCENT, then only features with a percent of missing are dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>bmi</th>\n",
       "      <th>active_smoking</th>\n",
       "      <th>pack_years</th>\n",
       "      <th>alcohol_abuse</th>\n",
       "      <th>real_function_ckd_stages</th>\n",
       "      <th>preoperative_hemoglobin_level</th>\n",
       "      <th>preoperative_leukocyte_count</th>\n",
       "      <th>preoperative_albumin_level</th>\n",
       "      <th>...</th>\n",
       "      <th>conversion</th>\n",
       "      <th>type_of_anastomosis -&gt; das von UK sind alles  Ileocolonic anastomosis</th>\n",
       "      <th>anastomotic_technique</th>\n",
       "      <th>anastomotic_configuration</th>\n",
       "      <th>protective_stomy</th>\n",
       "      <th>surgeon_experience</th>\n",
       "      <th>anastomotic_leackage</th>\n",
       "      <th>BIHistoryOfIschaemicHeartDisease</th>\n",
       "      <th>BIHistoryOfDiabetes</th>\n",
       "      <th>data_group_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.6</td>\n",
       "      <td>6.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.3</td>\n",
       "      <td>12.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>18.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.7</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>22.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.9</td>\n",
       "      <td>8.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>17.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.7</td>\n",
       "      <td>8.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5906</th>\n",
       "      <td>2.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>20.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.6</td>\n",
       "      <td>4.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5907</th>\n",
       "      <td>2.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5908</th>\n",
       "      <td>2.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>22.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.7</td>\n",
       "      <td>9.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5909</th>\n",
       "      <td>1.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>24.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5910</th>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5911 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sex   age   bmi  active_smoking  pack_years  alcohol_abuse  \\\n",
       "0     1.0  87.0   NaN             1.0        25.0            2.0   \n",
       "1     1.0  53.0  19.2             1.0        35.0            3.0   \n",
       "2     2.0  71.0  18.8             0.0         NaN            3.0   \n",
       "3     1.0  39.0  22.4             0.0         0.0            3.0   \n",
       "4     1.0  67.0  17.2             1.0        75.0            3.0   \n",
       "...   ...   ...   ...             ...         ...            ...   \n",
       "5906  2.0  24.0  20.4             0.0         NaN            3.0   \n",
       "5907  2.0  87.0  32.0             0.0         NaN            3.0   \n",
       "5908  2.0  74.0  22.7             1.0        20.0            3.0   \n",
       "5909  1.0  85.0  24.3             0.0         NaN            3.0   \n",
       "5910  2.0   NaN  14.8             0.0         NaN            3.0   \n",
       "\n",
       "      real_function_ckd_stages  preoperative_hemoglobin_level  \\\n",
       "0                          5.0                            9.6   \n",
       "1                          1.0                           11.3   \n",
       "2                          1.0                           13.7   \n",
       "3                          2.0                           11.9   \n",
       "4                          1.0                            7.7   \n",
       "...                        ...                            ...   \n",
       "5906                       1.0                           13.6   \n",
       "5907                       4.0                            NaN   \n",
       "5908                       1.0                            9.7   \n",
       "5909                       2.0                           13.2   \n",
       "5910                       NaN                            NaN   \n",
       "\n",
       "      preoperative_leukocyte_count  preoperative_albumin_level  ...  \\\n",
       "0                              6.4                         0.0  ...   \n",
       "1                             12.4                         0.0  ...   \n",
       "2                              9.0                         0.0  ...   \n",
       "3                              8.8                         0.0  ...   \n",
       "4                              8.6                         0.0  ...   \n",
       "...                            ...                         ...  ...   \n",
       "5906                           4.9                         NaN  ...   \n",
       "5907                           NaN                         NaN  ...   \n",
       "5908                           9.2                         NaN  ...   \n",
       "5909                           4.7                         NaN  ...   \n",
       "5910                          13.4                         NaN  ...   \n",
       "\n",
       "      conversion  \\\n",
       "0            NaN   \n",
       "1            NaN   \n",
       "2            0.0   \n",
       "3            0.0   \n",
       "4            NaN   \n",
       "...          ...   \n",
       "5906         NaN   \n",
       "5907         NaN   \n",
       "5908         NaN   \n",
       "5909         NaN   \n",
       "5910         1.0   \n",
       "\n",
       "      type_of_anastomosis -> das von UK sind alles  Ileocolonic anastomosis  \\\n",
       "0                                                   3.0                       \n",
       "1                                                   3.0                       \n",
       "2                                                   3.0                       \n",
       "3                                                   3.0                       \n",
       "4                                                   3.0                       \n",
       "...                                                 ...                       \n",
       "5906                                                3.0                       \n",
       "5907                                                3.0                       \n",
       "5908                                                3.0                       \n",
       "5909                                                3.0                       \n",
       "5910                                                3.0                       \n",
       "\n",
       "      anastomotic_technique  anastomotic_configuration  protective_stomy  \\\n",
       "0                       2.0                        1.0               3.0   \n",
       "1                       1.0                        3.0               3.0   \n",
       "2                       NaN                        1.0               3.0   \n",
       "3                       1.0                        3.0               3.0   \n",
       "4                       1.0                        3.0               3.0   \n",
       "...                     ...                        ...               ...   \n",
       "5906                    2.0                        2.0               3.0   \n",
       "5907                    2.0                        2.0               3.0   \n",
       "5908                    1.0                        3.0               3.0   \n",
       "5909                    2.0                        2.0               3.0   \n",
       "5910                    2.0                        NaN               3.0   \n",
       "\n",
       "      surgeon_experience  anastomotic_leackage  \\\n",
       "0                    1.0                     0   \n",
       "1                    1.0                     0   \n",
       "2                    1.0                     0   \n",
       "3                    1.0                     0   \n",
       "4                    1.0                     0   \n",
       "...                  ...                   ...   \n",
       "5906                 1.0                     0   \n",
       "5907                 1.0                     0   \n",
       "5908                 1.0                     0   \n",
       "5909                 1.0                     0   \n",
       "5910                 NaN                     0   \n",
       "\n",
       "      BIHistoryOfIschaemicHeartDisease  BIHistoryOfDiabetes  \\\n",
       "0                                    0                    0   \n",
       "1                                    0                    0   \n",
       "2                                    0                    0   \n",
       "3                                    0                    0   \n",
       "4                                    1                    0   \n",
       "...                                ...                  ...   \n",
       "5906                                 0                    0   \n",
       "5907                                 0                    0   \n",
       "5908                                 0                    1   \n",
       "5909                                 0                    0   \n",
       "5910                                 0                    0   \n",
       "\n",
       "      data_group_encoded  \n",
       "0                      0  \n",
       "1                      0  \n",
       "2                      0  \n",
       "3                      0  \n",
       "4                      0  \n",
       "...                  ...  \n",
       "5906                  13  \n",
       "5907                  13  \n",
       "5908                  13  \n",
       "5909                  13  \n",
       "5910                  13  \n",
       "\n",
       "[5911 rows x 36 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode data_group column\n",
    "if 'data_group' in df.columns.tolist():\n",
    "    clinics_2 = df['data_group'].unique().tolist()\n",
    "    code_clinics = {v : k for k , v in enumerate(clinics_2)}\n",
    "    df['data_group_encoded'] = df['data_group'].map(code_clinics)\n",
    "    df = df.drop(columns = ['data_group'])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing data using KNN\n",
      "                                                           0\n",
      "pack_years                                          0.703265\n",
      "preoperative_albumin_level                          0.669768\n",
      "liver_metastasis_at_time_of_anastomosis             0.588733\n",
      "tnf_alpha_inhib                                     0.582643\n",
      "preoperative_nsaids_use                             0.577736\n",
      "real_function_ckd_stages                            0.570969\n",
      "preoperative_leukocyte_count                        0.566402\n",
      "alcohol_abuse                                       0.553037\n",
      "protective_stomy                                    0.536457\n",
      "preoperative_blood_transfusion                      0.507528\n",
      "dosage_of_steroids                                  0.506513\n",
      "preoperative_use_of_immunosuppressive_drugs         0.460497\n",
      "preoperative_steroid_use                            0.459313\n",
      "perforation                                         0.416173\n",
      "conversion                                          0.402977\n",
      "preoperative_crp_level                              0.348334\n",
      "anastomotic_configuration                           0.255287\n",
      "neoadjuvant_therapy                                 0.160041\n",
      "bmi                                                 0.074945\n",
      "preoperative_hemoglobin_level                       0.062595\n",
      "surgeon_experience                                  0.057858\n",
      "active_smoking                                      0.046016\n",
      "anastomotic_technique                               0.030113\n",
      "prior_abdominal_surgery                             0.026222\n",
      "approach                                            0.016410\n",
      "asa_score                                           0.006598\n",
      "type_of_anastomosis -> das von UK sind alles  I...  0.003722\n",
      "age                                                 0.001184\n",
      "emergency_surgery                                   0.000677\n",
      "sex                                                 0.000169\n",
      "operation                                           0.000000\n",
      "indication                                          0.000000\n",
      "anastomotic_leackage                                0.000000\n",
      "BIHistoryOfIschaemicHeartDisease                    0.000000\n",
      "BIHistoryOfDiabetes                                 0.000000\n",
      "data_group_encoded                                  0.000000\n",
      "Columns deleted by missing values: ['pack_years', 'preoperative_albumin_level', 'liver_metastasis_at_time_of_anastomosis', 'tnf_alpha_inhib', 'preoperative_nsaids_use', 'real_function_ckd_stages', 'preoperative_leukocyte_count', 'alcohol_abuse', 'protective_stomy', 'preoperative_blood_transfusion', 'dosage_of_steroids', 'preoperative_use_of_immunosuppressive_drugs', 'preoperative_steroid_use', 'perforation', 'conversion', 'preoperative_crp_level']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>bmi</th>\n",
       "      <th>active_smoking</th>\n",
       "      <th>pack_years</th>\n",
       "      <th>alcohol_abuse</th>\n",
       "      <th>real_function_ckd_stages</th>\n",
       "      <th>preoperative_hemoglobin_level</th>\n",
       "      <th>preoperative_leukocyte_count</th>\n",
       "      <th>preoperative_albumin_level</th>\n",
       "      <th>...</th>\n",
       "      <th>conversion</th>\n",
       "      <th>type_of_anastomosis -&gt; das von UK sind alles  Ileocolonic anastomosis</th>\n",
       "      <th>anastomotic_technique</th>\n",
       "      <th>anastomotic_configuration</th>\n",
       "      <th>protective_stomy</th>\n",
       "      <th>surgeon_experience</th>\n",
       "      <th>anastomotic_leackage</th>\n",
       "      <th>BIHistoryOfIschaemicHeartDisease</th>\n",
       "      <th>BIHistoryOfDiabetes</th>\n",
       "      <th>data_group_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>23.96</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.60</td>\n",
       "      <td>6.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>19.20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.30</td>\n",
       "      <td>12.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>18.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.70</td>\n",
       "      <td>9.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>22.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.90</td>\n",
       "      <td>8.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>17.20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.70</td>\n",
       "      <td>8.60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5906</th>\n",
       "      <td>2.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>20.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.60</td>\n",
       "      <td>4.90</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5907</th>\n",
       "      <td>2.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>32.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.61</td>\n",
       "      <td>7.27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5908</th>\n",
       "      <td>2.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>22.70</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.70</td>\n",
       "      <td>9.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5909</th>\n",
       "      <td>1.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>24.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.20</td>\n",
       "      <td>4.70</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5910</th>\n",
       "      <td>2.0</td>\n",
       "      <td>76.2</td>\n",
       "      <td>14.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>11.01</td>\n",
       "      <td>13.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5911 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sex   age    bmi  active_smoking  pack_years  alcohol_abuse  \\\n",
       "0     1.0  87.0  23.96             1.0        25.0            2.0   \n",
       "1     1.0  53.0  19.20             1.0        35.0            3.0   \n",
       "2     2.0  71.0  18.80             0.0        20.7            3.0   \n",
       "3     1.0  39.0  22.40             0.0         0.0            3.0   \n",
       "4     1.0  67.0  17.20             1.0        75.0            3.0   \n",
       "...   ...   ...    ...             ...         ...            ...   \n",
       "5906  2.0  24.0  20.40             0.0         1.0            3.0   \n",
       "5907  2.0  87.0  32.00             0.0        20.3            3.0   \n",
       "5908  2.0  74.0  22.70             1.0        20.0            3.0   \n",
       "5909  1.0  85.0  24.30             0.0        15.9            3.0   \n",
       "5910  2.0  76.2  14.80             0.0        17.5            3.0   \n",
       "\n",
       "      real_function_ckd_stages  preoperative_hemoglobin_level  \\\n",
       "0                          5.0                           9.60   \n",
       "1                          1.0                          11.30   \n",
       "2                          1.0                          13.70   \n",
       "3                          2.0                          11.90   \n",
       "4                          1.0                           7.70   \n",
       "...                        ...                            ...   \n",
       "5906                       1.0                          13.60   \n",
       "5907                       4.0                           9.61   \n",
       "5908                       1.0                           9.70   \n",
       "5909                       2.0                          13.20   \n",
       "5910                       1.8                          11.01   \n",
       "\n",
       "      preoperative_leukocyte_count  preoperative_albumin_level  ...  \\\n",
       "0                             6.40                         0.0  ...   \n",
       "1                            12.40                         0.0  ...   \n",
       "2                             9.00                         0.0  ...   \n",
       "3                             8.80                         0.0  ...   \n",
       "4                             8.60                         0.0  ...   \n",
       "...                            ...                         ...  ...   \n",
       "5906                          4.90                         0.0  ...   \n",
       "5907                          7.27                         0.0  ...   \n",
       "5908                          9.20                         0.0  ...   \n",
       "5909                          4.70                         0.0  ...   \n",
       "5910                         13.40                         0.0  ...   \n",
       "\n",
       "      conversion  \\\n",
       "0            0.5   \n",
       "1            0.5   \n",
       "2            0.0   \n",
       "3            0.0   \n",
       "4            0.6   \n",
       "...          ...   \n",
       "5906         0.2   \n",
       "5907         0.4   \n",
       "5908         0.5   \n",
       "5909         0.5   \n",
       "5910         1.0   \n",
       "\n",
       "      type_of_anastomosis -> das von UK sind alles  Ileocolonic anastomosis  \\\n",
       "0                                                   3.0                       \n",
       "1                                                   3.0                       \n",
       "2                                                   3.0                       \n",
       "3                                                   3.0                       \n",
       "4                                                   3.0                       \n",
       "...                                                 ...                       \n",
       "5906                                                3.0                       \n",
       "5907                                                3.0                       \n",
       "5908                                                3.0                       \n",
       "5909                                                3.0                       \n",
       "5910                                                3.0                       \n",
       "\n",
       "      anastomotic_technique  anastomotic_configuration  protective_stomy  \\\n",
       "0                       2.0                        1.0               3.0   \n",
       "1                       1.0                        3.0               3.0   \n",
       "2                       1.7                        1.0               3.0   \n",
       "3                       1.0                        3.0               3.0   \n",
       "4                       1.0                        3.0               3.0   \n",
       "...                     ...                        ...               ...   \n",
       "5906                    2.0                        2.0               3.0   \n",
       "5907                    2.0                        2.0               3.0   \n",
       "5908                    1.0                        3.0               3.0   \n",
       "5909                    2.0                        2.0               3.0   \n",
       "5910                    2.0                        2.8               3.0   \n",
       "\n",
       "      surgeon_experience  anastomotic_leackage  \\\n",
       "0                    1.0                   0.0   \n",
       "1                    1.0                   0.0   \n",
       "2                    1.0                   0.0   \n",
       "3                    1.0                   0.0   \n",
       "4                    1.0                   0.0   \n",
       "...                  ...                   ...   \n",
       "5906                 1.0                   0.0   \n",
       "5907                 1.0                   0.0   \n",
       "5908                 1.0                   0.0   \n",
       "5909                 1.0                   0.0   \n",
       "5910                 1.0                   0.0   \n",
       "\n",
       "      BIHistoryOfIschaemicHeartDisease  BIHistoryOfDiabetes  \\\n",
       "0                                  0.0                  0.0   \n",
       "1                                  0.0                  0.0   \n",
       "2                                  0.0                  0.0   \n",
       "3                                  0.0                  0.0   \n",
       "4                                  1.0                  0.0   \n",
       "...                                ...                  ...   \n",
       "5906                               0.0                  0.0   \n",
       "5907                               0.0                  0.0   \n",
       "5908                               0.0                  1.0   \n",
       "5909                               0.0                  0.0   \n",
       "5910                               0.0                  0.0   \n",
       "\n",
       "      data_group_encoded  \n",
       "0                    0.0  \n",
       "1                    0.0  \n",
       "2                    0.0  \n",
       "3                    0.0  \n",
       "4                    0.0  \n",
       "...                  ...  \n",
       "5906                13.0  \n",
       "5907                13.0  \n",
       "5908                13.0  \n",
       "5909                13.0  \n",
       "5910                13.0  \n",
       "\n",
       "[5911 rows x 36 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if GLOBAL_PARAMETERS['NAN_HANDLE'] == 'MASK_ALL':\n",
    "    print('All features masked as -1')\n",
    "    df = df.fillna(-1)\n",
    "if GLOBAL_PARAMETERS['NAN_HANDLE'] == 'DROP_ALL':\n",
    "    percent_of_missing = pd.DataFrame((df.replace(-1 , np.nan).isnull().sum() / df.shape[0]).sort_values(ascending = False))\n",
    "    print(percent_of_missing)\n",
    "    to_drop_missing_columns = percent_of_missing[percent_of_missing[0] > 0.2].index.tolist()\n",
    "    print('Columns deleted by missing values:' , to_drop_missing_columns)\n",
    "    df = df.drop(columns = to_drop_missing_columns)\n",
    "    df = df.dropna()\n",
    "    print('Rows with nan dropped')\n",
    "if GLOBAL_PARAMETERS['NAN_HANDLE'] == 'DROP_PERCENT':\n",
    "    print('Maintaining features with certain percent of NaNs')\n",
    "    percent_of_missing = pd.DataFrame((df.replace(-1 , np.nan).isnull().sum() / df.shape[0]).sort_values(ascending = False))\n",
    "    print(percent_of_missing)\n",
    "    to_drop_missing_columns = percent_of_missing[percent_of_missing[0] > 0.2].index.tolist()\n",
    "    print('Columns deleted by missing values:' , to_drop_missing_columns)\n",
    "    df = df.drop(columns = to_drop_missing_columns)\n",
    "if GLOBAL_PARAMETERS['NAN_HANDLE'] == 'KNN':\n",
    "    print('Imputing data using KNN')\n",
    "    percent_of_missing = pd.DataFrame((df.replace(-1 , np.nan).isnull().sum() / df.shape[0]).sort_values(ascending = False))\n",
    "    print(percent_of_missing)\n",
    "    to_drop_missing_columns = percent_of_missing[percent_of_missing[0] > 0.3].index.tolist()\n",
    "    print('Columns deleted by missing values:' , to_drop_missing_columns)\n",
    "    imputer = KNNImputer(n_neighbors = 10)\n",
    "    df = pd.DataFrame(imputer.fit_transform(df) , columns = df.columns.tolist())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object saved --> ..\\models\\Sklearn_KNN_Imputer.sav\n"
     ]
    }
   ],
   "source": [
    "# Save imputer\n",
    "imputer_path = r'..\\models\\Sklearn_KNN_Imputer.sav'\n",
    "pickle.dump(imputer , open(imputer_path , 'wb'))\n",
    "print('Object saved -->' , imputer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1715801934259,
     "user": {
      "displayName": "Edgar David Varcarcel Trujillo",
      "userId": "04708129325083286457"
     },
     "user_tz": 300
    },
    "id": "sRxQj_luDbsE",
    "outputId": "5bd6d28a-dc22-4d15-e835-86249824ec4b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sex',\n",
       " 'active_smoking',\n",
       " 'alcohol_abuse',\n",
       " 'liver_metastasis_at_time_of_anastomosis',\n",
       " 'neoadjuvant_therapy',\n",
       " 'preoperative_use_of_immunosuppressive_drugs',\n",
       " 'preoperative_steroid_use',\n",
       " 'preoperative_nsaids_use',\n",
       " 'preoperative_blood_transfusion',\n",
       " 'tnf_alpha_inhib',\n",
       " 'prior_abdominal_surgery',\n",
       " 'indication',\n",
       " 'operation',\n",
       " 'emergency_surgery',\n",
       " 'perforation',\n",
       " 'approach',\n",
       " 'conversion',\n",
       " 'type_of_anastomosis -> das von UK sind alles  Ileocolonic anastomosis',\n",
       " 'anastomotic_technique',\n",
       " 'anastomotic_configuration',\n",
       " 'protective_stomy',\n",
       " 'BIHistoryOfIschaemicHeartDisease',\n",
       " 'BIHistoryOfDiabetes',\n",
       " 'data_group_encoded']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define numeric and categorical columns\n",
    "num_columns = ['age' , 'bmi' , 'pack_years' , 'preoperative_hemoglobin_level' , 'preoperative_leukocyte_count',\n",
    "              'preoperative_albumin_level' , 'preoperative_crp_level' , 'dosage_of_steroids']\n",
    "num_columns = [i for i in num_columns if i in df.columns.tolist()]\n",
    "ordinal_columns = ['real_function_ckd_stages' , 'charlson_comorbidity_index' ,\n",
    "                  'asa_score' , 'surgeon_experience']\n",
    "ordinal_columns = [i for i in ordinal_columns if i in df.columns.tolist()]\n",
    "cat_columns = df.drop(columns = num_columns + TARGET + ordinal_columns).columns.tolist()\n",
    "cat_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a73RK0mqKxXV",
    "tags": []
   },
   "source": [
    "# **2. Scaling**\n",
    "1. If YES, then MinMaxScaler is used to numeric features.\n",
    "2. If NO, then no scaling is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling performed\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>bmi</th>\n",
       "      <th>pack_years</th>\n",
       "      <th>preoperative_hemoglobin_level</th>\n",
       "      <th>preoperative_leukocyte_count</th>\n",
       "      <th>preoperative_albumin_level</th>\n",
       "      <th>preoperative_crp_level</th>\n",
       "      <th>dosage_of_steroids</th>\n",
       "      <th>real_function_ckd_stages</th>\n",
       "      <th>asa_score</th>\n",
       "      <th>...</th>\n",
       "      <th>approach</th>\n",
       "      <th>conversion</th>\n",
       "      <th>type_of_anastomosis -&gt; das von UK sind alles  Ileocolonic anastomosis</th>\n",
       "      <th>anastomotic_technique</th>\n",
       "      <th>anastomotic_configuration</th>\n",
       "      <th>protective_stomy</th>\n",
       "      <th>BIHistoryOfIschaemicHeartDisease</th>\n",
       "      <th>BIHistoryOfDiabetes</th>\n",
       "      <th>data_group_encoded</th>\n",
       "      <th>anastomotic_leackage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.835294</td>\n",
       "      <td>0.212811</td>\n",
       "      <td>0.004630</td>\n",
       "      <td>0.377119</td>\n",
       "      <td>0.047077</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013769</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.9</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.435294</td>\n",
       "      <td>0.128114</td>\n",
       "      <td>0.006481</td>\n",
       "      <td>0.449153</td>\n",
       "      <td>0.092635</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010327</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.120996</td>\n",
       "      <td>0.003833</td>\n",
       "      <td>0.550847</td>\n",
       "      <td>0.066819</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008606</td>\n",
       "      <td>0.04</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.270588</td>\n",
       "      <td>0.185053</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.474576</td>\n",
       "      <td>0.065300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.049914</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.092527</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.296610</td>\n",
       "      <td>0.063781</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.080895</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5906</th>\n",
       "      <td>0.094118</td>\n",
       "      <td>0.149466</td>\n",
       "      <td>0.000185</td>\n",
       "      <td>0.546610</td>\n",
       "      <td>0.035687</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009122</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5907</th>\n",
       "      <td>0.835294</td>\n",
       "      <td>0.355872</td>\n",
       "      <td>0.003759</td>\n",
       "      <td>0.377542</td>\n",
       "      <td>0.053683</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.044647</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5908</th>\n",
       "      <td>0.682353</td>\n",
       "      <td>0.190391</td>\n",
       "      <td>0.003704</td>\n",
       "      <td>0.381356</td>\n",
       "      <td>0.068337</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.053924</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5909</th>\n",
       "      <td>0.811765</td>\n",
       "      <td>0.218861</td>\n",
       "      <td>0.002944</td>\n",
       "      <td>0.529661</td>\n",
       "      <td>0.034169</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.033046</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5910</th>\n",
       "      <td>0.708235</td>\n",
       "      <td>0.049822</td>\n",
       "      <td>0.003241</td>\n",
       "      <td>0.436864</td>\n",
       "      <td>0.100228</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001549</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5911 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           age       bmi  pack_years  preoperative_hemoglobin_level  \\\n",
       "0     0.835294  0.212811    0.004630                       0.377119   \n",
       "1     0.435294  0.128114    0.006481                       0.449153   \n",
       "2     0.647059  0.120996    0.003833                       0.550847   \n",
       "3     0.270588  0.185053    0.000000                       0.474576   \n",
       "4     0.600000  0.092527    0.013889                       0.296610   \n",
       "...        ...       ...         ...                            ...   \n",
       "5906  0.094118  0.149466    0.000185                       0.546610   \n",
       "5907  0.835294  0.355872    0.003759                       0.377542   \n",
       "5908  0.682353  0.190391    0.003704                       0.381356   \n",
       "5909  0.811765  0.218861    0.002944                       0.529661   \n",
       "5910  0.708235  0.049822    0.003241                       0.436864   \n",
       "\n",
       "      preoperative_leukocyte_count  preoperative_albumin_level  \\\n",
       "0                         0.047077                         0.0   \n",
       "1                         0.092635                         0.0   \n",
       "2                         0.066819                         0.0   \n",
       "3                         0.065300                         0.0   \n",
       "4                         0.063781                         0.0   \n",
       "...                            ...                         ...   \n",
       "5906                      0.035687                         0.0   \n",
       "5907                      0.053683                         0.0   \n",
       "5908                      0.068337                         0.0   \n",
       "5909                      0.034169                         0.0   \n",
       "5910                      0.100228                         0.0   \n",
       "\n",
       "      preoperative_crp_level  dosage_of_steroids  real_function_ckd_stages  \\\n",
       "0                   0.013769                0.00                       5.0   \n",
       "1                   0.010327                0.00                       1.0   \n",
       "2                   0.008606                0.04                       1.0   \n",
       "3                   0.049914                0.00                       2.0   \n",
       "4                   0.080895                0.00                       1.0   \n",
       "...                      ...                 ...                       ...   \n",
       "5906                0.009122                0.00                       1.0   \n",
       "5907                0.044647                0.00                       4.0   \n",
       "5908                0.053924                0.00                       1.0   \n",
       "5909                0.033046                0.00                       2.0   \n",
       "5910                0.001549                0.00                       1.8   \n",
       "\n",
       "      asa_score  ...  approach  conversion  \\\n",
       "0           2.9  ...       1.0         0.5   \n",
       "1           2.0  ...       3.0         0.5   \n",
       "2           4.0  ...       3.0         0.0   \n",
       "3           2.0  ...       3.0         0.0   \n",
       "4           4.0  ...       3.0         0.6   \n",
       "...         ...  ...       ...         ...   \n",
       "5906        2.0  ...       1.0         0.2   \n",
       "5907        3.0  ...       3.0         0.4   \n",
       "5908        3.0  ...       3.0         0.5   \n",
       "5909        3.0  ...       3.0         0.5   \n",
       "5910        1.0  ...       2.8         1.0   \n",
       "\n",
       "      type_of_anastomosis -> das von UK sind alles  Ileocolonic anastomosis  \\\n",
       "0                                                   3.0                       \n",
       "1                                                   3.0                       \n",
       "2                                                   3.0                       \n",
       "3                                                   3.0                       \n",
       "4                                                   3.0                       \n",
       "...                                                 ...                       \n",
       "5906                                                3.0                       \n",
       "5907                                                3.0                       \n",
       "5908                                                3.0                       \n",
       "5909                                                3.0                       \n",
       "5910                                                3.0                       \n",
       "\n",
       "      anastomotic_technique  anastomotic_configuration  protective_stomy  \\\n",
       "0                       2.0                        1.0               3.0   \n",
       "1                       1.0                        3.0               3.0   \n",
       "2                       1.7                        1.0               3.0   \n",
       "3                       1.0                        3.0               3.0   \n",
       "4                       1.0                        3.0               3.0   \n",
       "...                     ...                        ...               ...   \n",
       "5906                    2.0                        2.0               3.0   \n",
       "5907                    2.0                        2.0               3.0   \n",
       "5908                    1.0                        3.0               3.0   \n",
       "5909                    2.0                        2.0               3.0   \n",
       "5910                    2.0                        2.8               3.0   \n",
       "\n",
       "      BIHistoryOfIschaemicHeartDisease  BIHistoryOfDiabetes  \\\n",
       "0                                  0.0                  0.0   \n",
       "1                                  0.0                  0.0   \n",
       "2                                  0.0                  0.0   \n",
       "3                                  0.0                  0.0   \n",
       "4                                  1.0                  0.0   \n",
       "...                                ...                  ...   \n",
       "5906                               0.0                  0.0   \n",
       "5907                               0.0                  0.0   \n",
       "5908                               0.0                  1.0   \n",
       "5909                               0.0                  0.0   \n",
       "5910                               0.0                  0.0   \n",
       "\n",
       "      data_group_encoded  anastomotic_leackage  \n",
       "0                    0.0                   0.0  \n",
       "1                    0.0                   0.0  \n",
       "2                    0.0                   0.0  \n",
       "3                    0.0                   0.0  \n",
       "4                    0.0                   0.0  \n",
       "...                  ...                   ...  \n",
       "5906                13.0                   0.0  \n",
       "5907                13.0                   0.0  \n",
       "5908                13.0                   0.0  \n",
       "5909                13.0                   0.0  \n",
       "5910                13.0                   0.0  \n",
       "\n",
       "[5911 rows x 36 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if GLOBAL_PARAMETERS['PERFORM_SCALING'] == 'YES':\n",
    "    print('Scaling performed\\n')\n",
    "    # Create scaler and one hot encoding obects\n",
    "    numeric_scaler = MinMaxScaler()\n",
    "    # Fit objects\n",
    "    numeric_scaler.fit(df[num_columns])\n",
    "    # Transform data\n",
    "    aux_numeric = pd.DataFrame(numeric_scaler.transform(df[num_columns]) , columns = df[num_columns].columns.tolist())\n",
    "    # Concat data\n",
    "    df = pd.concat([aux_numeric,\n",
    "                    df[ordinal_columns].fillna(-1).reset_index(drop = True),\n",
    "                    df[cat_columns].reset_index(drop = True),\n",
    "                    df[TARGET].fillna(0).reset_index(drop = True)] , axis = 1)\n",
    "if GLOBAL_PARAMETERS['PERFORM_SCALING'] == 'NO':\n",
    "    print('No scaling performed\\n')\n",
    "    #for i in range(len(train_set)):\n",
    "        #train_set[i] = train_set[i].drop(columns = ['data_group']).fillna(-1)\n",
    "        #test_set[i] = test_set[i].drop(columns = ['data_group']).fillna(-1)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object saved --> ..\\models\\SkLearn_Scaler.sav\n"
     ]
    }
   ],
   "source": [
    "# Save Scaler\n",
    "scaler_path = r'..\\models\\SkLearn_Scaler.sav'\n",
    "pickle.dump(numeric_scaler , open(scaler_path , 'wb'))\n",
    "print('Object saved -->' , scaler_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3 Feature Selection**\n",
    "\n",
    "1. If XGBOOST, then a XGBClassifier is trained and select the top n% (default 80%) of the features\n",
    "2. If VARIANCE_THRESHOLD, then Scikit Learn Variance Threshold with default parameters is applied\n",
    "3. If F_CLASSIF, then ANOVA is applied.\n",
    "4. If RFE, then Recursive Feature Elimination is applied.\n",
    "5. If LASSO, then Lasso model is applied.\n",
    "6. If NONE, then no feature selection is applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed selection\n",
      "Selected Features : ['age', 'preoperative_albumin_level', 'bmi', 'preoperative_hemoglobin_level', 'pack_years', 'sex', 'neoadjuvant_therapy', 'preoperative_use_of_immunosuppressive_drugs', 'tnf_alpha_inhib', 'emergency_surgery', 'approach', 'anastomotic_leackage', 'preoperative_steroid_use', 'preoperative_nsaids_use', 'active_smoking', 'liver_metastasis_at_time_of_anastomosis', 'BIHistoryOfDiabetes', 'preoperative_blood_transfusion', 'perforation', 'anastomotic_technique', 'surgeon_experience', 'conversion', 'anastomotic_configuration', 'protective_stomy', 'BIHistoryOfIschaemicHeartDisease', 'alcohol_abuse', 'asa_score']\n"
     ]
    }
   ],
   "source": [
    "if GLOBAL_PARAMETERS['FEATURE_SELECTION'] == 'XGBOOST':\n",
    "    print('Feature Selection : XGBoost Feature Importance')\n",
    "    # Create XGB Model\n",
    "    feature_selection_model = XGBClassifier().fit(df.drop(columns = TARGET),\n",
    "                                                  df[TARGET])\n",
    "    # Extract feature importance from the model\n",
    "    feature_importances = pd.DataFrame({'Feature' : feature_selection_model.feature_names_in_.tolist(),\n",
    "                                       'Importance' : feature_selection_model.feature_importances_.tolist()}).sort_values(by = 'Importance' , ascending = False).reset_index(drop = True)\n",
    "    feature_importances['Cumulative_Importance'] = feature_importances['Importance'].cumsum()\n",
    "    print('Feature Importance for XGBoost:')\n",
    "    print(feature_importances)\n",
    "    # Select top % of features\n",
    "    threshold = 0.8\n",
    "    features_selected = feature_importances[feature_importances['Cumulative_Importance'] <= threshold]['Feature'].tolist()\n",
    "    print('Features Selected:' , features_selected)\n",
    "    df = df[features_selected + TARGET]\n",
    "if GLOBAL_PARAMETERS['FEATURE_SELECTION'] == 'VARIANCE_THRESHOLD':\n",
    "    print('Feature Selection : Variance Threshold')\n",
    "    # Create the object for selection\n",
    "    feature_selection_model = VarianceThreshold()\n",
    "    feature_selection_model.fit(df.drop(columns = TARGET),\n",
    "                                df[TARGET])\n",
    "    features_selected = feature_selection_model.feature_names_in_.tolist()\n",
    "    print('Features Selected:' , features_selected)\n",
    "    df = df[features_selected + TARGET]\n",
    "if GLOBAL_PARAMETERS['FEATURE_SELECTION'] == 'F_CLASSIF':\n",
    "    print('Feature Selection : ANOVA')\n",
    "    threshold = int(0.6 * df.drop(columns = TARGET).shape[1]) # % of features to maintain\n",
    "    print('Maintaining' , threshold , 'features')\n",
    "    feature_selection_model = SelectKBest(f_classif,\n",
    "                                          k=threshold)\n",
    "    feature_selection_model.fit(df.drop(columns = TARGET),\n",
    "                                df[TARGET])\n",
    "    features_selected = feature_selection_model.get_feature_names_out().tolist()\n",
    "    print('Features Selected:' , features_selected)\n",
    "    df = df[features_selected + TARGET]\n",
    "if GLOBAL_PARAMETERS['FEATURE_SELECTION'] == 'RFE':\n",
    "    print('Feature Selection : RFE')\n",
    "    threshold = int(0.6 * df.drop(columns = TARGET).shape[1]) # % of features to maintain\n",
    "    feature_selection_model = RFECV(\n",
    "                                    estimator = XGBClassifier(),\n",
    "                                    step = 1,\n",
    "                                    cv = StratifiedKFold(5),\n",
    "                                    scoring = \"f1_macro\",\n",
    "                                    min_features_to_select = threshold,\n",
    "                                    n_jobs = -1,\n",
    "                                )\n",
    "    feature_selection_model.fit(df.drop(columns = TARGET),\n",
    "                                df[TARGET])\n",
    "    features_selected = feature_selection_model.get_feature_names_out().tolist()\n",
    "    print('Features Selected:' , features_selected)\n",
    "    df = df[features_selected + TARGET]\n",
    "if GLOBAL_PARAMETERS['FEATURE_SELECTION'] == 'NONE':\n",
    "    print('No Feature Selection applied')\n",
    "if GLOBAL_PARAMETERS['FEATURE_SELECTION'] == 'FIX':\n",
    "    print('Fixed selection')\n",
    "    selected_features = ['age' ,'preoperative_albumin_level', 'bmi', 'preoperative_hemoglobin_level' ,'pack_years',\n",
    "                         'sex' , 'neoadjuvant_therapy' , 'preoperative_use_of_immunosuppressive_drugs' , 'tnf_alpha_inhib' , \n",
    "                         'emergency_surgery' , 'approach' , 'anastomotic_leackage' , 'preoperative_steroid_use' , 'preoperative_nsaids_use',\n",
    "                         'active_smoking' , 'liver_metastasis_at_time_of_anastomosis' , 'BIHistoryOfDiabetes' , 'preoperative_blood_transfusion' ,\n",
    "                         'perforation' , 'anastomotic_technique' , 'surgeon_experience' , 'conversion' , 'anastomotic_configuration' , 'protective_stomy' , \n",
    "                         'BIHistoryOfIschaemicHeartDisease' , 'alcohol_abuse' , 'asa_score']\n",
    "    valid_features = [i for i in selected_features if i in df.columns.tolist()]\n",
    "    df = df[valid_features]\n",
    "    \n",
    "print('Selected Features' , ':' , df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Numeric Features: [['age', 'bmi', 'pack_years', 'preoperative_hemoglobin_level', 'preoperative_albumin_level']]\n",
      "New Categrocial Features: [['sex', 'neoadjuvant_therapy', 'preoperative_use_of_immunosuppressive_drugs', 'tnf_alpha_inhib', 'emergency_surgery', 'approach', 'preoperative_steroid_use', 'preoperative_nsaids_use', 'active_smoking', 'liver_metastasis_at_time_of_anastomosis', 'BIHistoryOfDiabetes', 'preoperative_blood_transfusion', 'perforation', 'anastomotic_technique', 'surgeon_experience', 'conversion', 'anastomotic_configuration', 'protective_stomy', 'BIHistoryOfIschaemicHeartDisease', 'alcohol_abuse', 'asa_score']]\n"
     ]
    }
   ],
   "source": [
    "# Re define selected categorical and numerical columns\n",
    "new_num_columns = []\n",
    "new_cat_columns = []\n",
    "new_num_columns.append([i for i in num_columns if i in df.columns.tolist()])\n",
    "new_cat_columns.append([i for i in df.columns.tolist() if i not in new_num_columns[0] and i not in TARGET])\n",
    "print('New Numeric Features:' , new_num_columns)\n",
    "print('New Categrocial Features:' , new_cat_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Categorical Features Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Feature Hash\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hash_0</th>\n",
       "      <th>hash_1</th>\n",
       "      <th>hash_2</th>\n",
       "      <th>hash_3</th>\n",
       "      <th>hash_4</th>\n",
       "      <th>hash_5</th>\n",
       "      <th>hash_6</th>\n",
       "      <th>hash_7</th>\n",
       "      <th>hash_8</th>\n",
       "      <th>hash_9</th>\n",
       "      <th>age</th>\n",
       "      <th>preoperative_albumin_level</th>\n",
       "      <th>bmi</th>\n",
       "      <th>preoperative_hemoglobin_level</th>\n",
       "      <th>pack_years</th>\n",
       "      <th>anastomotic_leackage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-2.4</td>\n",
       "      <td>0.835294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.212811</td>\n",
       "      <td>0.377119</td>\n",
       "      <td>0.004630</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>0.435294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.128114</td>\n",
       "      <td>0.449153</td>\n",
       "      <td>0.006481</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.2</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.120996</td>\n",
       "      <td>0.550847</td>\n",
       "      <td>0.003833</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.270588</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.185053</td>\n",
       "      <td>0.474576</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-3.4</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.092527</td>\n",
       "      <td>0.296610</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5906</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-1.8</td>\n",
       "      <td>0.094118</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.149466</td>\n",
       "      <td>0.546610</td>\n",
       "      <td>0.000185</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5907</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-2.6</td>\n",
       "      <td>0.835294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.355872</td>\n",
       "      <td>0.377542</td>\n",
       "      <td>0.003759</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5908</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>0.682353</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.190391</td>\n",
       "      <td>0.381356</td>\n",
       "      <td>0.003704</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5909</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>0.811765</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.218861</td>\n",
       "      <td>0.529661</td>\n",
       "      <td>0.002944</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5910</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.708235</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.049822</td>\n",
       "      <td>0.436864</td>\n",
       "      <td>0.003241</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5911 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      hash_0  hash_1  hash_2  hash_3  hash_4  hash_5  hash_6  hash_7  hash_8  \\\n",
       "0        0.0     1.0     0.0     2.0     1.0    -3.0     0.0     2.0    -3.0   \n",
       "1        0.0     1.0     0.0     1.0     1.0    -4.0     0.0     2.0    -3.0   \n",
       "2        0.0     2.0     0.0     0.7     1.0    -3.0     1.0    -1.2    -3.0   \n",
       "3        0.0     1.0     0.0     1.0     1.2    -3.0     0.0     1.0    -3.0   \n",
       "4        0.0     0.0     0.0     1.0     2.0    -3.0     0.0     2.0    -2.0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "5906     0.0     1.0     0.0     2.0     1.0    -3.0     0.0     3.0    -3.0   \n",
       "5907     0.0     2.0     0.0     2.0     1.0    -3.0     0.0     1.0    -3.0   \n",
       "5908    -1.0     2.0     0.0     1.0     1.0    -4.0     0.0     2.0    -3.0   \n",
       "5909     0.0     1.0     0.0     2.0     1.0    -3.0     0.0     1.0    -3.0   \n",
       "5910     0.0     2.0     0.0     2.0     1.0    -3.0     0.0     2.0    -3.0   \n",
       "\n",
       "      hash_9       age  preoperative_albumin_level       bmi  \\\n",
       "0       -2.4  0.835294                         0.0  0.212811   \n",
       "1       -1.5  0.435294                         0.0  0.128114   \n",
       "2       -4.0  0.647059                         0.0  0.120996   \n",
       "3       -2.0  0.270588                         0.0  0.185053   \n",
       "4       -3.4  0.600000                         0.0  0.092527   \n",
       "...      ...       ...                         ...       ...   \n",
       "5906    -1.8  0.094118                         0.0  0.149466   \n",
       "5907    -2.6  0.835294                         0.0  0.355872   \n",
       "5908    -2.5  0.682353                         0.0  0.190391   \n",
       "5909    -2.5  0.811765                         0.0  0.218861   \n",
       "5910     0.0  0.708235                         0.0  0.049822   \n",
       "\n",
       "      preoperative_hemoglobin_level  pack_years  anastomotic_leackage  \n",
       "0                          0.377119    0.004630                   0.0  \n",
       "1                          0.449153    0.006481                   0.0  \n",
       "2                          0.550847    0.003833                   0.0  \n",
       "3                          0.474576    0.000000                   0.0  \n",
       "4                          0.296610    0.013889                   0.0  \n",
       "...                             ...         ...                   ...  \n",
       "5906                       0.546610    0.000185                   0.0  \n",
       "5907                       0.377542    0.003759                   0.0  \n",
       "5908                       0.381356    0.003704                   0.0  \n",
       "5909                       0.529661    0.002944                   0.0  \n",
       "5910                       0.436864    0.003241                   0.0  \n",
       "\n",
       "[5911 rows x 16 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if GLOBAL_PARAMETERS['CATEGORICAL_FEATURES'] == 'NONE':\n",
    "    print('No categorical features processing')\n",
    "if GLOBAL_PARAMETERS['CATEGORICAL_FEATURES'] == 'HASH':\n",
    "    print('Using Feature Hash')\n",
    "    hasher = FeatureHasher(input_type = 'dict' , n_features=10)  # n_features is the number of output features\n",
    "    combined = df[new_cat_columns[0]].to_dict(orient = 'records')\n",
    "    hashed_features = hasher.transform(combined)\n",
    "    hashed_df = pd.DataFrame(hashed_features.toarray() , columns = ['hash_' + str(i) for i in range(hashed_features.toarray().shape[1])])\n",
    "    df = pd.concat([hashed_df,\n",
    "                    df.drop(columns = new_cat_columns[0])], axis = 1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object saved --> ..\\models\\SkLearn_Hasher.sav\n"
     ]
    }
   ],
   "source": [
    "# Save Hasher\n",
    "hasher_path = r'..\\models\\SkLearn_Hasher.sav'\n",
    "pickle.dump(hasher , open(hasher_path , 'wb'))\n",
    "print('Object saved -->' , hasher_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['sex',\n",
       "  'neoadjuvant_therapy',\n",
       "  'preoperative_use_of_immunosuppressive_drugs',\n",
       "  'tnf_alpha_inhib',\n",
       "  'emergency_surgery',\n",
       "  'approach',\n",
       "  'preoperative_steroid_use',\n",
       "  'preoperative_nsaids_use',\n",
       "  'active_smoking',\n",
       "  'liver_metastasis_at_time_of_anastomosis',\n",
       "  'BIHistoryOfDiabetes',\n",
       "  'preoperative_blood_transfusion',\n",
       "  'perforation',\n",
       "  'anastomotic_technique',\n",
       "  'surgeon_experience',\n",
       "  'conversion',\n",
       "  'anastomotic_configuration',\n",
       "  'protective_stomy',\n",
       "  'BIHistoryOfIschaemicHeartDisease',\n",
       "  'alcohol_abuse',\n",
       "  'asa_score']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_cat_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Sampling Method**\n",
    "\n",
    "Depending of the over/under sampling model is selected, then the object is applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling with ADASYN\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hash_0</th>\n",
       "      <th>hash_1</th>\n",
       "      <th>hash_2</th>\n",
       "      <th>hash_3</th>\n",
       "      <th>hash_4</th>\n",
       "      <th>hash_5</th>\n",
       "      <th>hash_6</th>\n",
       "      <th>hash_7</th>\n",
       "      <th>hash_8</th>\n",
       "      <th>hash_9</th>\n",
       "      <th>age</th>\n",
       "      <th>preoperative_albumin_level</th>\n",
       "      <th>bmi</th>\n",
       "      <th>preoperative_hemoglobin_level</th>\n",
       "      <th>pack_years</th>\n",
       "      <th>anastomotic_leackage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-2.400000</td>\n",
       "      <td>0.835294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.212811</td>\n",
       "      <td>0.377119</td>\n",
       "      <td>0.004630</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.435294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.128114</td>\n",
       "      <td>0.449153</td>\n",
       "      <td>0.006481</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.200000</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.120996</td>\n",
       "      <td>0.550847</td>\n",
       "      <td>0.003833</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.270588</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.185053</td>\n",
       "      <td>0.474576</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-3.400000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.092527</td>\n",
       "      <td>0.296610</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11013</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.713091</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.713091</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.285236</td>\n",
       "      <td>0.776274</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.313836</td>\n",
       "      <td>0.408102</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11014</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-2.235258</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.447052</td>\n",
       "      <td>0.806923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.288159</td>\n",
       "      <td>0.387536</td>\n",
       "      <td>0.002907</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11015</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.136989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.136989</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.054795</td>\n",
       "      <td>0.681386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.268732</td>\n",
       "      <td>0.461806</td>\n",
       "      <td>0.008278</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11016</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-2.481464</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.496293</td>\n",
       "      <td>0.789544</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.237779</td>\n",
       "      <td>0.394005</td>\n",
       "      <td>0.003760</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11017</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.296742</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-2.703258</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.400000</td>\n",
       "      <td>0.699425</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.269977</td>\n",
       "      <td>0.399235</td>\n",
       "      <td>0.004528</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11018 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       hash_0    hash_1  hash_2  hash_3  hash_4    hash_5  hash_6    hash_7  \\\n",
       "0         0.0  1.000000     0.0     2.0     1.0 -3.000000     0.0  2.000000   \n",
       "1         0.0  1.000000     0.0     1.0     1.0 -4.000000     0.0  2.000000   \n",
       "2         0.0  2.000000     0.0     0.7     1.0 -3.000000     1.0 -1.200000   \n",
       "3         0.0  1.000000     0.0     1.0     1.2 -3.000000     0.0  1.000000   \n",
       "4         0.0  0.000000     0.0     1.0     2.0 -3.000000     0.0  2.000000   \n",
       "...       ...       ...     ...     ...     ...       ...     ...       ...   \n",
       "11013     0.0  1.713091     0.0     1.0     2.0 -2.000000     0.0  1.713091   \n",
       "11014     0.0  2.000000     0.0     1.0     2.0 -2.235258     0.0  2.000000   \n",
       "11015     0.0  1.136989     0.0     1.0     2.0 -2.000000     0.0  1.136989   \n",
       "11016     0.0  2.000000     0.0     1.0     2.0 -2.481464     0.0  2.000000   \n",
       "11017     0.0  1.296742     0.0     1.0     2.0 -2.703258     0.0  2.000000   \n",
       "\n",
       "       hash_8    hash_9       age  preoperative_albumin_level       bmi  \\\n",
       "0        -3.0 -2.400000  0.835294                         0.0  0.212811   \n",
       "1        -3.0 -1.500000  0.435294                         0.0  0.128114   \n",
       "2        -3.0 -4.000000  0.647059                         0.0  0.120996   \n",
       "3        -3.0 -2.000000  0.270588                         0.0  0.185053   \n",
       "4        -2.0 -3.400000  0.600000                         0.0  0.092527   \n",
       "...       ...       ...       ...                         ...       ...   \n",
       "11013    -2.0 -2.285236  0.776274                         0.0  0.313836   \n",
       "11014    -2.0 -2.447052  0.806923                         0.0  0.288159   \n",
       "11015    -2.0 -2.054795  0.681386                         0.0  0.268732   \n",
       "11016    -2.0 -2.496293  0.789544                         0.0  0.237779   \n",
       "11017    -2.0 -2.400000  0.699425                         0.0  0.269977   \n",
       "\n",
       "       preoperative_hemoglobin_level  pack_years  anastomotic_leackage  \n",
       "0                           0.377119    0.004630                   0.0  \n",
       "1                           0.449153    0.006481                   0.0  \n",
       "2                           0.550847    0.003833                   0.0  \n",
       "3                           0.474576    0.000000                   0.0  \n",
       "4                           0.296610    0.013889                   0.0  \n",
       "...                              ...         ...                   ...  \n",
       "11013                       0.408102    0.004149                   1.0  \n",
       "11014                       0.387536    0.002907                   1.0  \n",
       "11015                       0.461806    0.008278                   1.0  \n",
       "11016                       0.394005    0.003760                   1.0  \n",
       "11017                       0.399235    0.004528                   1.0  \n",
       "\n",
       "[11018 rows x 16 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if GLOBAL_PARAMETERS['SAMPLING'] == 'SMOTE':\n",
    "    print('Oversampling with SMOTE')\n",
    "    X = df.drop(columns = TARGET)\n",
    "    Y =df[TARGET]\n",
    "    oversampler = SMOTE()\n",
    "    X_res , Y_res = oversampler.fit_resample(X , Y)\n",
    "    df = pd.concat([X_res ,\n",
    "                    Y_res] , axis = 1)\n",
    "if GLOBAL_PARAMETERS['SAMPLING'] == 'ADASYN':\n",
    "    print('Oversampling with ADASYN')\n",
    "    X = df.drop(columns = TARGET)\n",
    "    Y = df[TARGET]\n",
    "    oversampler = ADASYN()\n",
    "    X_res , Y_res = oversampler.fit_resample(X , Y)\n",
    "    df = pd.concat([X_res ,\n",
    "                    Y_res] , axis = 1)\n",
    "if GLOBAL_PARAMETERS['SAMPLING'] == 'BORDERLINE_SMOTE':\n",
    "    print('Oversampling with Borderline SMOTE')\n",
    "    X = df.drop(columns = TARGET)\n",
    "    Y = df[TARGET]\n",
    "    oversampler = BorderlineSMOTE(random_state=42)\n",
    "    X_res , Y_res = oversampler.fit_resample(X , Y)\n",
    "    df = pd.concat([X_res ,\n",
    "                    Y_res] , axis = 1)\n",
    "if GLOBAL_PARAMETERS['SAMPLING'] == 'NONE':\n",
    "    print('No oversampling applied')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aNv19lPIGT0V"
   },
   "source": [
    "# **5. Model Training and Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 503,
     "status": "ok",
     "timestamp": 1715802205303,
     "user": {
      "displayName": "Edgar David Varcarcel Trujillo",
      "userId": "04708129325083286457"
     },
     "user_tz": 300
    },
    "id": "Y2PTb7VLJ2H3"
   },
   "outputs": [],
   "source": [
    "# Define functions for evaluation metrics\n",
    "def calculate_confusion_matrix(true_labels, predicted_labels):\n",
    "    # Calculate confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(true_labels, predicted_labels).ravel()\n",
    "\n",
    "    return tn, tp, fp, fn\n",
    "\n",
    "# Define function that takes confusion matrix an compute required metrics\n",
    "def get_metrics(TN,TP,FP,FN):\n",
    "\n",
    "    acc = (TN+TP)/(TN+TP+FN+FP)\n",
    "    precision = TP / (TP + FP)\n",
    "\n",
    "    # Calculate recall\n",
    "    recall = TP / (TP + FN)\n",
    "    # Calulcate specificity\n",
    "    specificity = TN / (TN + FP)\n",
    "\n",
    "    # Calculate False Negative Rate\n",
    "    FNR = FN / (FN + TP)\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    return acc, precision, recall, f1 , specificity , FNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 460,
     "status": "ok",
     "timestamp": 1715802207533,
     "user": {
      "displayName": "Edgar David Varcarcel Trujillo",
      "userId": "04708129325083286457"
     },
     "user_tz": 300
    },
    "id": "CEYT3xz8SSgx"
   },
   "outputs": [],
   "source": [
    "# Function to smooth the ROC curve using a moving average\n",
    "def smooth_roc_curve(fpr, tpr, window_size=5):\n",
    "    df = pd.DataFrame({'fpr': fpr, 'tpr': tpr})\n",
    "    df_smoothed = df.rolling(window=window_size).mean().dropna()\n",
    "    # Add (1,1) point at the end\n",
    "    df_smoothed = pd.concat([df_smoothed,\n",
    "                             pd.DataFrame({'fpr' : [1] , 'tpr' : [1]})] , axis = 0)\n",
    "    return df_smoothed['fpr'].values, df_smoothed['tpr'].values\n",
    "\n",
    "\n",
    "# Function to smooth the Precision Recall curve using a moving average\n",
    "def smooth_precision_curve(precision, recall, window_size=5):\n",
    "    df = pd.DataFrame({'precision': precision, 'recall': recall})\n",
    "    df_smoothed = df.rolling(window=window_size).mean().dropna()\n",
    "    # Add (1,0) point at the end\n",
    "    df_smoothed = pd.concat([df_smoothed,\n",
    "                             pd.DataFrame({'precision' : [1] , 'recall' : [0]})] , axis = 0)\n",
    "    return df_smoothed['precision'].values, df_smoothed['recall'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define fully conected model\n",
    "class FullyConnectedModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, dropout_prob=0.5):\n",
    "        super(FullyConnectedModel, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.output_size = output_size\n",
    "        self.dropout_prob = dropout_prob\n",
    "\n",
    "        layers = []\n",
    "        # Create the hidden layers with linear, batch normalization, and dropout\n",
    "        prev_size = input_size\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            layers.append(nn.BatchNorm1d(hidden_size))\n",
    "            layers.append(nn.Dropout(p=self.dropout_prob))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_size = hidden_size\n",
    "\n",
    "        # Output layer with softmax activation\n",
    "        layers.append(nn.Linear(prev_size, output_size))\n",
    "        layers.append(nn.Softmax(dim = 1))\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Define function to save pytorch model for early stopping\n",
    "def checkpoint(model, filename):\n",
    "    torch.save(model.state_dict(), filename)\n",
    "\n",
    "# Define function to load best early stopping pytorch model to continue with the evaluation\n",
    "def resume(model, filename):\n",
    "    model.load_state_dict(torch.load(filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training loop\n",
    "results_test = pd.DataFrame()\n",
    "results_train = pd.DataFrame()\n",
    "skf = StratifiedKFold(5)\n",
    "# Extract total combinations\n",
    "num_combinations = len(list(product(*GLOBAL_PARAMETERS['PYTORCH_PARAMETERS'].values())))\n",
    "all_combinations = [\n",
    "    {key: value for key, value in zip(GLOBAL_PARAMETERS['PYTORCH_PARAMETERS'].keys(), combo)}\n",
    "    for combo in product(*GLOBAL_PARAMETERS['PYTORCH_PARAMETERS'].values())]\n",
    "# Loop throught all pytorch parameters\n",
    "for i in range(len(all_combinations)):\n",
    "    print('#' * 50)\n",
    "    print('Combination' , i + 1, 'of' , num_combinations)\n",
    "    print('Parameters -->' , all_combinations[i])\n",
    "    # Loop throgut test clinics\n",
    "    for ii , (train_index , test_index) in enumerate(skf.split(df.drop(columns = TARGET) , df[TARGET])):\n",
    "        print('-' * 50)\n",
    "        print('Fold' , ii + 1 , 'of 5')\n",
    "        \n",
    "        # Define X and Y in both train and test sets\n",
    "        X_train = df.loc[train_index].drop(columns = TARGET)\n",
    "        Y_train = df.loc[train_index][TARGET]\n",
    "\n",
    "        X_test = df.loc[test_index].drop(columns = TARGET)\n",
    "        Y_test = df.loc[test_index][TARGET]\n",
    "        \n",
    "        # Convert data to Pytorch tensors\n",
    "        x_train_tensor = torch.FloatTensor(X_train.values)\n",
    "        x_test_tensor = torch.FloatTensor(X_test.values)\n",
    "\n",
    "        y_train_tensor = torch.LongTensor([z[0] for z in Y_train.values.tolist()])\n",
    "        y_test_tensor = torch.LongTensor([z[0] for z in Y_test.values.tolist()])\n",
    "\n",
    "        dataset = torch.utils.data.TensorDataset(x_train_tensor, y_train_tensor)\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size = all_combinations[i]['BATCH_SIZE'], shuffle=True)\n",
    "        dataset_size = len(dataloader.dataset)\n",
    "        \n",
    "        # Define model\n",
    "        input_size = X_train.shape[1]\n",
    "        hidden_sizes = all_combinations[i]['hidden_sizes']\n",
    "        output_size = 2\n",
    "        dropout_prob = all_combinations[i]['dropout_prob']\n",
    "        model = FullyConnectedModel(input_size, hidden_sizes, output_size, dropout_prob)\n",
    "        # Define the loss function and optimizer\n",
    "        criterion = nn.CrossEntropyLoss(weight = torch.tensor(all_combinations[i]['class_weights']))  # Weights for each class\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "        \n",
    "        # Define the early stopping criteria\n",
    "        early_stop_thresh = 200\n",
    "        best_val_loss = 1e10\n",
    "        best_epoch = -1\n",
    "\n",
    "        # Define Scheduler for Learning Rate\n",
    "        scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.1, total_iters=30)\n",
    "        \n",
    "        # Training loop\n",
    "        print('Training Model')\n",
    "        num_epochs = 1_000\n",
    "        aux_val_loss = []\n",
    "        train_loss_history = []\n",
    "        test_loss_history = []\n",
    "        for epoch in range(num_epochs):\n",
    "            for id_batch, (x_batch, y_batch) in enumerate(dataloader):\n",
    "                if x_batch.shape[0] > 1:\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(x_batch)\n",
    "                    loss = criterion(outputs, y_batch)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "            # Scheduler learning rate\n",
    "            before_lr = optimizer.param_groups[0][\"lr\"]\n",
    "            scheduler.step()\n",
    "            after_lr = optimizer.param_groups[0][\"lr\"]\n",
    "            # Eval model for early stop\n",
    "            model.eval()\n",
    "            test_outputs = model(x_test_tensor)\n",
    "            test_loss = criterion(test_outputs , y_test_tensor)\n",
    "            test_loss_history.append(test_loss.item())\n",
    "            train_loss_history.append(loss.item())\n",
    "            if test_loss < best_val_loss:\n",
    "                best_val_loss = test_loss\n",
    "                best_epoch = epoch\n",
    "                checkpoint(model, \"pytorch_sklearn_hasher\" + str(MODEL_NAME) + \".pth\")\n",
    "            elif epoch - best_epoch > early_stop_thresh:\n",
    "                print(\"Early stopped training at epoch %d\" % epoch)\n",
    "                break  # terminate the training loop\n",
    "            if epoch % 25 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item():.4f} , Test Loss: {test_loss.item():.4f}')\n",
    "        \n",
    "        # Evaluation on train set\n",
    "        # Load best model\n",
    "        print('Making predictions')\n",
    "        print('#' * 25)\n",
    "        print('Evaluation on train set')\n",
    "        resume(model, \"pytorch_sklearn_hasher\" + str(MODEL_NAME) + \".pth\")\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_outputs = model(x_train_tensor)\n",
    "            test_outputs = test_outputs.squeeze()\n",
    "            predicted = torch.argmax(test_outputs , axis = 1).float()\n",
    "        # Compute metrics for test set\n",
    "        print('Computing Metrics for train set')\n",
    "        predictions = predicted.numpy()\n",
    "        TN, TP, FP, FN = calculate_confusion_matrix(Y_train.values,\n",
    "                                                    predictions)\n",
    "        acc, precision, recall, f1 , specificity, fnr = get_metrics(TN,\n",
    "                                                                    TP,\n",
    "                                                                    FP,\n",
    "                                                                    FN)\n",
    "        f1 = f1_score(Y_train.values , predictions , average = 'micro')\n",
    "        f2 = f1_score(Y_train.values , predictions , average = 'macro')\n",
    "        print(confusion_matrix(Y_train.values , predictions))\n",
    "        print('Accuracy:' , acc)\n",
    "        print('Precision:' , precision)\n",
    "        print('Recall:' , recall)\n",
    "        print('False Negative Ratio:' , fnr)\n",
    "        print('F1 Micro:' , f1)\n",
    "        print('F1 Macro:' , f2 )\n",
    "\n",
    "        # Save results of train set\n",
    "        aux_train =  pd.DataFrame({'Accuracy' : [acc],\n",
    "                          'Precision' : [precision],\n",
    "                          'Recall' : [recall],\n",
    "                          'F1_Micro' : [f1],\n",
    "                          'F1_Macro' : [f2],\n",
    "                          'Specificity' : [specificity],\n",
    "                          'False Negative Ratio' : [fnr],\n",
    "                          'Fold' : [ii],\n",
    "                          'model_parameters' :  [all_combinations[i]]})\n",
    "        results_train = pd.concat([results_train,\n",
    "                                  aux_train])\n",
    "        # Evaluation on test set\n",
    "        # Load best model\n",
    "        print('Making predictions')\n",
    "        print('#' * 25)\n",
    "        print('Evaluation on test set')\n",
    "        resume(model, \"pytorch_sklearn_hasher\" + str(MODEL_NAME) + \".pth\")\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_outputs = model(x_test_tensor)\n",
    "            test_outputs = test_outputs.squeeze()\n",
    "            predicted = torch.argmax(test_outputs , axis = 1).float()\n",
    "        # Compute metrics for test set\n",
    "        print('Computing Metrics for test set')\n",
    "        predictions = predicted.numpy()\n",
    "        TN, TP, FP, FN = calculate_confusion_matrix(Y_test.values,\n",
    "                                                    predictions)\n",
    "        acc, precision, recall, f1 , specificity, fnr = get_metrics(TN,\n",
    "                                                                    TP,\n",
    "                                                                    FP,\n",
    "                                                                    FN)\n",
    "        f1 = f1_score(Y_test.values , predictions , average = 'micro')\n",
    "        f2 = f1_score(Y_test.values , predictions , average = 'macro')\n",
    "        print(confusion_matrix(Y_test.values , predictions))\n",
    "        print('Accuracy:' , acc)\n",
    "        print('Precision:' , precision)\n",
    "        print('Recall:' , recall)\n",
    "        print('False Negative Ratio:' , fnr)\n",
    "        print('F1 Micro:' , f1)\n",
    "        print('F1 Macro:' , f2 )\n",
    "        # Save results of train set\n",
    "        aux_test =  pd.DataFrame({'Accuracy' : [acc],\n",
    "                          'Precision' : [precision],\n",
    "                          'Recall' : [recall],\n",
    "                          'F1_Micro' : [f1],\n",
    "                          'F1_Macro' : [f2],\n",
    "                          'Specificity' : [specificity],\n",
    "                          'False Negative Ratio' : [fnr],\n",
    "                          'model_parameters' :  [all_combinations[i]],\n",
    "                          'Fold' : [ii]})\n",
    "        results_test = pd.concat([results_test,\n",
    "                                  aux_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1_Micro</th>\n",
       "      <th>F1_Macro</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>False Negative Ratio</th>\n",
       "      <th>Fold</th>\n",
       "      <th>model_parameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.955071</td>\n",
       "      <td>0.941022</td>\n",
       "      <td>0.971583</td>\n",
       "      <td>0.955071</td>\n",
       "      <td>0.955049</td>\n",
       "      <td>0.938356</td>\n",
       "      <td>0.028417</td>\n",
       "      <td>0</td>\n",
       "      <td>{'hidden_sizes': [100, 100, 50, 100, 100], 'dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.959042</td>\n",
       "      <td>0.951041</td>\n",
       "      <td>0.968419</td>\n",
       "      <td>0.959042</td>\n",
       "      <td>0.959033</td>\n",
       "      <td>0.949555</td>\n",
       "      <td>0.031581</td>\n",
       "      <td>1</td>\n",
       "      <td>{'hidden_sizes': [100, 100, 50, 100, 100], 'dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.953710</td>\n",
       "      <td>0.944751</td>\n",
       "      <td>0.964358</td>\n",
       "      <td>0.953710</td>\n",
       "      <td>0.953698</td>\n",
       "      <td>0.942935</td>\n",
       "      <td>0.035642</td>\n",
       "      <td>2</td>\n",
       "      <td>{'hidden_sizes': [100, 100, 50, 100, 100], 'dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.961316</td>\n",
       "      <td>0.942104</td>\n",
       "      <td>0.983536</td>\n",
       "      <td>0.961316</td>\n",
       "      <td>0.961285</td>\n",
       "      <td>0.938827</td>\n",
       "      <td>0.016464</td>\n",
       "      <td>3</td>\n",
       "      <td>{'hidden_sizes': [100, 100, 50, 100, 100], 'dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.966307</td>\n",
       "      <td>0.949772</td>\n",
       "      <td>0.985115</td>\n",
       "      <td>0.966307</td>\n",
       "      <td>0.966287</td>\n",
       "      <td>0.947272</td>\n",
       "      <td>0.014885</td>\n",
       "      <td>4</td>\n",
       "      <td>{'hidden_sizes': [100, 100, 50, 100, 100], 'dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.947356</td>\n",
       "      <td>0.922700</td>\n",
       "      <td>0.977221</td>\n",
       "      <td>0.947356</td>\n",
       "      <td>0.947289</td>\n",
       "      <td>0.917123</td>\n",
       "      <td>0.022779</td>\n",
       "      <td>0</td>\n",
       "      <td>{'hidden_sizes': [100, 100, 50, 100, 100], 'dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.942478</td>\n",
       "      <td>0.922696</td>\n",
       "      <td>0.966614</td>\n",
       "      <td>0.942478</td>\n",
       "      <td>0.942427</td>\n",
       "      <td>0.918055</td>\n",
       "      <td>0.033386</td>\n",
       "      <td>1</td>\n",
       "      <td>{'hidden_sizes': [100, 100, 50, 100, 100], 'dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.960290</td>\n",
       "      <td>0.941979</td>\n",
       "      <td>0.981502</td>\n",
       "      <td>0.960290</td>\n",
       "      <td>0.960261</td>\n",
       "      <td>0.938827</td>\n",
       "      <td>0.018498</td>\n",
       "      <td>2</td>\n",
       "      <td>{'hidden_sizes': [100, 100, 50, 100, 100], 'dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.943165</td>\n",
       "      <td>0.920103</td>\n",
       "      <td>0.971358</td>\n",
       "      <td>0.943165</td>\n",
       "      <td>0.943099</td>\n",
       "      <td>0.914631</td>\n",
       "      <td>0.028642</td>\n",
       "      <td>3</td>\n",
       "      <td>{'hidden_sizes': [100, 100, 50, 100, 100], 'dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.943052</td>\n",
       "      <td>0.929070</td>\n",
       "      <td>0.960081</td>\n",
       "      <td>0.943052</td>\n",
       "      <td>0.943022</td>\n",
       "      <td>0.925816</td>\n",
       "      <td>0.039919</td>\n",
       "      <td>4</td>\n",
       "      <td>{'hidden_sizes': [100, 100, 50, 100, 100], 'dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.913206</td>\n",
       "      <td>0.887762</td>\n",
       "      <td>0.947226</td>\n",
       "      <td>0.913206</td>\n",
       "      <td>0.913068</td>\n",
       "      <td>0.878767</td>\n",
       "      <td>0.052774</td>\n",
       "      <td>0</td>\n",
       "      <td>{'hidden_sizes': [100, 100, 50, 100, 100], 'dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.907307</td>\n",
       "      <td>0.879672</td>\n",
       "      <td>0.944958</td>\n",
       "      <td>0.907307</td>\n",
       "      <td>0.907133</td>\n",
       "      <td>0.869208</td>\n",
       "      <td>0.055042</td>\n",
       "      <td>1</td>\n",
       "      <td>{'hidden_sizes': [100, 100, 50, 100, 100], 'dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.926594</td>\n",
       "      <td>0.902082</td>\n",
       "      <td>0.958042</td>\n",
       "      <td>0.926594</td>\n",
       "      <td>0.926493</td>\n",
       "      <td>0.894773</td>\n",
       "      <td>0.041958</td>\n",
       "      <td>2</td>\n",
       "      <td>{'hidden_sizes': [100, 100, 50, 100, 100], 'dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.912535</td>\n",
       "      <td>0.888276</td>\n",
       "      <td>0.944971</td>\n",
       "      <td>0.912535</td>\n",
       "      <td>0.912408</td>\n",
       "      <td>0.879708</td>\n",
       "      <td>0.055029</td>\n",
       "      <td>3</td>\n",
       "      <td>{'hidden_sizes': [100, 100, 50, 100, 100], 'dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.913443</td>\n",
       "      <td>0.885366</td>\n",
       "      <td>0.951060</td>\n",
       "      <td>0.913443</td>\n",
       "      <td>0.913280</td>\n",
       "      <td>0.875371</td>\n",
       "      <td>0.048940</td>\n",
       "      <td>4</td>\n",
       "      <td>{'hidden_sizes': [100, 100, 50, 100, 100], 'dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.946108</td>\n",
       "      <td>0.908229</td>\n",
       "      <td>0.993234</td>\n",
       "      <td>0.946108</td>\n",
       "      <td>0.945955</td>\n",
       "      <td>0.898402</td>\n",
       "      <td>0.006766</td>\n",
       "      <td>0</td>\n",
       "      <td>{'hidden_sizes': [100, 100, 50, 100, 100], 'dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.944860</td>\n",
       "      <td>0.905986</td>\n",
       "      <td>0.993458</td>\n",
       "      <td>0.944860</td>\n",
       "      <td>0.944696</td>\n",
       "      <td>0.895686</td>\n",
       "      <td>0.006542</td>\n",
       "      <td>1</td>\n",
       "      <td>{'hidden_sizes': [100, 100, 50, 100, 100], 'dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.943839</td>\n",
       "      <td>0.902988</td>\n",
       "      <td>0.995263</td>\n",
       "      <td>0.943839</td>\n",
       "      <td>0.943654</td>\n",
       "      <td>0.891806</td>\n",
       "      <td>0.004737</td>\n",
       "      <td>2</td>\n",
       "      <td>{'hidden_sizes': [100, 100, 50, 100, 100], 'dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.942371</td>\n",
       "      <td>0.900776</td>\n",
       "      <td>0.995038</td>\n",
       "      <td>0.942371</td>\n",
       "      <td>0.942172</td>\n",
       "      <td>0.889066</td>\n",
       "      <td>0.004962</td>\n",
       "      <td>3</td>\n",
       "      <td>{'hidden_sizes': [100, 100, 50, 100, 100], 'dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.943732</td>\n",
       "      <td>0.906650</td>\n",
       "      <td>0.990077</td>\n",
       "      <td>0.943732</td>\n",
       "      <td>0.943578</td>\n",
       "      <td>0.896827</td>\n",
       "      <td>0.009923</td>\n",
       "      <td>4</td>\n",
       "      <td>{'hidden_sizes': [100, 100, 50, 100, 100], 'dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.872915</td>\n",
       "      <td>0.991430</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.922648</td>\n",
       "      <td>0.853881</td>\n",
       "      <td>0.008570</td>\n",
       "      <td>0</td>\n",
       "      <td>{'hidden_sizes': [100, 100, 50, 100, 100], 'dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.932607</td>\n",
       "      <td>0.887230</td>\n",
       "      <td>0.992105</td>\n",
       "      <td>0.932607</td>\n",
       "      <td>0.932318</td>\n",
       "      <td>0.872404</td>\n",
       "      <td>0.007895</td>\n",
       "      <td>1</td>\n",
       "      <td>{'hidden_sizes': [100, 100, 50, 100, 100], 'dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.928409</td>\n",
       "      <td>0.878987</td>\n",
       "      <td>0.994586</td>\n",
       "      <td>0.928409</td>\n",
       "      <td>0.928036</td>\n",
       "      <td>0.861447</td>\n",
       "      <td>0.005414</td>\n",
       "      <td>2</td>\n",
       "      <td>{'hidden_sizes': [100, 100, 50, 100, 100], 'dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.920023</td>\n",
       "      <td>0.865373</td>\n",
       "      <td>0.995940</td>\n",
       "      <td>0.920023</td>\n",
       "      <td>0.919483</td>\n",
       "      <td>0.843186</td>\n",
       "      <td>0.004060</td>\n",
       "      <td>3</td>\n",
       "      <td>{'hidden_sizes': [100, 100, 50, 100, 100], 'dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.943165</td>\n",
       "      <td>0.901736</td>\n",
       "      <td>0.995489</td>\n",
       "      <td>0.943165</td>\n",
       "      <td>0.942971</td>\n",
       "      <td>0.890208</td>\n",
       "      <td>0.004511</td>\n",
       "      <td>4</td>\n",
       "      <td>{'hidden_sizes': [100, 100, 50, 100, 100], 'dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.900159</td>\n",
       "      <td>0.836553</td>\n",
       "      <td>0.996166</td>\n",
       "      <td>0.900159</td>\n",
       "      <td>0.899107</td>\n",
       "      <td>0.802968</td>\n",
       "      <td>0.003834</td>\n",
       "      <td>0</td>\n",
       "      <td>{'hidden_sizes': [100, 100, 50, 100, 100], 'dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.893465</td>\n",
       "      <td>0.831877</td>\n",
       "      <td>0.987819</td>\n",
       "      <td>0.893465</td>\n",
       "      <td>0.892385</td>\n",
       "      <td>0.797991</td>\n",
       "      <td>0.012181</td>\n",
       "      <td>1</td>\n",
       "      <td>{'hidden_sizes': [100, 100, 50, 100, 100], 'dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.890742</td>\n",
       "      <td>0.826496</td>\n",
       "      <td>0.990751</td>\n",
       "      <td>0.890742</td>\n",
       "      <td>0.889504</td>\n",
       "      <td>0.789546</td>\n",
       "      <td>0.009249</td>\n",
       "      <td>2</td>\n",
       "      <td>{'hidden_sizes': [100, 100, 50, 100, 100], 'dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.878162</td>\n",
       "      <td>0.808370</td>\n",
       "      <td>0.993234</td>\n",
       "      <td>0.878162</td>\n",
       "      <td>0.876351</td>\n",
       "      <td>0.761698</td>\n",
       "      <td>0.006766</td>\n",
       "      <td>3</td>\n",
       "      <td>{'hidden_sizes': [100, 100, 50, 100, 100], 'dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.897674</td>\n",
       "      <td>0.834850</td>\n",
       "      <td>0.993009</td>\n",
       "      <td>0.897674</td>\n",
       "      <td>0.896613</td>\n",
       "      <td>0.801187</td>\n",
       "      <td>0.006991</td>\n",
       "      <td>4</td>\n",
       "      <td>{'hidden_sizes': [100, 100, 50, 100, 100], 'dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.913433</td>\n",
       "      <td>0.911086</td>\n",
       "      <td>0.917456</td>\n",
       "      <td>0.913433</td>\n",
       "      <td>0.913425</td>\n",
       "      <td>0.909361</td>\n",
       "      <td>0.082544</td>\n",
       "      <td>0</td>\n",
       "      <td>{'hidden_sizes': [100, 100, 50, 100, 100], 'dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.933515</td>\n",
       "      <td>0.923772</td>\n",
       "      <td>0.945861</td>\n",
       "      <td>0.933515</td>\n",
       "      <td>0.933493</td>\n",
       "      <td>0.921023</td>\n",
       "      <td>0.054139</td>\n",
       "      <td>1</td>\n",
       "      <td>{'hidden_sizes': [100, 100, 50, 100, 100], 'dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.931246</td>\n",
       "      <td>0.908431</td>\n",
       "      <td>0.960072</td>\n",
       "      <td>0.931246</td>\n",
       "      <td>0.931164</td>\n",
       "      <td>0.902077</td>\n",
       "      <td>0.039928</td>\n",
       "      <td>2</td>\n",
       "      <td>{'hidden_sizes': [100, 100, 50, 100, 100], 'dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.922632</td>\n",
       "      <td>0.905359</td>\n",
       "      <td>0.944971</td>\n",
       "      <td>0.922632</td>\n",
       "      <td>0.922571</td>\n",
       "      <td>0.900023</td>\n",
       "      <td>0.055029</td>\n",
       "      <td>3</td>\n",
       "      <td>{'hidden_sizes': [100, 100, 50, 100, 100], 'dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.924447</td>\n",
       "      <td>0.912796</td>\n",
       "      <td>0.939558</td>\n",
       "      <td>0.924447</td>\n",
       "      <td>0.924414</td>\n",
       "      <td>0.909153</td>\n",
       "      <td>0.060442</td>\n",
       "      <td>4</td>\n",
       "      <td>{'hidden_sizes': [100, 100, 50, 100, 100], 'dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.893125</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.918809</td>\n",
       "      <td>0.893125</td>\n",
       "      <td>0.893020</td>\n",
       "      <td>0.867123</td>\n",
       "      <td>0.081191</td>\n",
       "      <td>0</td>\n",
       "      <td>{'hidden_sizes': [100, 100, 50, 100, 100], 'dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.888927</td>\n",
       "      <td>0.878728</td>\n",
       "      <td>0.903903</td>\n",
       "      <td>0.888927</td>\n",
       "      <td>0.888881</td>\n",
       "      <td>0.873773</td>\n",
       "      <td>0.096097</td>\n",
       "      <td>1</td>\n",
       "      <td>{'hidden_sizes': [100, 100, 50, 100, 100], 'dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.898457</td>\n",
       "      <td>0.871795</td>\n",
       "      <td>0.935709</td>\n",
       "      <td>0.898457</td>\n",
       "      <td>0.898271</td>\n",
       "      <td>0.860762</td>\n",
       "      <td>0.064291</td>\n",
       "      <td>2</td>\n",
       "      <td>{'hidden_sizes': [100, 100, 50, 100, 100], 'dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.912762</td>\n",
       "      <td>0.886196</td>\n",
       "      <td>0.948354</td>\n",
       "      <td>0.912762</td>\n",
       "      <td>0.912613</td>\n",
       "      <td>0.876740</td>\n",
       "      <td>0.051646</td>\n",
       "      <td>3</td>\n",
       "      <td>{'hidden_sizes': [100, 100, 50, 100, 100], 'dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.916166</td>\n",
       "      <td>0.902243</td>\n",
       "      <td>0.934596</td>\n",
       "      <td>0.916166</td>\n",
       "      <td>0.916117</td>\n",
       "      <td>0.897512</td>\n",
       "      <td>0.065404</td>\n",
       "      <td>4</td>\n",
       "      <td>{'hidden_sizes': [100, 100, 50, 100, 100], 'dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.871568</td>\n",
       "      <td>0.846703</td>\n",
       "      <td>0.909337</td>\n",
       "      <td>0.871568</td>\n",
       "      <td>0.871326</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.090663</td>\n",
       "      <td>0</td>\n",
       "      <td>{'hidden_sizes': [100, 100, 50, 100, 100], 'dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.864193</td>\n",
       "      <td>0.838211</td>\n",
       "      <td>0.904579</td>\n",
       "      <td>0.864193</td>\n",
       "      <td>0.863909</td>\n",
       "      <td>0.823328</td>\n",
       "      <td>0.095421</td>\n",
       "      <td>1</td>\n",
       "      <td>{'hidden_sizes': [100, 100, 50, 100, 100], 'dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.878489</td>\n",
       "      <td>0.851086</td>\n",
       "      <td>0.919242</td>\n",
       "      <td>0.878489</td>\n",
       "      <td>0.878229</td>\n",
       "      <td>0.837252</td>\n",
       "      <td>0.080758</td>\n",
       "      <td>2</td>\n",
       "      <td>{'hidden_sizes': [100, 100, 50, 100, 100], 'dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.862167</td>\n",
       "      <td>0.832885</td>\n",
       "      <td>0.908209</td>\n",
       "      <td>0.862167</td>\n",
       "      <td>0.861800</td>\n",
       "      <td>0.815567</td>\n",
       "      <td>0.091791</td>\n",
       "      <td>3</td>\n",
       "      <td>{'hidden_sizes': [100, 100, 50, 100, 100], 'dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.844810</td>\n",
       "      <td>0.814268</td>\n",
       "      <td>0.895805</td>\n",
       "      <td>0.844810</td>\n",
       "      <td>0.844315</td>\n",
       "      <td>0.793198</td>\n",
       "      <td>0.104195</td>\n",
       "      <td>4</td>\n",
       "      <td>{'hidden_sizes': [100, 100, 50, 100, 100], 'dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.917858</td>\n",
       "      <td>0.869522</td>\n",
       "      <td>0.984438</td>\n",
       "      <td>0.917858</td>\n",
       "      <td>0.917423</td>\n",
       "      <td>0.850457</td>\n",
       "      <td>0.015562</td>\n",
       "      <td>0</td>\n",
       "      <td>{'hidden_sizes': [100, 100, 50, 100, 100], 'dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.904016</td>\n",
       "      <td>0.848862</td>\n",
       "      <td>0.984435</td>\n",
       "      <td>0.904016</td>\n",
       "      <td>0.903297</td>\n",
       "      <td>0.822643</td>\n",
       "      <td>0.015565</td>\n",
       "      <td>1</td>\n",
       "      <td>{'hidden_sizes': [100, 100, 50, 100, 100], 'dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.922850</td>\n",
       "      <td>0.869899</td>\n",
       "      <td>0.995488</td>\n",
       "      <td>0.922850</td>\n",
       "      <td>0.922372</td>\n",
       "      <td>0.849349</td>\n",
       "      <td>0.004512</td>\n",
       "      <td>2</td>\n",
       "      <td>{'hidden_sizes': [100, 100, 50, 100, 100], 'dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.906410</td>\n",
       "      <td>0.851823</td>\n",
       "      <td>0.985341</td>\n",
       "      <td>0.906410</td>\n",
       "      <td>0.905731</td>\n",
       "      <td>0.826524</td>\n",
       "      <td>0.014659</td>\n",
       "      <td>3</td>\n",
       "      <td>{'hidden_sizes': [100, 100, 50, 100, 100], 'dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.908905</td>\n",
       "      <td>0.855632</td>\n",
       "      <td>0.985115</td>\n",
       "      <td>0.908905</td>\n",
       "      <td>0.908287</td>\n",
       "      <td>0.831774</td>\n",
       "      <td>0.014885</td>\n",
       "      <td>4</td>\n",
       "      <td>{'hidden_sizes': [100, 100, 50, 100, 100], 'dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.879623</td>\n",
       "      <td>0.812373</td>\n",
       "      <td>0.989175</td>\n",
       "      <td>0.879623</td>\n",
       "      <td>0.877993</td>\n",
       "      <td>0.768721</td>\n",
       "      <td>0.010825</td>\n",
       "      <td>0</td>\n",
       "      <td>{'hidden_sizes': [100, 100, 50, 100, 100], 'dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.869639</td>\n",
       "      <td>0.800293</td>\n",
       "      <td>0.987142</td>\n",
       "      <td>0.869639</td>\n",
       "      <td>0.867626</td>\n",
       "      <td>0.750742</td>\n",
       "      <td>0.012858</td>\n",
       "      <td>1</td>\n",
       "      <td>{'hidden_sizes': [100, 100, 50, 100, 100], 'dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.868618</td>\n",
       "      <td>0.796702</td>\n",
       "      <td>0.991879</td>\n",
       "      <td>0.868618</td>\n",
       "      <td>0.866391</td>\n",
       "      <td>0.743894</td>\n",
       "      <td>0.008121</td>\n",
       "      <td>2</td>\n",
       "      <td>{'hidden_sizes': [100, 100, 50, 100, 100], 'dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.873398</td>\n",
       "      <td>0.804069</td>\n",
       "      <td>0.989400</td>\n",
       "      <td>0.873398</td>\n",
       "      <td>0.871486</td>\n",
       "      <td>0.755992</td>\n",
       "      <td>0.010600</td>\n",
       "      <td>3</td>\n",
       "      <td>{'hidden_sizes': [100, 100, 50, 100, 100], 'dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.877595</td>\n",
       "      <td>0.808989</td>\n",
       "      <td>0.990528</td>\n",
       "      <td>0.877595</td>\n",
       "      <td>0.875840</td>\n",
       "      <td>0.763296</td>\n",
       "      <td>0.009472</td>\n",
       "      <td>4</td>\n",
       "      <td>{'hidden_sizes': [100, 100, 50, 100, 100], 'dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.835943</td>\n",
       "      <td>0.757320</td>\n",
       "      <td>0.991655</td>\n",
       "      <td>0.835943</td>\n",
       "      <td>0.831533</td>\n",
       "      <td>0.678311</td>\n",
       "      <td>0.008345</td>\n",
       "      <td>0</td>\n",
       "      <td>{'hidden_sizes': [100, 100, 50, 100, 100], 'dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.822668</td>\n",
       "      <td>0.747329</td>\n",
       "      <td>0.978119</td>\n",
       "      <td>0.822668</td>\n",
       "      <td>0.817936</td>\n",
       "      <td>0.665373</td>\n",
       "      <td>0.021881</td>\n",
       "      <td>1</td>\n",
       "      <td>{'hidden_sizes': [100, 100, 50, 100, 100], 'dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.821307</td>\n",
       "      <td>0.744776</td>\n",
       "      <td>0.980826</td>\n",
       "      <td>0.821307</td>\n",
       "      <td>0.816287</td>\n",
       "      <td>0.659895</td>\n",
       "      <td>0.019174</td>\n",
       "      <td>2</td>\n",
       "      <td>{'hidden_sizes': [100, 100, 50, 100, 100], 'dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.837890</td>\n",
       "      <td>0.761714</td>\n",
       "      <td>0.986243</td>\n",
       "      <td>0.837890</td>\n",
       "      <td>0.833937</td>\n",
       "      <td>0.687743</td>\n",
       "      <td>0.013757</td>\n",
       "      <td>3</td>\n",
       "      <td>{'hidden_sizes': [100, 100, 50, 100, 100], 'dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.845831</td>\n",
       "      <td>0.774015</td>\n",
       "      <td>0.979477</td>\n",
       "      <td>0.845831</td>\n",
       "      <td>0.842770</td>\n",
       "      <td>0.710568</td>\n",
       "      <td>0.020523</td>\n",
       "      <td>4</td>\n",
       "      <td>{'hidden_sizes': [100, 100, 50, 100, 100], 'dr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy  Precision    Recall  F1_Micro  F1_Macro  Specificity  \\\n",
       "0  0.955071   0.941022  0.971583  0.955071  0.955049     0.938356   \n",
       "0  0.959042   0.951041  0.968419  0.959042  0.959033     0.949555   \n",
       "0  0.953710   0.944751  0.964358  0.953710  0.953698     0.942935   \n",
       "0  0.961316   0.942104  0.983536  0.961316  0.961285     0.938827   \n",
       "0  0.966307   0.949772  0.985115  0.966307  0.966287     0.947272   \n",
       "0  0.947356   0.922700  0.977221  0.947356  0.947289     0.917123   \n",
       "0  0.942478   0.922696  0.966614  0.942478  0.942427     0.918055   \n",
       "0  0.960290   0.941979  0.981502  0.960290  0.960261     0.938827   \n",
       "0  0.943165   0.920103  0.971358  0.943165  0.943099     0.914631   \n",
       "0  0.943052   0.929070  0.960081  0.943052  0.943022     0.925816   \n",
       "0  0.913206   0.887762  0.947226  0.913206  0.913068     0.878767   \n",
       "0  0.907307   0.879672  0.944958  0.907307  0.907133     0.869208   \n",
       "0  0.926594   0.902082  0.958042  0.926594  0.926493     0.894773   \n",
       "0  0.912535   0.888276  0.944971  0.912535  0.912408     0.879708   \n",
       "0  0.913443   0.885366  0.951060  0.913443  0.913280     0.875371   \n",
       "0  0.946108   0.908229  0.993234  0.946108  0.945955     0.898402   \n",
       "0  0.944860   0.905986  0.993458  0.944860  0.944696     0.895686   \n",
       "0  0.943839   0.902988  0.995263  0.943839  0.943654     0.891806   \n",
       "0  0.942371   0.900776  0.995038  0.942371  0.942172     0.889066   \n",
       "0  0.943732   0.906650  0.990077  0.943732  0.943578     0.896827   \n",
       "0  0.923077   0.872915  0.991430  0.923077  0.922648     0.853881   \n",
       "0  0.932607   0.887230  0.992105  0.932607  0.932318     0.872404   \n",
       "0  0.928409   0.878987  0.994586  0.928409  0.928036     0.861447   \n",
       "0  0.920023   0.865373  0.995940  0.920023  0.919483     0.843186   \n",
       "0  0.943165   0.901736  0.995489  0.943165  0.942971     0.890208   \n",
       "0  0.900159   0.836553  0.996166  0.900159  0.899107     0.802968   \n",
       "0  0.893465   0.831877  0.987819  0.893465  0.892385     0.797991   \n",
       "0  0.890742   0.826496  0.990751  0.890742  0.889504     0.789546   \n",
       "0  0.878162   0.808370  0.993234  0.878162  0.876351     0.761698   \n",
       "0  0.897674   0.834850  0.993009  0.897674  0.896613     0.801187   \n",
       "0  0.913433   0.911086  0.917456  0.913433  0.913425     0.909361   \n",
       "0  0.933515   0.923772  0.945861  0.933515  0.933493     0.921023   \n",
       "0  0.931246   0.908431  0.960072  0.931246  0.931164     0.902077   \n",
       "0  0.922632   0.905359  0.944971  0.922632  0.922571     0.900023   \n",
       "0  0.924447   0.912796  0.939558  0.924447  0.924414     0.909153   \n",
       "0  0.893125   0.875000  0.918809  0.893125  0.893020     0.867123   \n",
       "0  0.888927   0.878728  0.903903  0.888927  0.888881     0.873773   \n",
       "0  0.898457   0.871795  0.935709  0.898457  0.898271     0.860762   \n",
       "0  0.912762   0.886196  0.948354  0.912762  0.912613     0.876740   \n",
       "0  0.916166   0.902243  0.934596  0.916166  0.916117     0.897512   \n",
       "0  0.871568   0.846703  0.909337  0.871568  0.871326     0.833333   \n",
       "0  0.864193   0.838211  0.904579  0.864193  0.863909     0.823328   \n",
       "0  0.878489   0.851086  0.919242  0.878489  0.878229     0.837252   \n",
       "0  0.862167   0.832885  0.908209  0.862167  0.861800     0.815567   \n",
       "0  0.844810   0.814268  0.895805  0.844810  0.844315     0.793198   \n",
       "0  0.917858   0.869522  0.984438  0.917858  0.917423     0.850457   \n",
       "0  0.904016   0.848862  0.984435  0.904016  0.903297     0.822643   \n",
       "0  0.922850   0.869899  0.995488  0.922850  0.922372     0.849349   \n",
       "0  0.906410   0.851823  0.985341  0.906410  0.905731     0.826524   \n",
       "0  0.908905   0.855632  0.985115  0.908905  0.908287     0.831774   \n",
       "0  0.879623   0.812373  0.989175  0.879623  0.877993     0.768721   \n",
       "0  0.869639   0.800293  0.987142  0.869639  0.867626     0.750742   \n",
       "0  0.868618   0.796702  0.991879  0.868618  0.866391     0.743894   \n",
       "0  0.873398   0.804069  0.989400  0.873398  0.871486     0.755992   \n",
       "0  0.877595   0.808989  0.990528  0.877595  0.875840     0.763296   \n",
       "0  0.835943   0.757320  0.991655  0.835943  0.831533     0.678311   \n",
       "0  0.822668   0.747329  0.978119  0.822668  0.817936     0.665373   \n",
       "0  0.821307   0.744776  0.980826  0.821307  0.816287     0.659895   \n",
       "0  0.837890   0.761714  0.986243  0.837890  0.833937     0.687743   \n",
       "0  0.845831   0.774015  0.979477  0.845831  0.842770     0.710568   \n",
       "\n",
       "   False Negative Ratio  Fold  \\\n",
       "0              0.028417     0   \n",
       "0              0.031581     1   \n",
       "0              0.035642     2   \n",
       "0              0.016464     3   \n",
       "0              0.014885     4   \n",
       "0              0.022779     0   \n",
       "0              0.033386     1   \n",
       "0              0.018498     2   \n",
       "0              0.028642     3   \n",
       "0              0.039919     4   \n",
       "0              0.052774     0   \n",
       "0              0.055042     1   \n",
       "0              0.041958     2   \n",
       "0              0.055029     3   \n",
       "0              0.048940     4   \n",
       "0              0.006766     0   \n",
       "0              0.006542     1   \n",
       "0              0.004737     2   \n",
       "0              0.004962     3   \n",
       "0              0.009923     4   \n",
       "0              0.008570     0   \n",
       "0              0.007895     1   \n",
       "0              0.005414     2   \n",
       "0              0.004060     3   \n",
       "0              0.004511     4   \n",
       "0              0.003834     0   \n",
       "0              0.012181     1   \n",
       "0              0.009249     2   \n",
       "0              0.006766     3   \n",
       "0              0.006991     4   \n",
       "0              0.082544     0   \n",
       "0              0.054139     1   \n",
       "0              0.039928     2   \n",
       "0              0.055029     3   \n",
       "0              0.060442     4   \n",
       "0              0.081191     0   \n",
       "0              0.096097     1   \n",
       "0              0.064291     2   \n",
       "0              0.051646     3   \n",
       "0              0.065404     4   \n",
       "0              0.090663     0   \n",
       "0              0.095421     1   \n",
       "0              0.080758     2   \n",
       "0              0.091791     3   \n",
       "0              0.104195     4   \n",
       "0              0.015562     0   \n",
       "0              0.015565     1   \n",
       "0              0.004512     2   \n",
       "0              0.014659     3   \n",
       "0              0.014885     4   \n",
       "0              0.010825     0   \n",
       "0              0.012858     1   \n",
       "0              0.008121     2   \n",
       "0              0.010600     3   \n",
       "0              0.009472     4   \n",
       "0              0.008345     0   \n",
       "0              0.021881     1   \n",
       "0              0.019174     2   \n",
       "0              0.013757     3   \n",
       "0              0.020523     4   \n",
       "\n",
       "                                    model_parameters  \n",
       "0  {'hidden_sizes': [100, 100, 50, 100, 100], 'dr...  \n",
       "0  {'hidden_sizes': [100, 100, 50, 100, 100], 'dr...  \n",
       "0  {'hidden_sizes': [100, 100, 50, 100, 100], 'dr...  \n",
       "0  {'hidden_sizes': [100, 100, 50, 100, 100], 'dr...  \n",
       "0  {'hidden_sizes': [100, 100, 50, 100, 100], 'dr...  \n",
       "0  {'hidden_sizes': [100, 100, 50, 100, 100], 'dr...  \n",
       "0  {'hidden_sizes': [100, 100, 50, 100, 100], 'dr...  \n",
       "0  {'hidden_sizes': [100, 100, 50, 100, 100], 'dr...  \n",
       "0  {'hidden_sizes': [100, 100, 50, 100, 100], 'dr...  \n",
       "0  {'hidden_sizes': [100, 100, 50, 100, 100], 'dr...  \n",
       "0  {'hidden_sizes': [100, 100, 50, 100, 100], 'dr...  \n",
       "0  {'hidden_sizes': [100, 100, 50, 100, 100], 'dr...  \n",
       "0  {'hidden_sizes': [100, 100, 50, 100, 100], 'dr...  \n",
       "0  {'hidden_sizes': [100, 100, 50, 100, 100], 'dr...  \n",
       "0  {'hidden_sizes': [100, 100, 50, 100, 100], 'dr...  \n",
       "0  {'hidden_sizes': [100, 100, 50, 100, 100], 'dr...  \n",
       "0  {'hidden_sizes': [100, 100, 50, 100, 100], 'dr...  \n",
       "0  {'hidden_sizes': [100, 100, 50, 100, 100], 'dr...  \n",
       "0  {'hidden_sizes': [100, 100, 50, 100, 100], 'dr...  \n",
       "0  {'hidden_sizes': [100, 100, 50, 100, 100], 'dr...  \n",
       "0  {'hidden_sizes': [100, 100, 50, 100, 100], 'dr...  \n",
       "0  {'hidden_sizes': [100, 100, 50, 100, 100], 'dr...  \n",
       "0  {'hidden_sizes': [100, 100, 50, 100, 100], 'dr...  \n",
       "0  {'hidden_sizes': [100, 100, 50, 100, 100], 'dr...  \n",
       "0  {'hidden_sizes': [100, 100, 50, 100, 100], 'dr...  \n",
       "0  {'hidden_sizes': [100, 100, 50, 100, 100], 'dr...  \n",
       "0  {'hidden_sizes': [100, 100, 50, 100, 100], 'dr...  \n",
       "0  {'hidden_sizes': [100, 100, 50, 100, 100], 'dr...  \n",
       "0  {'hidden_sizes': [100, 100, 50, 100, 100], 'dr...  \n",
       "0  {'hidden_sizes': [100, 100, 50, 100, 100], 'dr...  \n",
       "0  {'hidden_sizes': [100, 100, 50, 100, 100], 'dr...  \n",
       "0  {'hidden_sizes': [100, 100, 50, 100, 100], 'dr...  \n",
       "0  {'hidden_sizes': [100, 100, 50, 100, 100], 'dr...  \n",
       "0  {'hidden_sizes': [100, 100, 50, 100, 100], 'dr...  \n",
       "0  {'hidden_sizes': [100, 100, 50, 100, 100], 'dr...  \n",
       "0  {'hidden_sizes': [100, 100, 50, 100, 100], 'dr...  \n",
       "0  {'hidden_sizes': [100, 100, 50, 100, 100], 'dr...  \n",
       "0  {'hidden_sizes': [100, 100, 50, 100, 100], 'dr...  \n",
       "0  {'hidden_sizes': [100, 100, 50, 100, 100], 'dr...  \n",
       "0  {'hidden_sizes': [100, 100, 50, 100, 100], 'dr...  \n",
       "0  {'hidden_sizes': [100, 100, 50, 100, 100], 'dr...  \n",
       "0  {'hidden_sizes': [100, 100, 50, 100, 100], 'dr...  \n",
       "0  {'hidden_sizes': [100, 100, 50, 100, 100], 'dr...  \n",
       "0  {'hidden_sizes': [100, 100, 50, 100, 100], 'dr...  \n",
       "0  {'hidden_sizes': [100, 100, 50, 100, 100], 'dr...  \n",
       "0  {'hidden_sizes': [100, 100, 50, 100, 100], 'dr...  \n",
       "0  {'hidden_sizes': [100, 100, 50, 100, 100], 'dr...  \n",
       "0  {'hidden_sizes': [100, 100, 50, 100, 100], 'dr...  \n",
       "0  {'hidden_sizes': [100, 100, 50, 100, 100], 'dr...  \n",
       "0  {'hidden_sizes': [100, 100, 50, 100, 100], 'dr...  \n",
       "0  {'hidden_sizes': [100, 100, 50, 100, 100], 'dr...  \n",
       "0  {'hidden_sizes': [100, 100, 50, 100, 100], 'dr...  \n",
       "0  {'hidden_sizes': [100, 100, 50, 100, 100], 'dr...  \n",
       "0  {'hidden_sizes': [100, 100, 50, 100, 100], 'dr...  \n",
       "0  {'hidden_sizes': [100, 100, 50, 100, 100], 'dr...  \n",
       "0  {'hidden_sizes': [100, 100, 50, 100, 100], 'dr...  \n",
       "0  {'hidden_sizes': [100, 100, 50, 100, 100], 'dr...  \n",
       "0  {'hidden_sizes': [100, 100, 50, 100, 100], 'dr...  \n",
       "0  {'hidden_sizes': [100, 100, 50, 100, 100], 'dr...  \n",
       "0  {'hidden_sizes': [100, 100, 50, 100, 100], 'dr...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EwdtpqrqZNiJ"
   },
   "source": [
    "# **3. Export Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_path = r'..\\results'\n",
    "output_filename = r'\\Results_25_21_' + MODEL_NAME + '.xlsx' \n",
    "with pd.ExcelWriter(output_path + output_filename) as export:\n",
    "    results_train.to_excel(export , sheet_name = 'train')\n",
    "    results_test.to_excel(export , sheet_name = 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **4. Feature Importance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model\n",
      "Epoch [1/1000], Train Loss: 0.7372 , Test Loss: 0.7143\n",
      "Epoch [26/1000], Train Loss: 0.5380 , Test Loss: 0.6003\n",
      "Epoch [51/1000], Train Loss: 0.5272 , Test Loss: 0.5970\n",
      "Epoch [76/1000], Train Loss: 0.5860 , Test Loss: 0.5943\n",
      "Epoch [101/1000], Train Loss: 0.5223 , Test Loss: 0.5915\n",
      "Epoch [126/1000], Train Loss: 0.4863 , Test Loss: 0.5889\n",
      "Epoch [151/1000], Train Loss: 0.5149 , Test Loss: 0.5858\n",
      "Epoch [176/1000], Train Loss: 0.5159 , Test Loss: 0.5823\n",
      "Epoch [201/1000], Train Loss: 0.5414 , Test Loss: 0.5792\n",
      "Epoch [226/1000], Train Loss: 0.4476 , Test Loss: 0.5759\n",
      "Epoch [251/1000], Train Loss: 0.4645 , Test Loss: 0.5727\n",
      "Epoch [276/1000], Train Loss: 0.4627 , Test Loss: 0.5695\n",
      "Epoch [301/1000], Train Loss: 0.4321 , Test Loss: 0.5664\n",
      "Epoch [326/1000], Train Loss: 0.4764 , Test Loss: 0.5634\n",
      "Epoch [351/1000], Train Loss: 0.4555 , Test Loss: 0.5609\n",
      "Epoch [376/1000], Train Loss: 0.4419 , Test Loss: 0.5583\n",
      "Epoch [401/1000], Train Loss: 0.4843 , Test Loss: 0.5559\n",
      "Epoch [426/1000], Train Loss: 0.5034 , Test Loss: 0.5536\n",
      "Epoch [451/1000], Train Loss: 0.4662 , Test Loss: 0.5511\n",
      "Epoch [476/1000], Train Loss: 0.4345 , Test Loss: 0.5491\n",
      "Epoch [501/1000], Train Loss: 0.5178 , Test Loss: 0.5472\n",
      "Epoch [526/1000], Train Loss: 0.4377 , Test Loss: 0.5455\n",
      "Epoch [551/1000], Train Loss: 0.4611 , Test Loss: 0.5433\n",
      "Epoch [576/1000], Train Loss: 0.4589 , Test Loss: 0.5420\n",
      "Epoch [601/1000], Train Loss: 0.4020 , Test Loss: 0.5403\n",
      "Epoch [626/1000], Train Loss: 0.5625 , Test Loss: 0.5390\n",
      "Epoch [651/1000], Train Loss: 0.3709 , Test Loss: 0.5377\n",
      "Epoch [676/1000], Train Loss: 0.4466 , Test Loss: 0.5365\n",
      "Epoch [701/1000], Train Loss: 0.4537 , Test Loss: 0.5349\n",
      "Epoch [726/1000], Train Loss: 0.3946 , Test Loss: 0.5339\n",
      "Epoch [751/1000], Train Loss: 0.3810 , Test Loss: 0.5328\n",
      "Epoch [776/1000], Train Loss: 0.4230 , Test Loss: 0.5317\n",
      "Epoch [801/1000], Train Loss: 0.4248 , Test Loss: 0.5306\n",
      "Epoch [826/1000], Train Loss: 0.3678 , Test Loss: 0.5295\n",
      "Epoch [851/1000], Train Loss: 0.4102 , Test Loss: 0.5285\n",
      "Epoch [876/1000], Train Loss: 0.3969 , Test Loss: 0.5279\n",
      "Epoch [901/1000], Train Loss: 0.3562 , Test Loss: 0.5272\n",
      "Epoch [926/1000], Train Loss: 0.4035 , Test Loss: 0.5267\n",
      "Epoch [951/1000], Train Loss: 0.3912 , Test Loss: 0.5255\n",
      "Epoch [976/1000], Train Loss: 0.4166 , Test Loss: 0.5251\n"
     ]
    }
   ],
   "source": [
    "# Train model with the best hyperparameters\n",
    "# Define X and Y in both train and test sets\n",
    "X_train = df.loc[train_index].drop(columns = TARGET)\n",
    "Y_train = df.loc[train_index][TARGET]\n",
    "\n",
    "X_test = df.loc[test_index].drop(columns = TARGET)\n",
    "Y_test = df.loc[test_index][TARGET]\n",
    "\n",
    "# Convert data to Pytorch tensors\n",
    "x_train_tensor = torch.FloatTensor(X_train.values)\n",
    "x_test_tensor = torch.FloatTensor(X_test.values)\n",
    "\n",
    "y_train_tensor = torch.LongTensor([z[0] for z in Y_train.values.tolist()])\n",
    "y_test_tensor = torch.LongTensor([z[0] for z in Y_test.values.tolist()])\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(x_train_tensor, y_train_tensor)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size = 64, shuffle=True)\n",
    "dataset_size = len(dataloader.dataset)\n",
    "\n",
    "# Define model\n",
    "input_size = X_train.shape[1]\n",
    "hidden_sizes = [100, 100, 50, 100, 100]\n",
    "output_size = 2\n",
    "dropout_prob = 0.1\n",
    "model = FullyConnectedModel(input_size, hidden_sizes, output_size, dropout_prob)\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(weight = torch.tensor([1.0 , 2.0]))  # Weights for each class\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "\n",
    "# Define the early stopping criteria\n",
    "early_stop_thresh = 200\n",
    "best_val_loss = 1e10\n",
    "best_epoch = -1\n",
    "\n",
    "# Define Scheduler for Learning Rate\n",
    "scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.1, total_iters=30)\n",
    "\n",
    "# Training loop\n",
    "print('Training Model')\n",
    "num_epochs = 1_000\n",
    "aux_val_loss = []\n",
    "train_loss_history = []\n",
    "test_loss_history = []\n",
    "for epoch in range(num_epochs):\n",
    "    for id_batch, (x_batch, y_batch) in enumerate(dataloader):\n",
    "        if x_batch.shape[0] > 1:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    # Scheduler learning rate\n",
    "    before_lr = optimizer.param_groups[0][\"lr\"]\n",
    "    scheduler.step()\n",
    "    after_lr = optimizer.param_groups[0][\"lr\"]\n",
    "    # Eval model for early stop\n",
    "    model.eval()\n",
    "    test_outputs = model(x_test_tensor)\n",
    "    test_loss = criterion(test_outputs , y_test_tensor)\n",
    "    test_loss_history.append(test_loss.item())\n",
    "    train_loss_history.append(loss.item())\n",
    "    if test_loss < best_val_loss:\n",
    "        best_val_loss = test_loss\n",
    "        best_epoch = epoch\n",
    "        checkpoint(model, \"pytorch_\" + str(MODEL_NAME) + \".pth\")\n",
    "    elif epoch - best_epoch > early_stop_thresh:\n",
    "        print(\"Early stopped training at epoch %d\" % epoch)\n",
    "        break  # terminate the training loop\n",
    "    if epoch % 25 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item():.4f} , Test Loss: {test_loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create Pytorch Model Wrapper\n",
    "class PyTorchModelWrapper(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(PyTorchModelWrapper, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.softmax(self.model(x), dim=1)\n",
    "\n",
    "# Create wrapper object\n",
    "wrapped_model = PyTorchModelWrapper(model)\n",
    "\n",
    "# Create explainer\n",
    "explainer = shap.DeepExplainer(wrapped_model, x_test_tensor)\n",
    "instance_idx = 0\n",
    "X_instance = x_test_tensor[instance_idx].unsqueeze(0)\n",
    "shap_values = explainer.shap_values(shap.sample(x_test_tensor , 100) , check_additivity=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x750 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Summary plot for class 0\n",
    "shap.summary_plot(shap_values[: , : , 0],\n",
    "                  plot_type = 'bar',\n",
    "                  feature_names = X.columns.tolist(),\n",
    "                 title = 'Feature Importance for Class 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x750 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Summary plot for class 1\n",
    "shap.summary_plot(shap_values[: , : , 1],\n",
    "                  plot_type = 'bar',\n",
    "                  feature_names = X.columns.tolist(),\n",
    "                 title = 'Feature Importance for Class 0')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
